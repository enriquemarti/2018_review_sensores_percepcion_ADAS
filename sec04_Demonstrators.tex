This section describes some of the most relevant technological demonstrations, 
competitions, challenges and commercial platforms related with Automated 
Driving, starting from pioneering works in late 1980s until present day. Figure 
\ref{fig:tech-demos} arranges them in a timeline, with the focus on the sensors 
equipped by each platform.

The timeline allows to discern different stages (``ages'') in the development 
of Automated Driving technology, and to identify trends and approaches from the 
perception point of view for Automated Vehicles.

\begin{figure}[p] %[h]
  %\centering
  \includegraphics[width=0.95\textheight,angle=90,keepaspectratio]{"img/AD_demos_Timeline"}
  \caption{Timeline: relevant AD demos and their exteroceptive sensor 
  setup}
  \label{fig:tech-demos}
\end{figure}

\subsection{Pioneer works (1980-2000)}

Pioneer works in Automated Driving starts around mid-1980s focused in vision 
based techniques, which represented a huge computational burden for the
embeddable computers of the time. Automated Vehicles VaMoRs 
\cite{Dickmanns1987} and VaMP \cite{Gregor2002} from Bundeswehr 
University of Munich used a saccadic vision system: cameras on a rotating 
platform that focus in relevant elements. 
The University of Parma started its project ARGO in 1996. % \cite{Broggi1998}.
The vehicle completed over 2000 km of autonomous driving in public roads 
\cite{Broggi1999}, using a two camera system for road following,
platooning and obstacle avoidance. 
%Sixty transputers executed an intelligent 4-D approach to object tracking,
%delivering a huge amount of computational power according to the standards at
%that moment.

%In early 1990s INRIA creates the concept of a "Cybercar", a small automated 
%electric vehicle for shared urban use \cite{Parent1993}. Project Praxit√®le
%\cite{Massot1999} (a mixed public and industry initiative) led to a fully
%functional prototype. 
The Cybercar concept is born in early 1990s \cite{Parent1993}
as an urban vehicle with no pedals or steering wheel. 
In 1997 a prototype is installed in Schippol airport to transport passengers 
between terminal and parking \cite{Ozguner2007}. It used a LiDAR and vision
system to drive automatically in a dedicated lane with semaphores and
pedestrian crossings.

Also in 1997, the National Automated Highway System Consortium presented a 
demonstration of Automated Driving functionalities \cite{Thorpe1997}, intended 
to be a proof of technical feasibility. 
The demo showed road following functionality based on vision sensors, distance 
maintenance based on LiDAR, vehicle following based on Radar and other 
functionalities including cooperative maneuvers and mixed environments.

%At that time, relevant functional demonstrators strongly relied on visual 
%processing. 
%The University of Parma started in 1996 a project with its vehicle 
%ARGO \cite{Broggi1998}, equipped with two cameras that allowed road following,
%platooning and obstacle avoidance. The vehicle completed over 2000 km of 
%autonomous driving in public roads \cite{Broggi1999}.
%They detected lane lines and 
%vehicles using classical image processing techniques including preprocessing 
%steps, feature extraction, model fitting and spatio-temporal filtering.


\subsection{Proof of feasibility (2000-2010)}

In year 2004 DARPA started its Grand Challenge series to foster development of
Automated Driving technologies. The achievements over those three years 
not only represented a huge leap forward, but also called the attention of
powerful agents.
Two first challenges (2004 and 2005) consisted in covering a route over dirt
roads with off-road sections, with a strong focus in navigation and control.
Stanford University won the 2005 edition, equipping its vehicle Stanley with
5 LiDAR units, a frontal camera, GPS sensors, an IMU, wheel odometry and two 
automotive radars \cite{Thrun2006}. 
The Urban Challenge (2007) changed the focus to interaction with other vehicles,
pedestrians and obeying complex traffic regulations. Carnegie Mellon University
team ended in first position with its vehicle Boss 
%\footnote{http://www.tartanracing.org/press/boss-glance.pdf} 
\cite{TartanRacing2005, Urmson2007}, 
featuring a perception system composed by two video cameras, 5 radars and 13
LiDAR (including a roof mounted unit of the novel Velodyne 64HDL).
%A cluster of 10 server blades processed a complex behavioral model
%\cite{Urmson2007} for covering all the expected situations.


%In 2004, DARPA started its series of Grand Challenges to foster the development
%of robotics technology. 
%The first edition ended without a declared winner, since no contestant manage 
%to
%cover even a 10\% of the 240km route including dirt roads and off-road 
%sections.
%In 2005 edition, five of the 23 contestants finished the 212 km race. 
%The winner was Stanford University racing team. Its vehicle Stanley carried 5 
%LiDAR units 
%%used to create a 3D map of the environment with special attention to road 
%%geometry 
%, a frontal camera, GPS sensors, an IMU, wheel odometry and two 
%automotive radars \cite{Thrun2006}. These sensors were the input of a 
%complex processing pipeline distributed over 6 computers, that involved
%perception and estimation techniques, artificial intelligence, 3D mapping, 
%risk assessment and path planning.
%%, requiring 6 computers to do the processing. 
%
%For the next DARPA Challenge (Urban Challenge, 2007), participants had to
%complete a 96 km course in urban area, sharing the road 
%with other participants and cars driven by professional drivers. Robots were 
%required to obey traffic regulations, avoid obstacles and negotiate 
%intersections properly. Carnegie Mellon University team won the contest with 
%its vehicle Boss \cite{TartanRacing2005}, featuring a complex perception
%system composed by two video cameras, 5 
%radars and 13 LiDAR (including a roof mounted unit of the novel Velodyne 
%64HDL).
%A cluster of 10 server blades processed a complex behavioral model
%\cite{Urmson2007} for covering all the expected situations.

These events triggered the attention of Google. 
The company hired around 15 scientists from the DARPA challenge, 
including the winners of 2005 and 2007 \cite{Montemerlo2008}, 
\cite{Levinson2011}. Google's (and Waymo's) approach to self-driving vehicles
is largely founded in LiDAR and 3D mapping technologies \cite{Chapell2016}. 
All their vehicles have had a roof-mounted spinning LiDAR: Toyota Prius (2009),
the Firefly prototype (2014) and Chrysler Pacifica (2016-present).

The University of Parma created the spin-off VisLab in 2009. 
They are strong supporters of artificial vision as the main component of 
perception systems for AD. 
In 2010 they completed the VisLab Intercontinental Autonomous 
Challenge (VIAC): four automated vans drove from Italy to China over public 
roads that included degraded dirt roads and unmapped areas \cite{Bertozzi2011}.
The leading vehicle did perception (with cameras and LiDARs), decision and 
control, with some human intervention for selecting the route and managing 
critical situations \cite{Broggi2012}. 
%The 13,000 km long trip included
%unmapped areas, degraded dirt roads and different traffic conditions. 
In 2013 the PROUD test put a vehicle with no driver behind the wheel in Parma 
roads for doing urban driving in real traffic \cite{Broggi2013}. 
%Selected sensor configuration was very similar to that used in VIAC three years
%before.
 
\subsection{Race to commercial products (2010-present)}
 
In the last decade the landscape of Automated Driving has been dominated by 
private initiatives that foresee the coming of Level 4 and 5 systems in a few 
years. This vision gave birth to several companies devoted to this end, most of 
which were founded by people coming from the DARPA experience, or hired them to
lead the project \cite{Chapell2016}. 

Examples include the nuTonomy (co-founded
by the leader of the MIT team in 2007 Challenge), Cruise (founded by a 
member of the same team), Otto (founded by a participant in 2004 and 2005 
Challenges), Uber (hired up to 50 people from the CMU Robotics Lab),  
Zoox robotaxi company (co-founded by a member of the Stanford 
Autonomous Driving team) %with an expertise in LiDAR automated calibration 
\cite{Levinson2011a}, and Aurora (similar story with people from 
Uber, MIT and Waymo \cite{Anderson2013}).

Car manufacturers reacted a bit slower. 
%Stanford and Audi teamed to complete an
%automated ascent to Pikes Peak in 2010 (focused on control and not in
%perception) \cite{Funke2012}, they have not really entered into scene
%until the last five years. 
Some of them started independent research lines, for example
BMW has been testing automation prototypes in roads since 2011 
\cite{Aeberhard2015a} and Mercedes-Benz Bertha project \cite{Ziegler2014}
%\cite{Bender2014} 
drove in 2013 a 103 km route in automated
mode using close-to-market sensors (8 radars and 3 video cameras)
%This work presented new concepts as the \emph{Lanelets} for road 
%representation 
% and other innovative solutions .
% processed in a heterogeneous computing platform (FPGAs, embedded 
%processors).
, but in the end most manufacturers have created coalitions with technological
startups as enumerated in section \ref{sec:oem-ad}.


%This approach to perception --avoiding LiDARs. 
%as expensive and far from mass 
%production devices-- has been supported by other companies. As an example,
%Mobileye started working in embedded computer vision devices in 1999, and by
%2015 their technology was present in more than 25 car brands. Some years ago
Mobileye
%\footnote{https://www.mobileye.com/our-technology/} 
started working in a vision-only approach to Automated Driving
%\cite{Mobileye2018} 
a few years ago. 
%Their AI is claimed to hand the car with an ``\emph{assertive driving style}'' 
%that deals with traffic in a much less conservative way than its competitors,
%while being safe \cite{Shalev-shwartz2016, Shalev-Shwartz2017}. 
After testing in real conditions \cite{Edelstein2018}, they presented a demo 
with an automated Ford equipped just with 12 small monocular cameras for fully 
Automated Driving in 2018 \cite{Scheer2018}.

Tesla entered the Automated Driving scene in 2014.
All their vehicles were equipped with a monocular camera (based on 
Mobileye system) and an automotive radar that enabled the Level 2-3 AutoPilot
functionality. 
%that gathered data for training
%a "ghost" self-driving system based on reinforcement learning. 
Starting 2017 new Tesla vehicles include the ``version 2'' hardware, 
composed by a frontal radar, 12 sonars, and 8 cameras.
%, together with a nVidia Drive PX2 processing platform.
This sensor set is claimed to be enough for full Level 5 Automated Driving
\cite{Hawkins2017}, which will be available for a fee (when ready) through a
software update.

In 2015 VisLab was acquired by Ambarella, a company working on low power chips
able to process high resolution dense disparity maps from stereo cameras
\cite{Ambarella2018}. 
Its latest demo \cite{AUVSI2018} fused data from 10 stereo pairs into a
ultra-high resolution 3D scene delivering 900 million points per second.
Long range vision mix a forward facing 4k stereo pair with a radar for better
performance under low light or adverse weather conditions. 
%This is a different approach to visual-based perception, 
%since stereo vision aims to get the best of both LiDARs and image processing 
%in 
%a single tool, with additional advantages.

Delphi Automotive completed in 2015 an automated trip between San Francisco and
New York city using a custom Audi Q5 with 10 radars, 6 LiDARs and 3 cameras 
onboard. In 2017 they acquired nuTonomy (the first company to deliver a 
robotaxi service in public roads) and created Aptiv. 
Aptiv presented an automated taxi for CES conference in january 2018, as part
of a 20 vehicle fleet that has been serving a set of routes in Las Vegas for
some months. The taxis have an extensive set of 10 radars and 9 LiDARs embedded 
in the bodywork, plus one camera.

Meanwhile, Waymo has grown a fleet of Chrysler Pacifica minivans that has
self-driven 10 million miles by october 2018. Their efforts have reportedly 
cut prices of LiDAR sensors to less than one tenth in a few years. 
They claim to have created two ``new categories of LiDAR'' \cite{Waymoteam2017} 
in the way, one for close range perception including below the car, and the
other for long range. The long-range LiDAR can reportedly zoom dynamically into 
objects on the road, letting the vehicle see small objects up to 200 m away. 
This reminds the features of OPA solid state LiDARs (see section
\ref{sec:03-lidar-emerging}): random sampling across the 
scanning area and adaptive resolution.

% (which still can be high, since the cost of the
%64-layer Velodyne was over US\$ 75,000 when Waymo started its experiments).
