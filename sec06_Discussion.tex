
To conclude the survey this section makes a discussion of the future challenges that new autonomous vehicles will need to cope with, technical and implantation ones. Then a description of the next comercial initiatives and OEMs in automation driving is shown followed by the final conclusions.



\subsection{Future challenges}

As shown in sections \ref{sec:02-sensors} and \ref{sec:03-problemsapplications} there exist many works that solve the most important perception competences in different ways, using different types of sensors and with a large variety of algorithms. However, there still exist different challenges that need to be solved in order to achieve a functional secure autonomous vehicle. 

\subsubsection{Technical challenges}


\emph{Plain 360 degree coverage is not enough}

Sensor setups in Automated Driving are usually focused on the areas relevant for the usual driving task: long range ahead from the vehicle, also long but no so much behind, and short-mid range in the laterals. This covers all the behavioral compentences enumerated in section \ref{sec:03-problemsapplications}, but there are some specific challenges like critical distances (too short and too large), oclusions, bad weather conditions and very fast events that are not usually covered and don't have a proper solution yet.


\begin{itemize}
    \item \emph{Very short distance} (including close to or below the 
    car)
        A person, animal or object right below the vehicle or intersecting 
        the path of the wheels represents a safety issue. While most situations
        can be anticipated when the element approaches the vehicle from the
        distance, it is not the case right before starting the vehicle, 
        while executing high accuracy maneuvers in certain conditions 
        (close to people or other moving elements).  
        
        This is a problem that can be tackled by adding redundant sensors in 
        specific positions. Some commercial 360-degree-view parking systems 
        \cite{gandhi2006vehicle} already seem to have a good visibility of vehicle close 
        surroudings. Waymo claims to have a special LiDAR monitoring this area
        and even below the vehicle. 
        
        In the future there will be a need of devices specific for this task 
        that can make automated vehicles even safer than human drivers in such 
        situations.
          
    \item \emph{Very long distance} 
        Detection and classification at 200 meters is an open issue.
        The resolution needed to monitor the whole area with the required 
        accuracy is overwhelming. Among current approaches, Ambarella 
        integrates a Ultra High Resolution camera (4k video, probably 8k) 
        (cited in \ref{sec:04-relevantdemos}), that is claimed to be enough for 
        discerning small objects at that target distance. The biggest problem
        is the raw computational power required to process so much information.
        
        A different approach could consist on sensors able to determine 
        saliency (a common term in artificial vision 
        \cite{Zhang2016a,Palazzi2018,Duthon2016}), 
        so that other adaptive sensors can focus on that area, increasing
        resolution, frame rate or accuracy. 
        This reminds in some way the saccadic vision system used in Dickmann's 
        pioneer vehicles \cite{Dickmanns1987,Gregor2002}, where a rotating 
        platform where used to take images of areas of interest.
        
        While the last part is something feasible today, for example using solid
        state LiDARs capable of random or adaptive sampling, the real challenge
        resides in the saliency part: design a sensor that can determine that 
        something very far away can be relevant, without falling in the
        brute-force approach.
        
    \item \emph{Indirect observations} (e.g. after oclusions)
        Humans are able to infer knowledge about the world based on subtle 
        hints, e.g. see that someone is behind a car from a glimpse through
        vehicle glasses, or attending at the shadow projected in the road. 
        
        There are two different ways to approach this problem:
        the first one is to process the observations of high resolution sensors 
        using and powerful processing algorithms. 
        The second way is to create 
        sensing technologies with super-human perception capabilities that can 
        directly register such events or elements. 
        Radars represent at present time the best example, since the propagation
        of electromagnetic waves has properties very different to usual 
        biological sensing capabilities. For example, automotive radars can 
        sometimes detect several vehicles in a row, totally occluded by the 
        first one. Another example is WaveSense's ground-penetrating radar 
        \ref{https://www.bloomberg.com/news/articles/2018-09-17/self-driving-cars-still-can-t-handle-bad-weather}
        previuosly presented in radar emerging technologies (section 
        \ref{sec:02-sensors}). The challenge remains open.
    
    \item {Environmental and weather conditions}    
        Section \ref{sec:02-sensors} summarizes the suitability of common 
        technologies under different conditions, some of which surpass human 
        capacities. 
        However, this is always an active field of research because perception 
        can solve what humans compensate with reasoning. Following the road 
        when most marks are covered by snow, or improving detection under heavy 
        rain are examples of problems that can be solved at sensing level
        without requiring further efforts on processing algorithms.
    
    \item \emph{Adaptation to very fast events} ???
    
\end{itemize}


\subsubsection{Implantation challenges}

The final purpose of research in autonomous driving is to create an autonomous vehicle that will be in the market (either for private custumers or for commercial use in a fleet). This means that the final product have to fulfill certain scalability, costs, and durability characteristics so its production is feasible. The final cost has to be as low as possible, and the durability of the vehicle need to be similar to the current vehicle's. The sensors are one of the most expensive parts and one of the most fragile, so their correct implantation is a key factor in the development of autonomous driving vehicles.

\emph{Production scalability and costs}

Mature technologies as visible light cameras and radars have already scaled up 
their production and reduced costs so that every vehicle can equip them without
a significant impact on its price. But it remains a challenge for LiDAR devices
and other breakthrough technologies.

The cost that is considered acceptable for a production vehicle, however, 
varies depending on the scenario. For private owned vehicle, it must be kept at
a rather small fraction of vehicle cost. In the case of fleet vehicles with a 
commercial use it can be higher because an Automated Driving system can 
compensate the cost of an employee.
It is difficult to provide estimations, because Automated Driving can have a 
significant effect in mobility, economiy an other factors. For a discussion
on costs and impact of Automated Mobility services, see \cite{Bosch2018}.

\emph{Durability and tolerance to failure}

The perception system of an Automated Vehicle must work flawlessly for long
periods under harsh conditions, as the rest of vehicle critical components. 
In case of failure, redundancy and emergency fallback routines must be able to 
mitigate the problem and drive the vehicle to a safe state, but it is a 
threat that has to be avoided.

Mechanical LiDARs have been around for about a decade, and are highly 
specialized devices mostly used with research purposes.
The controversial CEO of Quanergy, Eldada, claims 
(https://www.freightwaves.com/news/autonomous-trucking/quanergyceoripsvelodyne)
that mechanical LiDAR sensors are unsuitable for commercial 
automotive applications because the mean-time-to-failure (MTF)
"between 1,000 to 3,000 hours of operation" on the rotating 
components is far too low for industry requirements. Automakers want an MTF
of at least 13,000 hours.

Solid state lidars based on vibrating micro-mirror (MEMS) can reduce costs and increase laser resolution but 
still have movile parts (micro-mirror), which makes them susceptible to vehicle vibrations and more fragile.

External factors can affect sensors. A stone chip can crack a glass while 
driving at high speeds ways, and this is something that can affect the 
performance of video cameras and possibly LiDARs even when protected behind
a plastic or glass layer. It is desirable to create or improve sensing 
technology that can minimize the impact of that kind of events.

Another kind of external factors are intentional attacks. A radar can be jammed,
and a camera or a LiDAR can be blinded by the appropriate source of light.
Future sensors will have to be robust against external interferences.

\subsection{Commercial initiatives}

At the end of the pervious decade (2009), 
the most requested  Advanced Driving Assistance Systems (ADAS) 
\cite{Frost&Sullivan2010} were the Anti-lock braking system and the Parking 
Assistance by Warning. These systems cannot be classified
higher than SAE Level 0.

In the last decade the automotive market have grown the offer and complexity
of ADAS \cite{Perez2016}. The most advanced cars today equip an ensemble of 
ADAS that place them somewhere between SAE Levels 2 and 3 in the scale of Automated
Driving. Some examples are Tesla AutoPilot and Audi JamAssist, able to
drive the vehicle under user supervision in specific scenarios.

The fact that manufacturers are starting to talk about SAE Levels
is a sign that ADAS are completely integrated into the market and, thus, are
not a subject of research per se anymore.  


%\emph{RETOS FUTUROS??}

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=1.00\textwidth]{"img/Resumen de algunas 
%aplicaciones 
%        y ADAS en el mercado_v3_trim"}
%    \caption{ADAS and AD applications in the market}
%    \label{fig:adas-apps}
%\end{figure}
%
%[3] 
%https://www.continental-automotive.com/en-gl/Landing-Pages/Industrial-Sensors/Products/ARS-404-21
%
%[4] 
%https://www.bosch-engineering.de/en/de/einsatzgebiete/schienenfahrzeuge/kollisionswarnung/kollisionswarnung\_1.html
%
%[5] https://www.mobileye.com/our-technology/
%
%[6] 
%https://brigade-electronics.com/products/radar-obstacle-detection/?gclid=EAIaIQobChMI2NTc\_ZKI3AIV6rXtCh1akgc6EAAYASAAEgK5xfD\_BwE
%
%[7] 
%https://www.bosch-mobility-solutions.com/en/highlights/automated-mobility/driver-assistance-systems-for-urban-areas/
%
%[8] https://www.nissan-global.com/EN/TECHNOLOGY/OVERVIEW/emergency\_brake.html
%
%[9] 
%https://www.zf.com/corporate/en\_de/products/product\_range/cars/cars\_automatic\_emergency\_braking\_function.shtml
%
%[10] https://www.mobileye.com/our-technology/
%
%[11] 
%https://support.volvocars.com/en-CA/cars/Pages/owners-manual.aspx?mc=y286\&my=2016\&sw=15w17\&article=d3d274ecedf1c586c0a801e8004927e7
%
%[20] https://www.valeo.com/en/lane-change-assistance-system/
%
%[30] 
%https://www.audi-mediacenter.com/en/techday-piloted-driving-the-traffic-jam-pilot-in-the-new-audi-a8-9276/automated-driving-at-a-new-level-the-audi-ai-traffic-jam-pilot-9283


\subsubsection{OEMs in Automated Driving}
\label{sec:oem-ad}

Back in 2010, most traditional vehicle manufacturers did not consider Automated
Driving as a priority. In the last years, the achievements of technological 
pioneers (Google/Waymo, Uber, Tesla among others) gave place to early alliances
between those companies that had the technology with OEMs that had the
platform, the experience and the market.

By the end of 2018, all the important brands are involved in a race for creating
the first highly automated vehicle (SAE Levels 4 and 5). It is difficult to
get information about their research further than public demonstrations and
marketing products. However, their alliances with other companies, startups
and technology/research centers are easier to trace and can hint about their 
approach to Automated Driving.


Figure \ref{fig:oem-ad} gathers the most important coallitions for Automated
Driving with OEMs involved.



\begin{figure*}[p]
    \centering
    \includegraphics[width=1.00\textwidth]{"img/OEM_jp_trim"}
    \caption{OEM projects and alliances in Automated Driving}
    \label{fig:oem-ad}
\end{figure*}

%\begin{table}
%    \caption{OEMs with Automated Driving projects}
%    \label{tab:oems}
%    \begin{tabular}{ | l | l | l | l | l | l | l | }
%    \textbf{OEMs} & \textbf{Test side} & \textbf{Technologies} & \textbf{Since}
%     & \textbf{Collaborations} & \textbf{Forecast} & \textbf{Test fleet} 
%    \\
%    \hline
%    Ford & Detroit, Arizona \& California (U.S.A.) & AI, LiDAR, and mapping  & 
%    \~2016 & Argo, Velodyne, SAIPS, civilmaps. & Level 4 (2021) & Fusion 
%Hybrid 
%    sedans \~100 by 2018 
%    \\
%    GM & Detroit San Francisco \& Scottsdale, Arizona (U.S.A.) & Lidar, very 
%    accurance map, radar, camera & \~2016 & Google’s Waymo and Jaguar-Land 
%    Rover & before 2020 (Fortune) & Fifty vehicles have been built by GM 
%(2017) 
%    \\
%    Renault-Nissan & Japan, EE.UU. \& China & Maintains speed, Steering 
%    control, Front radar, Lidars & \~2017 & Transdev, Microsoft and TechCrunch 
%    (from Oath)  & Fully autonomous car within the next 10 years. Level 3 -> 
%    2020 & --- 
%    \\
%    Daimler & Germany & Vision, data fusion, radar. & 2015 (Truck \& F015) & 
%    Bosch & 2020 & Commercial cars (level 2) 
%    \\
%    Volkswagen Group (Audi) & Germany & Lidar, data fusion, adaptive cruise 
%    control, self-parking \& TJA  & 2015 & Audi -> Delphi (2015);  Aurora 
%    (2017) & 2025 (level 4) & Commercial cars (level 3 -> Traffic Jams) 
%    \\ 
%    BMW & Germany, China & Vision, lidar, DGPS & 2011 & Intel,  and With Baidu 
%    \& Nokia’s HERE   & Level 5 autonomous car on the road by 2022. & 
%    Commercial cars (level 2) 
%    \\
%    Waymo & California (U.S.A.) & Lidar, vision system, radar, data fusion, RT 
%    Path plan.. & 2010 & Fiat-Chrysler,  Velodyne. & --- & 100 autonomous 
%    Pacifica minivans  
%    \\
%    Volvo & Sweden. \& Uber: San Francisco, Pittsburgh  & Vision, lidar, GPS, 
%    V2I & 2011 & Uber (U.S), Autoliv (Sweden)  & \~2020 & Commercial cars 
%    (level 2) 
%    \\
%    Tesla & U.S.A. & Camera, radar, AI & \~2015 & Apple, Mobileye and Nvidia & 
%    \~Full automated 2020  & Commercial cars (level 2) \\
%    Hyundai & South Korea & AI, LiDAR, Camera & 2014 & KIA, Aurora & AD Level 
%    3-> Highways by 2020 and to city streets by 2030 & --- 
%    \\
%    \end{tabular}
%\end{table}

\subsection{Conclusions}

Here it has been presented a survey about one of the most critical parts in autonomous vehicles, their sensors. Choosing the sensors configuration of an autonomous vehicle can be challenging. For all the perception competences, each sensor has different strengths and weakness. The task in which one sensor is not good, the other outperforms and vice versa. Usually the best solution consists in getting information from more than one type of sensor and fuse their information. It creates a more robust  perception system as it has more data variety from different sources and also increases the safety as the fault of an specific sensor can be managed by another one. As all of these advantages, it also presents some challenges like finding a proper way to calibrate all those sensors, or making good decisions when two sensors have different outputs, or in other words, make a proper fusion of all the information. Computational power and energy consumption are also related with the sensors choice. The more data the vehicle gets, the more computational power it will be needed with it corresponding energy consumption. That means that, if the vehicle has more sensors, the final cost will increase and therefore it will be more difficult  to produce those vehicles.

This survey has reviewed all the different sensors technologies avalible, describing their characteristics and how are they applied to get information usefull to solve the main perception competences. It gives a global vision of different sensor configurations and techniques to obtain usefull information from the environment.

The relevant works and demos provide a good perspective of how different manufacturers and research groups do perception tasks and wich kind of sensors they use for that purpose.

Finally, the section \ref{sec:05-commercialsystems}, which talks about commercial sensor systems fo ADAS can form an intuition about where are the manufacturers going in the autonomous vehicle process and how are they planning to get there.



%As the technology advances, new sensors with new specifications and charasteristics appears in the market [research with new platforms]