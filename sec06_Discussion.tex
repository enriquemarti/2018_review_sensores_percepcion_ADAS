
The last section of this article presents a discussion of the future challenges 
for sensors and perception systems in new automated vehicles, both from the 
technical and implantation point of view. A description of the next 
commercial initiatives and OEMs forecasts is shown followed by the final 
conclusions.


\subsection{Future challenges}

As shown in sections \ref{sec:02-sensors} and \ref{sec:03-problemsapplications} there exist many works that solve the most important perception competences, based on different types of sensors and with a large variety of algorithms. However, there still exist some challenges that need to be solved in order to achieve a functional secure automated vehicle.

\subsubsection{Technical challenges}

Sensor setups in Automated Driving are usually focused on the areas relevant 
for the usual driving tasks (covered in section \ref{sec:03-problemsapplications}). 
But for a commercial system expected to work in the real world there are still
some specific challenges that do not have a proper solution yet.

%\begin{itemize}
\textbf{Very short distance}, including close to or below the car.
    A person, animal or object right below the vehicle or intersecting 
    the path of the wheels represents a safety issue. While most situations
    can be anticipated when the element approaches the vehicle from the
    distance, it is not the case right before starting the vehicle, 
    while executing high accuracy maneuvers in certain conditions 
    (close to people or other moving elements).  
    This problem can be tackled by adding redundant sensors like \cite{gandhi2006vehicle}
    which uses a 360-degree-view parking system or a special LiDAR monitoring this area
    used by Waymo.    
    In the future there will be a need of specific devices for this task.
      
\textbf{Very long distance} 
    Detection and classification above 200 meters is an open issue. 
    Among current approaches, Ambarella integrates a Ultra High Resolution 
    camera (cited in \ref{sec:04-relevantdemos}) that is claimed to be enough
    for discerning small objects at that target distance, subject to the 
    limitations of visible light cameras.    
    Solutions based on saliency (a common term in artificial vision 
    \cite{Zhang2016a,Palazzi2018,Duthon2016} to name relevancy or importance) 
    can be an alternative to the high resolution and computational cost 
    associated to brute force approaches. Solid state LiDAR capable of
    random and adaptive sampling is a potential candidate solution for such 
    technology, achieving something similar to Waymo's claims about their custom
    built LiDARs.
%    Early saccadic vision system in \cite{Dickmanns1987,Gregor2002}, where a 
%    rotating platform where used to take images of areas of interest.    
%    While the last part is something feasible today, for example using solid
%    state LiDARs capable of random or adaptive sampling, the real challenge
%    resides in the saliency part: design a sensor that can determine that 
%    something very far away can be relevant, without falling in the
%    brute-force approach.

\textbf{Environmental and weather conditions}    
    Section \ref{sec:02-sensors} summarizes the suitability of common 
    technologies under different conditions, some of which surpass human 
    capacities. 
    This is an always active field of research, following the road  when
    most marks are covered by snow, improving detection under heavy rain 
    or dense fog are examples of problems that can be solved at sensing
    level without requiring further efforts on processing algorithms.
        
%\textbf{Intention detection}        
%    Human drivers infer the intentions of pedestrians and vehicles 
%    combining subtle sensory hints with a mind model of the observed actor,
%    and taking into account the context.
%    Head pose, gaze direction, motion of limbs can be used to assess the
%    awareness level and intentions of pedestrians and drivers.
%                
%    Equivalently, some works can predict vehicle lane changes \cite{Kim2017},
%    risk at intersections \cite{Lefevre2012} or pedestrian intentions
%    \cite{Kohler2015,Fang2017} applying machine learning and bayesian
%    techniques to data acquired by exteroceptive sensors. 
%    Further sensor developments in resolution and accuracy potentially lead
%    to much powerful intention predictors, that will result in safer and more
%     fluent driving algorithms.
    
%\end{itemize}


\subsubsection{Implantation challenges}

The final goal of research in automated driving is to bring technologies to
market, either for private customers or for shared applications (automated 
fleets). Commercialization and implantation is feasible only if products 
fulfill certain scalability, costs, and durability requirements.
Some sensors are among the most expensive and fragile components of a vehicle,
so their implantation is a key factor in the development of automated driving
vehicles.

\textbf{Production scalability and costs}. 
Mature technologies as visible light cameras and radars have already scaled up 
their production and reduced costs so that every vehicle can equip them without
a significant impact on its price. This remains a challenge for LiDAR devices
and other breakthrough technologies.
%The cost that is considered acceptable for a production vehicle, however, 
%varies depending on the scenario. For private owned vehicles it must be kept at 
%a rather small fraction of vehicle cost. Commercial fleet vehicles can afford 
%higher prices because an Automated Driving system can compensate other costs 
%(i.e. reducing the number of drivers).
It is difficult to get an exact estimation of an acceptable cost, 
it depends on the use of the vehicle (private or commercial fleet)
in between many other factors. 
For a discussion on costs and impact of Automated Mobility services, see
\cite{Bosch2018}.

\textbf{Durability and tolerance to failure}.
The perception system of an Automated Vehicle must work for long
periods under harsh conditions, as the rest of critical components in a vehicle.
%In case of failure, redundancy and emergency fallback routines must be able to 
%mitigate the problem and drive the vehicle to a safe state, but it is a 
%threat that has to be avoided.
Low mean-time between-failures (as for mechanical LiDARs), external factors 
(a stone chip at high speeds can damage a sensor) or intentional attacks \cite{Petit2015a}
are important factors to consider in the future sensors technologies.

%Mechanical LiDARs represent the clearest example of reliability issues: their 
%mean-time-between-failures (MTF)(1,000 to 3,000 hours) is too low for 
%automotive industry requirements (at least 13,000 hours) \cite{Hampstead2018}.
%External factors can affect sensors. A stone chip can crack a glass while 
%driving at high speeds ways, affecting the performance of video cameras and 
%possibly LiDARs even when protected behind a plastic or glass layer. 
%Another kind of external factors are intentional attacks \cite{Petit2015a}: 
%radars can be jammed and a camera or a LiDAR can be blinded by the appropriate
%source of light.


\subsection{Commercial initiatives}

In the last decade the automotive market has grown the offer and complexity
of ADAS \cite{Perez2016}. The most requested ADAS in 2009
\cite{Frost&Sullivan2010} were Anti-lock braking system and Parking Assistance
by Warning (SAE Level 0). Today most advanced cars equip an ensemble of
ADAS that place them between SAE Levels 2 and 3. 
%Some examples are Tesla AutoPilot and Audi JamAssist, able to drive the vehicle 
%under user supervision in specific scenarios.  

\subsubsection{OEMs in Automated Driving}
\label{sec:oem-ad}

Around 2015 most important OEMs decided to take serious initiatives towards
bringing high and fully Automated Driving (SAE Levels 4 and 5) to the market.
In order to accelerate their roadmaps, they established alliances with
technological companies startups and technology/research centers that
can hint about their approach to Automated Driving.

%It is difficult to get information about their research, further than public
%demonstrations and marketing products. However, these alliances with other
%companies, startups and technology/research centers are easier to trace and can
%hint about their approach to Automated Driving.


%\begin{figure*}[t]
%    \centering
%    \includegraphics[width=0.9\textwidth]{"img/OEM_jp_trim"}
%    \caption{OEM projects and alliances in Automated Driving}
%    \label{fig:oem-ad}
%\end{figure*}

\begin{table*}[h]
    \ra{1.3}
    \caption{OEM projects and alliances in Automated Driving}
    \label{tab:oem-ad}
    \begin{tabular}{@{}p{1.5cm}p{2.5cm}p{3.0cm}r*{2}{p{2.5cm}}p{2cm}p{3cm}@{}} 
        %p{2cm}p{2cm}p{2cm}@{}}
        \toprule
        \textbf{OEM} & \textbf{Test site} & \textbf{Technologies} &
        \textbf{Since} & \textbf{Collaborations} & \textbf{Forecast} & 
        \textbf{Test fleet} 
        \\    
        \cmidrule(r{4pt}){1-1} \cmidrule(l){2-7}
        Ford & Detroit, Arizona \& California (USA) & LiDAR, and mapping  & 
        $\sim$2016 & Argo, Velodyne, SAIPS, civilmaps. & Level 4 (2021) & 
        Fusion Hybrid ($\sim$100 by 2018)
        \\
        GM & Detroit, S. Francisco \& Scottsdale (USA) & LiDAR, 
        HD map, radar, camera & $\sim$2016 & Waymo and 
        Jaguar-Land Rover & 2020 (Fortune) & $\sim$50 vehicles (2017) 
        \\
        Renault-Nissan & Japan, USA \& China & Front radar, LiDAR. 
        Speed/steering control & $\sim$2017 & Transdev, Microsoft. & 
        \parbox[t][][t]{2cm}{$<$2030 (Level 5)\\ 2020 (Level 3)} 
        & --- 
        \\
        Daimler & Germany & Vision, data fusion, radar. & 2015 & 
        Bosch & 2020 & Commercial cars (Level 2) 
        \\
        Volkswagen-Audi Group & Germany & LiDAR, data fusion, adaptive cruise 
        control, Trafic Jam Assist, self-parking & 2015 & 
        \parbox[t][][t]{2.2cm}{Delphi (2015) \\ Aurora (2017)} & 2025 (Level 4) 
        & Commercial cars (Level 3, Traffic Jams) 
        \\ 
        BMW & Germany, China & Vision, LiDAR, DGPS & 2011 & Intel, Baidu, HERE 
        & 2022 (Level 5) & 
        Commercial cars (Level 2) 
        \\
        Waymo & California (USA) & LiDAR, vision system, radar, data fusion, 
        RT 
        Path plan.. & 2010 & Fiat-Chrysler,  Velodyne. & --- & 100 autonomous 
        Pacifica minivans  
        \\
        Volvo & Sweden. \& Uber: San Francisco, Pittsburgh  & Vision, LiDAR, 
        GPS, 
        V2I & 2011 & Uber (U.S), Autoliv (Sweden)  & $\sim$2020 & Commercial 
        cars 
        (Level 2) 
        \\
        Tesla & USA & Camera, radar, AI & $\sim$2015 & Apple, Mobileye and 
        Nvidia 
        & 
        $\sim$2020 Level 5)  & Commercial cars (Level 2) \\
        Hyundai & South Korea & AI, LiDAR, Camera & 2014 & KIA, Aurora & 
        \parbox[t][][t]{2.2cm}{AD Level 3.\\2020 (Highways).\\2030 (city 
        streets)} & --- 
        \\
        \bottomrule  
    \end{tabular}
\end{table*}

Table \ref{tab:oem-ad} shows a resume of the most promising research and 
collaboration for Automated Driving with OEMs involved. The most relevant works 
are leaded by Ford, GM and Daimler. However, the influences of Waymo and Tesla, 
and the alliances with other actors (NVIDIA, Apple or Intel-Mobileye) plays an
important role in this automated race. Another important consideration is that 
most of the OEMs started their Automated program just two years ago.
% Other OEMs have also 
%their focus in these technologies, but in this review only mention some of the 
%most promising.

\subsection{Conclusions}

%A survey about one of the most critical sensor parts in automated vehicles has 
%been presented. Choosing the sensors configuration of an automated vehicle can 
%be challenging. Each sensor has different strengths and weaknesses regarding
%the type of information acquired, overall accuracy and quality and working conditions. 
%Usually the best solution consists in getting 
%information from more than one type of sensor and fuse their information. It 
%creates a more robust  perception system as it has more data variety from 
%different sources and also increases the safety as the fault of an specific 
%sensor can be managed by another one. As all of these advantages, it also 
%presents some challenges like finding a proper way to calibrate all those 
%sensors, or making good decisions when two sensors have different outputs.
%, or in other words, make a proper fusion of all the information.
%Computational power and energy consumption are also related with the sensors choice. The more 
%data the vehicle gets, the more computational power it will be needed with it 
%corresponding energy consumption. The challenge is: if the vehicle has more 
%sensors, the final cost will increase and therefore it will be more difficult  
%to produce those vehicles.
Choosing the sensors configuration of an automated vehicle can be challenging.
Each sensor has different strengths and weaknesses regarding
the type of information acquired, overall accuracy and quality and working conditions. 
This survey has reviewed the most popular sensors technologies, 
describing their characteristics and how they are applied to get useful information 
to solve the main perception competences.
The relevant works and demos section provide a good perspective of how different 
manufacturers and research groups do perception tasks and which kind of sensors 
they use for that purpose. 
Finally, the section \ref{sec:oem-ad} can form an intuition about where are the 
manufacturers going in the autonomous vehicle process and how are they planning 
to get there.


%As the technology advances, new sensors with new specifications and charasteristics appears in the market [research with new platforms]