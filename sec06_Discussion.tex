
The last section of this article presents a discussion of the future challenges 
for sensors and perception systems in new automated vehicles, both from the 
technical and implantation point of view. A description of the next 
commercial initiatives and OEMs forecasts is shown followed by the final 
conclusions.


\subsection{Future challenges}

As shown in sections \ref{sec:02-sensors} and \ref{sec:03-problemsapplications} there exist many works that solve the most important perception competences, based on different types of sensors and with a large variety of algorithms. However, there still exist some challenges that need to be solved in order to achieve a functional secure automated vehicle.

\subsubsection{Technical challenges}

Sensor setups in Automated Driving are usually focused on the areas relevant 
for the usual driving tasks: long range ahead from the vehicle, also long but 
no so much behind, and short-mid range in the laterals. This covers all the 
behavioral competences enumerated in section 
\ref{sec:03-problemsapplications}. 
But plain 360 degree coverage can be insufficient for a commercial system
expected to work in the real world. Some specific challenges like 
critical distances (too short and too large), occlusions or bad weather 
conditions do not have a proper solution yet.

%\begin{itemize}
\textbf{Very short distance}, including close to or below the car.
    A person, animal or object right below the vehicle or intersecting 
    the path of the wheels represents a safety issue. While most situations
    can be anticipated when the element approaches the vehicle from the
    distance, it is not the case right before starting the vehicle, 
    while executing high accuracy maneuvers in certain conditions 
    (close to people or other moving elements).  
    
    This problem be tackled by adding redundant sensors in specific positions.
    Some commercial 360-degree-view parking systems \cite{gandhi2006vehicle} 
    already seem to have a good visibility of vehicle immediate surroundings.
    Waymo claims to have a special LiDAR monitoring this area and even below
    the vehicle.         
    In the future there will be a need of devices specific for this task 
    that can make automated vehicles even safer than human drivers in such 
    situations.
      
\textbf{Very long distance} 
    Detection and classification at 200 meters is an open issue.
    The resolution needed to monitor the whole area with the required 
    accuracy is overwhelming. Among current approaches, Ambarella 
    integrates a Ultra High Resolution camera (4k video, probably 8k) 
    (cited in \ref{sec:04-relevantdemos}), that is claimed to be enough for 
    discerning small objects at that target distance. The biggest problem
    is the raw computational power required to process so much information.
    
    A different approach could consist on sensors able to determine 
    saliency (a common term in artificial vision 
    \cite{Zhang2016a,Palazzi2018,Duthon2016} to name relevancy or importance), 
    so that other adaptive sensors can focus on that area, increasing
    resolution, frame rate or accuracy. 
    This reminds in some way the saccadic vision system used in Dickmann's 
    pioneer vehicles \cite{Dickmanns1987,Gregor2002}, where a rotating 
    platform where used to take images of areas of interest.
    
    While the last part is something feasible today, for example using solid
    state LiDARs capable of random or adaptive sampling, the real challenge
    resides in the saliency part: design a sensor that can determine that 
    something very far away can be relevant, without falling in the
    brute-force approach.

\textbf{Environmental and weather conditions}    
    Section \ref{sec:02-sensors} summarizes the suitability of common 
    technologies under different conditions, some of which surpass human 
    capacities. 
    This is an always active field of research, because perception 
    can solve what humans compensate with reasoning. Following the road 
    when most marks are covered by snow, improving detection under heavy 
    rain or dense fog are examples of problems that can be solved at sensing
    level without requiring further efforts on processing algorithms.
        
\textbf{Intention detection}        
    Human drivers infer the intentions of pedestrians and vehicles 
    combining subtle sensory hints with a mind model of the observed actor,
    and taking into account the context.
    Head pose, gaze direction, position of hands, or motion of arms and 
    legs can be used to assess the awareness of a pedestrian or a driver
    about what is going on in a certain road area but are also indicative 
    of intended motion direction. The steering angle of front wheels can
    hint if a vehicle is going to leave a roundabout or stay inside.
            
    Many works can predict vehicle lane changes \cite{Kim2017}, risk at
    intersections \cite{Lefevre2012} or pedestrian intentions
    \cite{Kohler2015,Fang2017} applying machine learning and bayesian
    techniques to data acquired by exteroceptive sensors. Its accuracy 
    is sometimes limited by resolution and accuracy of sensor technologies,
    which cannot capture the aforementioned hints.
    Further sensor developments can lead to much powerful intention 
    predictors, that will result in safer and more fluent driving 
    algorithms.
    
%\end{itemize}


\subsubsection{Implantation challenges}

The final goal of research in automated driving is to bring technologies to
market, either for private customers or for shared applications (automated 
fleets). Commercialization and implantation is feasible only if products 
fulfill certain scalability, costs, and durability requirements. 
Some sensors are among the most expensive and fragile components of a vehicle,
so their implantation is a key factor in the development of automated driving
vehicles.

\textbf{Production scalability and costs}. 
Mature technologies as visible light cameras and radars have already scaled up 
their production and reduced costs so that every vehicle can equip them without
a significant impact on its price. This remains a challenge for LiDAR devices
and other breakthrough technologies.
The cost that is considered acceptable for a production vehicle, however, 
varies depending on the scenario. For private owned vehicles it must be kept at 
a rather small fraction of vehicle cost. In the case of fleet vehicles with a 
commercial use it can be higher because an Automated Driving system can 
compensate other costs (i.e. to reduce the number of drivers).
It is difficult to provide an exact estimation, because Automated Driving can 
have a significant effect in mobility, economy an other factors. 
For a discussion on costs and impact of Automated Mobility services, see
\cite{Bosch2018}.

\textbf{Durability and tolerance to failure}.
The perception system of an Automated Vehicle must work for long
periods under harsh conditions, as the rest of critical components in a vehicle.
In case of failure, redundancy and emergency fallback routines must be able to 
mitigate the problem and drive the vehicle to a safe state, but it is a 
threat that has to be avoided.

Mechanical LiDARs have been around for about a decade, but they are still
highly specialized devices mostly used with research purposes.
The controversial CEO of Quanergy claims \cite{Hampstead2018} that mechanical
LiDAR sensors are unsuitable for commercial automotive applications because the
mean-time-between-failures (MTF) is
``between 1,000 to 3,000 hours of operation'' on the rotating 
components is far too low for industry requirements. And that automakers want 
an MTF of at least 13,000 hours.
Solid state LiDAR based on vibrating micro-mirror (MEMS) can reduce costs and 
increase laser resolution but still have mobile parts (micro-mirror), which
makes them susceptible to vehicle vibrations and more fragile.

External factors can affect sensors. A stone chip can crack a glass while 
driving at high speeds ways, and this is something that can affect the 
performance of video cameras and possibly LiDARs even when protected behind
a plastic or glass layer. It is desirable to create or improve sensing 
technology that can minimize the impact of that kind of events.
Another kind of external factors are intentional attacks. A radar can be jammed,
and a camera or a LiDAR can be blinded by the appropriate source of light.
Future sensors will have to be robust against external interferences.

\subsection{Commercial initiatives}

At the end of the previous decade (2009), the most requested ADAS
\cite{Frost&Sullivan2010} were the Anti-lock braking system and the Parking 
Assistance by Warning. These systems cannot be classified
higher than SAE Level 0.

In the last decade the automotive market have grown the offer and complexity
of ADAS \cite{Perez2016}. The most advanced cars today equip an ensemble of 
ADAS that place them somewhere between SAE Levels 2 and 3 in the scale of Automated
Driving. Some examples are Tesla AutoPilot and Audi JamAssist, able to
drive the vehicle under user supervision in specific scenarios.

The fact that manufacturers are starting to talk about SAE Levels 2 and 3
is a sign that ADAS technology is completely integrated into the market and, thus, are not a subject of research per se anymore.  

\subsubsection{OEMs in Automated Driving}
\label{sec:oem-ad}

Back in 2010, most traditional vehicle manufacturers did not consider Automated
Driving as a priority. In the last years, the achievements of technological 
pioneers (Google/Waymo, Uber, Tesla among others) gave place to early alliances
between those companies --that had the technology-- and OEMs --that had the
platform, the experience and the market.

By the end of 2018, all the important brands are involved in a race for creating
the first highly automated vehicle (SAE Levels 4 and 5). It is difficult to
get information about their research further than public demonstrations and
marketing products. However, their alliances with other companies, startups
and technology/research centers are easier to trace and can hint about their 
approach to Automated Driving.


Figure \ref{fig:oem-ad} shows a resume of the most important research and collaboration for Automated Driving with OEMs involved. The most relevant works are leaded by Ford, GM and Daimler, based on LiDARs, cameras and radar technologies. However, the influences of Waymo and Tesla, and the alliances with other actors (i.e.: NVIDIA, Apple or Intel-Mobileye-) plays an important role in this automated race. Another important consideration is that most the OEMs started their Automated program just two years ago. Other OEMs have also their focus in these technologies, but in this review only mention some of the most promising.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{"img/OEM_jp_trim"}
    \caption{OEM projects and alliances in Automated Driving}
    \label{fig:oem-ad}
\end{figure*}

%\begin{table}
%    \caption{OEMs with Automated Driving projects}
%    \label{tab:oems}
%    \begin{tabular}{ | l | l | l | l | l | l | l | }
%    \textbf{OEMs} & \textbf{Test side} & \textbf{Technologies} & \textbf{Since}
%     & \textbf{Collaborations} & \textbf{Forecast} & \textbf{Test fleet} 
%    \\
%    \hline
%    Ford & Detroit, Arizona \& California (U.S.A.) & AI, LiDAR, and mapping  & 
%    \~2016 & Argo, Velodyne, SAIPS, civilmaps. & Level 4 (2021) & Fusion 
%Hybrid 
%    sedans \~100 by 2018 
%    \\
%    GM & Detroit San Francisco \& Scottsdale, Arizona (U.S.A.) & Lidar, very 
%    accurance map, radar, camera & \~2016 & Google’s Waymo and Jaguar-Land 
%    Rover & before 2020 (Fortune) & Fifty vehicles have been built by GM 
%(2017) 
%    \\
%    Renault-Nissan & Japan, EE.UU. \& China & Maintains speed, Steering 
%    control, Front radar, Lidars & \~2017 & Transdev, Microsoft and TechCrunch 
%    (from Oath)  & Fully autonomous car within the next 10 years. Level 3 -> 
%    2020 & --- 
%    \\
%    Daimler & Germany & Vision, data fusion, radar. & 2015 (Truck \& F015) & 
%    Bosch & 2020 & Commercial cars (level 2) 
%    \\
%    Volkswagen Group (Audi) & Germany & Lidar, data fusion, adaptive cruise 
%    control, self-parking \& TJA  & 2015 & Audi -> Delphi (2015);  Aurora 
%    (2017) & 2025 (level 4) & Commercial cars (level 3 -> Traffic Jams) 
%    \\ 
%    BMW & Germany, China & Vision, lidar, DGPS & 2011 & Intel,  and With Baidu 
%    \& Nokia’s HERE   & Level 5 autonomous car on the road by 2022. & 
%    Commercial cars (level 2) 
%    \\
%    Waymo & California (U.S.A.) & Lidar, vision system, radar, data fusion, RT 
%    Path plan.. & 2010 & Fiat-Chrysler,  Velodyne. & --- & 100 autonomous 
%    Pacifica minivans  
%    \\
%    Volvo & Sweden. \& Uber: San Francisco, Pittsburgh  & Vision, lidar, GPS, 
%    V2I & 2011 & Uber (U.S), Autoliv (Sweden)  & \~2020 & Commercial cars 
%    (level 2) 
%    \\
%    Tesla & U.S.A. & Camera, radar, AI & \~2015 & Apple, Mobileye and Nvidia & 
%    \~Full automated 2020  & Commercial cars (level 2) \\
%    Hyundai & South Korea & AI, LiDAR, Camera & 2014 & KIA, Aurora & AD Level 
%    3-> Highways by 2020 and to city streets by 2030 & --- 
%    \\
%    \end{tabular}
%\end{table}

\subsection{Conclusions}

A survey about one of the most critical sensor parts in automated vehicles has 
been presented. Choosing the sensors configuration of an automated vehicle can 
be challenging. Each sensor has different strengths and weaknesses regarding
the type of information acquired, overall accuracy and quality and working 
conditions. Usually the best solution consists in getting 
information from more than one type of sensor and fuse their information. It 
creates a more robust  perception system as it has more data variety from 
different sources and also increases the safety as the fault of an specific 
sensor can be managed by another one. As all of these advantages, it also 
presents some challenges like finding a proper way to calibrate all those 
sensors, or making good decisions when two sensors have different outputs, or 
in other words, make a proper fusion of all the information. Computational 
power and energy consumption are also related with the sensors choice. The more 
data the vehicle gets, the more computational power it will be needed with it 
corresponding energy consumption. The challenge is: if the vehicle has more 
sensors, the final cost will increase and therefore it will be more difficult  
to produce those vehicles.

This survey has reviewed the most popular sensors technologies, 
describing their characteristics and how are they applied to get information 
useful to solve the main perception competences. It gives a global vision of 
different sensor configurations and techniques to obtain useful information 
from the environment.

The relevant works and demos provide a good perspective of how different 
manufacturers and research groups do perception tasks and which kind of sensors 
they use for that purpose. 
Finally, the section \ref{sec:oem-ad} can form an intuition about where are the 
manufacturers going in the autonomous vehicle process and how are they planning 
to get there.


%As the technology advances, new sensors with new specifications and charasteristics appears in the market [research with new platforms]