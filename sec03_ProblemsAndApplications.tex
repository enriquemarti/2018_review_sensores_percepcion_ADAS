
This section analyzes the state of the art in perception systems for Automated
Driving. A set of behavioral competences is identified, followed
by a systematic literature review that analyzes the 
solutions for each category, organized by sensor technology.

\subsection{Behavioral competencies}

Behavioral competencies in Automated Driving ``refers to the ability of an 
Automated Vehicle to operate in the traffic conditions that it will regularly
encounter" \cite{Nowakowski2015}. The NHTSA defined a set of 28 core 
competencies for normal driving \cite{NHTSA2016}, that have been augmented to a 
total of 47 by Waymo \cite{Waymo2017} in their internal tests.
Table \ref{tab:behavioral-competences} selects a subset of those behavioral
competencies and arranges them in categories that are used to structure the state of the art in
perception algorithms in a purpose oriented approach.

\begin{table*}[t] %[H]
    \caption{Behavioral competences and relation with information taxonomy 
        (see Table \ref{tab:info-taxonomy})}
    \label{tab:behavioral-competences}
    \begin{tabular*}{\textwidth}{m{4cm} l p{11cm}}%{XlL}
        \hline %\toprule
        \textbf{Competence}	& \textbf{Information type} & \textbf{Behavior}	
        \\
        \hline %\midrule
        \multirow{4}{4cm}{Automatic Traffic Sign Detection
            and Recognition (TSDR)}
        & 8    & Detect Speed Limit Changes, Speed Advisories, Traffic Signals 
        and Stop/Yield Signs \\
        & 8    & Detect Access Restrictions (One-Way, No Turn, Ramps, etc.) \\
        & 8    & Detect Temporary Traffic Control Devices \\
        & 6, 8 & Detect Passing and No Passing Zones  \\
        \hline %\midrule
        \multirow{4}{*}{Perception of the environment}
        & 8 & Detect Lines \\
        & 6, 8 & Detect Detours  \\
        & 6 & Detect faded/missing roadway markings, signs and other 
        temporary changes in traffic patterns \\
        & 9 & Perception under weather or lighting conditions 
        outside 
        vehicle’s capability (e.g. rainstorm) \\
        \hline %\midrule
        \multirow{6}{4cm}{Vehicles, pedestrians and other obstacles 
            detection}
        & 10, 12, 13 & Detect Non-Collision Safety Situations (e.g. vehicle 
        doors ajar) \\
        & 10, 11, 12, 13 & Detect Stopped Vehicles, Emergency Vehicles, Lead 
        Vehicle, Motorcyclists, School Buses \\
        & 6(, 1)  & Detect Static Obstacles in the Path of the Vehicle \\
        & 6, 8, 9, 10, 11, 12 & Detect Pedestrians and Bicyclists at 
        Intersections, Crosswalks and in the Road. \\
        & 10, 11, 12 & Detect Animals \\
        & 10, 12, 13 & Detect instructions from Work Zones and People 
        Directing Traffic in Unplanned or Planned Events, Police/First 
        Responder Controlling Traffic, Construction Zone Workers Controlling, 
        Citizens Directing Traffic After a Crash (Overriding or Acting as 
        Traffic Control Device) \\
        
        \hline %\bottomrule
    \end{tabular*}
\end{table*}

This set of competences represents the link between perception and decision
(planning), as a counterpart to the information taxonomy presented in the
previous section (Table \ref{tab:info-taxonomy}), which linked sensors and 
perception algorithms. 
Both tables can be combined to evaluate the suitability of sensor technologies
for creating some set of Automated Driving capacities.

The next subsections describe the state of the art in perception techniques for
the three identified categories of behavioral competencies.

\subsection{Automatic Traffic Sign Detection and Recognition (TSDR)} 
%[Definition? ]. 
Traffic signs are visual devices with a well defined aspect, that transmit a 
clear and precise piece of information about traffic regulation, warnings about
factors affecting driving and other informative statements. The spatial and
temporal scopes of applicability are also defined in the sign, either 
explicitly or implicitly.
Acquiring information from road traffic signs involves two major tasks: 
Traffic Sign Detection (TSD) which consists on finding the location, 
orientation and size of traffic signs in natural scene images, and Traffic Sign 
Recognition (TDR) or classifying the detected traffic signs into types 
and categories in order to extract the information that they are providing to 
drivers.
%Automatic TSDR has two different applications: Real time detection and
%recognition is used in ADAS or autonomous driving, and automatic road traffic 
%sign mapping systems are used for generating a database of traffic signs 
%of a certain area. This last application does not need to work on real-time. 

%Traffic signs are designed for human visual perception. Two sensors are mostly
%used for these tasks: monocular cameras in different configurations 
%(single camera, multiple focal or multiple cameras) and LiDAR sensors.
Below are shown the most relevant solutions according to the type of sensor 
and the technology used.

\subsubsection{Camera based solutions}
Cameras are the most common sensor for TSDR. They can be used for TSR, TSD or both at the same time.
%As an example of TSR, \cite{frejlichowski2015application} proposes a method 
%based on the Polar-Fourier Grayscale Descriptor, which applies the information 
%about silhouette and intensity of an object. In \cite{gao2015learning} a 
%learning method based on a histogram intersection kernel is used to quantize 
%features, that are encoded in a look-up table.
As an example of TSR, \cite{frejlichowski2015application} proposes a method 
based on the Polar-Fourier Grayscale Descriptor, and \cite{gao2015learning} a 
learning method based on a histogram intersection kernel.
%For TSD, \cite{zhang2017real} proposes a method based on a fast Convolutional 
%Neural Network (CNN) inspired in the YOLOv2 network. This algorithm 
%can detect the position of the traffic sign and classify it into Mandatory 
%(blue colored), Danger (triangle shaped) and Prohibitory (red circle). 
%\cite{villalon2017traffic} detects stop and yield signs with a statistical 
%template built using color information in different color spaces (YCbCR and ErEgEb).
For TSD, \cite{zhang2017real} proposes a method based on a fast Convolutional 
Neural Network (CNN) inspired in the YOLOv2 network. This algorithm 
can detect the position of the traffic sign and classify it according to its shape. 
\cite{villalon2017traffic} detects stop and yield signs with a statistical 
template built using color information in different color spaces (YCbCR and ErEgEb).
TSD techniques can also be applied to traffic light detection, as in 
\cite{hosseinyalamdary2017bayesian}, where a Bayesian inference framework to 
detect and map traffic lights is described. A different approach is proposed by 
\cite{gu2011traffic} that uses a dual focal camera system composed of a wide 
angle camera and a telephoto camera which is moved by mirrors 
in order to get higher quality images of the traffic signs.
%Camera sensors can also perform TSD and TSR tasks as is 
%shown in the following works. \cite{miyata2017automatic} uses local binary 
%pattern method for detecting speed signals and a neural network for the 
%recognition of the numbers of the sped limit sign. \cite{yang2016towards} 
%presents a fast detection method based on traffic sign proposal extraction and 
%classification built upon a color probability model and a color Histogram of
%Oriented Gradients (HOG) combined with a convolutional neural network to
%further classify the detected signs into subclasses.
%\cite{wali2015automatic} performs detection and recognition tasks using a RGB 
%colour segmentation and shape matching followed by support vector machine (SVM) 
%classifier. 
Camera sensors can also perform TSD and TSR tasks as is 
shown in the following works where first the signals are detected attending to
their color or shape, and then they are classified using machine learning techniques (CNN or SVM)
\cite{miyata2017automatic, yang2016towards, wali2015automatic}. In \cite{timofte2014multi}
a system composed by eight roof-mounted cameras which takes images every meter
perform offline TDSR to create a database with more than 13,000 traffic signs annotations


\subsubsection{LiDAR based solutions}
LiDAR sensors have been used for TSD. Their 3D perception capabilities are 
usefult to determine the position of the sign and its shape, and can also use 
the intensity of reflected light to improve detection accuracy based on the
high reflectivity of traffic signs. \cite{gargoum2017automated} 
performs detection in three steps: first the point cloud is filtered by 
laser reflection intensity, then a clustering algorithm is used to detect 
potential candidates, followed by a filtering step based on the lateral 
position, elevation and geometry that extracts the signs. 
\cite{weng2016road} goes one step further and makes a primary 
classification attending to the sign shape (rectangular, triangular and 
circular).

\subsubsection{Sensors Fusion solutions}
A system that combines LiDAR and Cameras can improve the sign detection and 
recognition as it has the advantages and the information of both sources. 
\cite{zhou2014lidar} trains a SVM with 10 variables: 9 of different color 
spaces provided by the camera (RGB, HSV, CIEL*a*b*) plus reflection intensity 
observed by LiDAR. After verifying the 3D geometry of detected signs, a linear
SVM classifier is applied to HOG features.
\cite{guan2018robust} method detects traffic signs in LiDAR point clouds
using prior knowledge of road width, pole height, and traffic sign reflectance, 
geometry and size. Traffic sign images are normalized to perform classification 
based on a supervised Gaussian–Bernoulli deep Boltzmann machine model.


%Como el resto no tiene un summary lo quito

%\subsubsection{Summary of sensors and their use for TSDR}
%\begin{itemize}%[leftmargin=20mm,labelsep=5.8mm]
%\item \textbf{LiDAR:} Filtering for intensity (traffic signal has high Reflectance), 3D position and shape. Clustering for sign candidates
%\item \textbf{Camera}
%\subitem \textbf{Monocular B/W:} Feature extraction.
%\subitem \textbf{Monocular Color:} Color information. Classification with 
%different methods: SVM, CNN, ..., or both, detection and classification.
%\subitem \textbf{Multiple cameras:} Mapping signals in roads, 3d position.
%\subitem \textbf{Multiple focal:} Short focal for detection and long focal for 
%higher quality sign image.
%\item \textbf{Fusion LiDAR and Camera:} Using LiDAR intensity and 3d position 
%and shape information for detecting, and camera color and shape information for 
%recognizing
%\end{itemize}

\subsection{Perception of the environment}
The purpose of this competence is to characterize and describe the road, which
represents the most direct piece of environment of a vehicle. This 
involves two different aspects: characterize road surface geometry and detect road marks (lanes
and complements traffic signs as stops, turns or stopping lines).

%) and the lane marks that delimit 
%those lanes. This information can be used in real-time, or can be combined with
%a GNSS-based high accuracy localization device to generate a detailed road map
%that can be used by other vehicles.
%The second task is related to the localization of the vehicle in the road. It 
%can be either on map generated by the first map, or in unknown roads. 

%Reliability is a critical factor for this competence, since missing a mark and
%false detections can cause an Automated Vehicle to change lanes inadvertently 
%or even go out of the road. Guaranteeing the quality of mark detection is a 
%challenging task not only due to varying lane marks (heterogenous widths, 
%faded marks), but also due to weather and environmental conditions: 
%Different light intensity (sunny day, night, shadows), different 
%visibility conditions (fog, snow, rain) and different reflectivity of the road
%(wet or dry asphalt). 

Road marks, as traffic signs, are designed to be detected and 
correctly interpreted by human drivers under a wide variety of external 
conditions. This is achieved using reflective painting and high contrast 
colors. Cameras and less frequently LiDARs have been used for detecting them.
Road geometry description has been approached using cameras, LiDARs and radars.

In the following lines, the most relevant works about this topic are presented, 
organized by the type of sensor they use.

%Semantic representation of the lanes
%Using the reflectivity information recorded by lidars enable the our system to detect lane markers in the presence of shadows, against direct sunlight and even at night

\subsubsection{Camera based solutions}
can be grouped in three categories depending on the specific sensor 
configuration.

\textbf{Single Monocular.}
Using only one 
camera looking at the road in front of the vehicle it is possible to estimate 
its shape and lanes, the position of the vehicle in the road and  
detect road marks. A survey of the most 
relevant algorithms used for this purpose, mainly for camera sensors is 
presented in \cite{hillel2014recent}.

\textbf{Multiple Monocular cameras.} 
Some works \cite{lee2017avm, kum2013lane} arrange multiple cameras 
around the vehicle (typically four, one on each side) to get 360-degree 
visual coverage of the surroundings. 
A different configuration is used in \cite{Ieng2003}, where two lateral cameras
are used to localize the vehicle. 

\textbf{Binocular or Stereo.} 
The main advantage of binocular cameras is their 3D perception capabilities.
It makes possible to detect the ground plane and road boundaries 
\cite{schreiber2013laneloc, ozgunalp2017multiple}, improving road mark 
detection. 

\subsubsection{LiDAR based solutions}
Main application of LiDARs in road perception is related with detecting the 
ground plane and road limits, as well as detecting obstacles that could occlude 
parts of the road.
In recent works, LiDAR based solutions also take advantage of the higher 
reflectivity of road marks with respect to the pavement (gray and black 
material) to detect lane \cite{yang2012automated, li2013new} and
pavement markers \cite{Zhang2016}.
Poor road maintenance can affect markers reflectivity to the point of making 
them undetectable by LiDAR. This can be solved by fusing LiDAR 
data with cameras able to perceive non reflective lane marks \cite{lee2017avm}.
Some works use a 2D LiDAR sensor to extract road geometry and road marks 
\cite{nie2012camera, kim2015lane}.

%In some works LiDARs are used to detect road boundaries and map them into images in order to get training data for segmentation algorithms Used to classify images. \cite{}

\subsubsection{Radar based solutions}
%The main advantage of Radar sensors is their ability to work in all weather 
%conditions (darkness, rain, fog, snow, etc). But due to the poor image 
%contrast 
%of these types of sensors, the only usefull information for that competence 
%that can be extracted is the position of the obstacles (in order to know 
%oclusions of the road) and the road itself. 
Radars have been used to determine road geometry based on the principle that the
road acts as a mirror for the sensor, returning a very small amount of the 
emitted power, while the sides of the roads return a slightly higher 
amount of power. Road limits have been estimated with a
maximum error of half a lane at zero distance from the host vehicle and less 
than one lane width at 50 meters distance. This information can be fused with
camera images to improve both detections 
\cite{kaliyaperumal2001algorithm, ma2000simultaneous, Janda2013}.

\subsection{Vehicles, pedestrians and other obstacles detection}
%This section reviews the use of different sensors for detecting other elements 
%in the road, including vehicles, pedestrians and any other kind of obstacle 
%that may appear such as motorcycles, bicycles, animals, etc. 
%The advantages and disadvantages of sensors for this application are discussed.
This competence involves moving elements that can be in the path of the 
vehicle, so it requires extracting more information. Apart from detection and 
classification, it is also important to determine the position of obstacles 
with respect to the vehicle, their motion direction, speed, and 
future intentions when possible. 
This information will be the input to other systems like path planners 
or collision avoidance systems (reviewed in \cite{mukhtar2015vehicle}).

\subsubsection{Camera based solutions}
Different configurations have been used for camera based obstacle detection, 
includinng single monocular camera, multiple cameras, stereo cameras and 
infrared cameras.

Cameras can be placed in different locations. The front of the vehicle is 
the most common placement since the most critical obstacles will be in front of 
the vehicle, but many works explored other positions in order to increase the
FoV. 
A camera placed on the side-view mirror, in the 
passengers window \cite{chang2008real} or looking backwards \cite{liu2007rear}
can detect and track vehicles trying to overtake the ego-vehicle, 
improving the decision of lane change maneuvers \cite{alonso2008lane, 
song2007lateral, blanc2007larasidecam}. 
An omnidirectional camera mounted
on the top of the vehicle has been used in \cite{gandhi2006vehicle}
to detect obstacles and estimate ego-motion.

Stereo cameras are widely used for obstacle detection as they provide 3D 
information of the position of the obstacles. A large review of the different 
algorithms used for this kind of cameras can be found in \cite{bernini2014real}.
FIR cameras are independent of scene illumination and can spot obstacles at
night \cite{olmeda2013pedestrian}. Relevant moving elements (vehicles, 
pedestrians, animals) are usually hot and, thus, easy to detect with FIR 
cameras. However, this sensor has to be complemented with other technologies
as in \cite{krotosky2007color}, since cold obstacles like parked vehicles or
trees can be not perceived.
\cite{sivaraman2013looking} presents and explains in detail several camera
solutions and the algorithms used for detection.

\subsubsection{LiDAR based solutions}
LiDAR technology allows to detect and classify surrounding elements, providing
a very accurate 3D position and its shape. 
As it is an active sensor its performance is not affected by the illumination 
of the scene, so it can work also at night. Several approaches for LiDAR 
obstacle detection are shown in \cite{li2016vehicle}.

\subsubsection{Radar based solutions}
The primary use of automotive radars is detection and tracking of other 
vehicles on the road, thanks to their high accuracy measuring target 
distances and relative speed, long range detection and performance in adverse 
weather conditions \cite{blanc2004obstacle}. 
Because of its low resolution radar detections are
usually fused with other sensors as cameras \cite{garcia2012data} or, in some
works, with LiDAR \cite{gohring2011radar}.

%\textbf{Microphone based solutions:}
%Microphones sensors can be used to detect aproaching vehicles from the rear 
%side, an array of multiple microphones placed in the surroundings of the 
%vehicle can estimate even the direction of these aproaching vehicles 
%\cite{mizumachi2014robust}.

\subsubsection{Multiple sensors fusion solutions}
This competence requires estimating a large number of variables simultaneously,
creating difficulties for any single sensor solution. This is a good scenario
for sensor fusion systems, that can combine the strengths of each sensor to
improve the solution. 
%Almost every possible combination have been tested. 

%The most common combinations are described in the following lines.
Radar and LiDAR fusion \cite{gohring2011radar} increases the precision of 
the speed obtained only with LiDAR and keeps a good position and speed 
estimation quality when radar is unavailable (especially in curvy roads).
Radar and vision fusion techniques use radar information to locate areas of 
interest on the images, which are then processed to detect vehicles and improve 
their position estimation \cite{alessandretti2007vehicle}.
LiDAR and vision sensors are fused in \cite{premebida2007lidar}. Obstacles
are detected and tracked with the LiDAR, and the targets are classified using
a combination of camera and LiDAR detections.

