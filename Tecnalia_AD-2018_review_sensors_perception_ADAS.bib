% This file was created with JabRef 2.10.
% Encoding: UTF-8


@Misc{waymoteam2017,
  Title                    = {{Introducing Waymo's suite of custom-built, self-driving hardware}},

  Author                   = {Waymo team},
  Year                     = {2017},

  Booktitle                = {medium.com},
  Url                      = {https://medium.com/waymo/introducing-waymos-suite-of-custom-built-self-driving-hardware-c47d1714563},
  Urldate                  = {2018-10-06}
}

@Article{alessandretti2007vehicle,
  Title                    = {Vehicle and guard rail detection using radar and vision data fusion},
  Author                   = {Alessandretti, Giancarlo and Broggi, Alberto and Cerri, Pietro},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2007},
  Number                   = {1},
  Pages                    = {95--105},
  Volume                   = {8},

  Publisher                = {IEEE}
}

@Misc{ambarella2018,
  Title                    = {{Ambarella Introduces CV2 4K Computer Vision SoC with CVflow™ Architecture and Stereovision}},

  Author                   = {Ambarella},
  Year                     = {2018},

  Booktitle                = {Ambarella webpage},
  Url                      = {https://www.ambarella.com/news/122/74/Ambarella-Introduces-CV2-4K-Computer-Vision-SoC-with-CVflow-Architecture-and-Stereovision},
  Urldate                  = {2018-10-06}
}

@Article{anderson2013,
  Title                    = {{The intelligent copilot: A constraint-based approach to shared-adaptive control of ground vehicles}},
  Author                   = {Anderson, Sterling J. and Karumanchi, Sisir B. and Iagnemma, Karl and Walker, James M.},
  Journal                  = {IEEE Intelligent Transportation Systems Magazine},
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {45--54},
  Volume                   = {5},

  Abstract                 = {This work presents a new approach to semi-autonomous
 vehicle hazard avoidance and stability control, based on
 the design and selective enforcement of constraints. This
 differs from traditional approaches that rely on the
 planning and tracking of paths and facilitates
 "minimally-invasive" control for human-machine systems.
 Instead of forcing a human operator to follow an
 automation-determined path, the constraint-based approach
 identifies safe homotopies, and allows the operator to
 navigate freely within them, introducing control action
 only as necessary to ensure that the vehicle does not
 violate safety constraints. This method evaluates candidate
 homotopies based on "restrictiveness," rather than
 traditional measures of path goodness, and designs and
 enforces requisite constraints on the human's control
 commands to ensure that the vehicle never leaves the
 controllable subset of a desired homotopy. This paper
 demonstrates the approach in simulation and characterizes
 its effect on human teleoperation of unmanned ground
 vehicles via a 20-user, 600-trial study on an outdoor
 obstacle course. Aggregated across all drivers and
 experiments, the constraintbased control system required an
 average of 43{\%} of the available control authority to
 reduce collision frequency by 78{\%} relative to
 traditional teleoperation, increase average speed by
 26{\%}, and moderate operator steering commands by 34{\%}.},
  Doi                      = {10.1109/MITS.2013.2247796},
  ISBN                     = {1939-1390},
  ISSN                     = {1939-1390},
  Url                      = {http://ieeexplore.ieee.org/document/6507273/}
}

@Article{aqel2016,
  Title                    = {{Review of visual odometry: types, approaches, challenges, and applications.}},
  Author                   = {Aqel, Mohammad O A and Marhaban, Mohammad H and Saripan, M Iqbal and Ismail, Napsiah Bt},
  Journal                  = {SpringerPlus},
  Year                     = {2016},
  Number                   = {1},
  Pages                    = {1897},
  Volume                   = {5},

  Abstract                 = {Accurate localization of a vehicle is a fundamental
 challenge and one of the most important tasks of mobile
 robots. For autonomous navigation, motion tracking, and
 obstacle detection and avoidance, a robot must maintain
 knowledge of its position over time. Vision-based odometry
 is a robust technique utilized for this purpose. It allows
 a vehicle to localize itself robustly by using only a
 stream of images captured by a camera attached to the
 vehicle. This paper presents a review of state-of-the-art
 visual odometry (VO) and its types, approaches,
 applications, and challenges. VO is compared with the most
 common localization sensors and techniques, such as
 inertial navigation systems, global positioning systems,
 and laser sensors. Several areas for future research are
 also highlighted.},
  Doi                      = {10.1186/s40064-016-3573-7},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Aqel et al. - 2016 -
  Review of visual odometry types, approaches, challenges,
  and applications.pdf:pdf},
  ISSN                     = {2193-1801},
  Keywords                 = {Global positioning system,Image stream,Inertial navigation system,Localization sensors,Visual odometry},
  Pmid                     = {27843754},
  Publisher                = {Springer},
  Url                      = {http://www.ncbi.nlm.nih.gov/pubmed/27843754
 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5084145}
}

@Misc{auvsi2018,
  Title                    = {{Ambarella testing its fully autonomous EVA vehicle on the roads of Silicon Valley |}},

  Author                   = {AUVSI},
  Year                     = {2018},

  Booktitle                = {Association for Unmanned Vehicle Systems International},
  Url                      = {https://www.auvsi.org/industry-news/ambarella-testing-its-fully-autonomous-eva-vehicle-roads-silicon-valley},
  Urldate                  = {2018-10-06}
}

@Misc{bosch2018,
  Title                    = {{Cost-based analysis of autonomous mobility services}},

  Author                   = {B{\"{o}}sch, Patrick M. and Becker, Felix and Becker, Henrik and Axhausen, Kay W.},
  Month                    = {may},
  Year                     = {2017},

  Abstract                 = {Fast advances in autonomous driving technology trigger the
 question of suitable operational models for future
 autonomous vehicles. A key determinant of such operational
 models' viability is the competitiveness of their cost
 structures. Using a comprehensive analysis of the
 respective cost structures, this research shows that public
 transportation (in its current form) will only remain
 economically competitive where demand can be bundled to
 larger units. In particular, this applies to dense urban
 areas, where public transportation can be offered at lower
 prices than autonomous taxis (even if pooled) and private
 cars. Wherever substantial bundling is not possible, shared
 and pooled vehicles serve travel demand more efficiently.
 Yet, in contrast to current wisdom, shared fleets may not
 be the most efficient alternative. Higher costs and more
 effort for vehicle cleaning could change the equation.
 Moreover, the results suggest that a substantial share of
 vehicles may remain in private possession and use due to
 their low variable costs. Even more than today, high fixed
 costs of private vehicles will continue to be accepted,
 given the various benefits of a private mobility robot.},
  Booktitle                = {Transport Policy},
  Doi                      = {10.1016/j.tranpol.2017.09.005},
  File                     = {::},
  ISBN                     = {9783033046412},
  ISSN                     = {1879310X},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Pages                    = {76--91},
  Publisher                = {Pergamon},
  Url                      = {https://www.sciencedirect.com/science/article/pii/S0967070X17300811},
  Volume                   = {64}
}

@InProceedings{bak2012,
  Title                    = {Multi-sensor localization - Visual Odometry as a low cost proprioceptive sensor},
  Author                   = {Bak, Adrien and Gruyer, Dominique and Bouchafa, Samia and Aubert, Didier},
  Booktitle                = {2012 15th International IEEE Conference on Intelligent Transportation Systems},
  Year                     = {2012},
  Month                    = {sep},
  Pages                    = {1365--1370},
  Publisher                = {IEEE},

  Doi                      = {10.1109/ITSC.2012.6338771},
  ISBN                     = {978-1-4673-3063-3},
  Url                      = {http://ieeexplore.ieee.org/document/6338771/}
}

@InProceedings{behere2015functional,
  Title                    = {A functional architecture for autonomous driving},
  Author                   = {Behere, Sagar and Torngren, Martin},
  Booktitle                = {Automotive Software Architecture (WASA), 2015 First International Workshop on},
  Year                     = {2015},
  Organization             = {IEEE},
  Pages                    = {3--10}
}

@InProceedings{bender2014,
  Title                    = {{Lanelets: Efficient map representation for autonomous driving}},
  Author                   = {Bender, Philipp and Ziegler, Julius and Stiller, Christoph},
  Booktitle                = {IEEE Intelligent Vehicles Symposium, Proceedings},
  Year                     = {2014},
  Month                    = {jun},
  Pages                    = {420--425},
  Publisher                = {IEEE},

  Abstract                 = {In this paper we propose a highly detailed map for the
 field of autonomous driving. We introduce the notion of
 lanelets to represent the drivable environment under both
 geometrical and topological aspects. Lanelets are atomic,
 inter- connected drivable road segments which may carry
 additional data to describe the static environment. We
 describe the map specification, an example creation process
 as well as the access library libLanelet which is available
 for download. Based on the map, we briefly describe our
 behavioural layer (which we call behaviour generation)
 which is heavily exploiting the proposed map structure.
 Both contributions have been used throughout the autonomous
 journey of the MERCEDES BENZ S 500 INTELLIGENT DRIVE
 following the Bertha Benz Memorial Route in summer 2013.},
  Doi                      = {10.1109/IVS.2014.6856487},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Bender, Ziegler, Stiller -
  2014 - Lanelets Efficient map representation for autonomous
  driving.pdf:pdf},
  ISBN                     = {9781479936380},
  Url                      = {http://ieeexplore.ieee.org/document/6856487/}
}

@InProceedings{bernini2014real,
  Title                    = {Real-time obstacle detection using stereo vision for autonomous ground vehicles: A survey},
  Author                   = {Bernini, Nicola and Bertozzi, Massimo and Castangia, Luca and Patander, Marco and Sabbatelli, Mario},
  Booktitle                = {Intelligent Transportation Systems (ITSC), 2014 IEEE 17th International Conference on},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {873--878}
}

@InProceedings{bertozzi2011,
  Title                    = {{VIAC expedition toward autonomous mobility}},
  Author                   = {Bertozzi, Massimo and Broggi, Alberto and Cardarelli, Elena and Fedriga, Rean Isabella and Mazzei, Luca and Porta, Pier Paolo},
  Booktitle                = {IEEE Robotics and Automation Magazine},
  Year                     = {2011},
  Month                    = {sep},
  Number                   = {3},
  Pages                    = {120--124},
  Volume                   = {18},

  Doi                      = {10.1109/MRA.2011.942490},
  ISSN                     = {10709932},
  Url                      = {http://ieeexplore.ieee.org/document/6016589/}
}

@Article{besbes2015,
  Title                    = {Pedestrian detection in far-infrared daytime images using a hierarchical codebook of SURF},
  Author                   = {Besbes, Bassem and Rogozan, Alexandrina and Rus, Adela Maria and Bensrhair, Abdelaziz and Broggi, Alberto},
  Journal                  = {Sensors (Switzerland)},
  Year                     = {2015},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {8570--8594},
  Volume                   = {15},

  Abstract                 = {One of the main challenges in intelligent vehicles
 concerns pedestrian detection for driving assistance.
 Recent experiments have showed that state-of-the-art
 descriptors provide better performances on the far-infrared
 (FIR) spectrum than on the visible one, even in daytime
 conditions, for pedestrian classification. In this paper,
 we propose a pedestrian detector with on-board FIR camera.
 Our main contribution is the exploitation of the specific
 characteristics of FIR images to design a fast,
 scale-invariant and robust pedestrian detector. Our system
 consists of three modules, each based on speeded-up robust
 feature (SURF) matching. The first module allows generating
 regions-of-interest (ROI), since in FIR images of the
 pedestrian shapes may vary in large scales, but heads
 appear usually as light regions. ROI are detected with a
 high recall rate with the hierarchical codebook of SURF
 features located in head regions. The second module
 consists of pedestrian full-body classification by using
 SVM. This module allows one to enhance the precision with
 low computational cost. In the third module, we combine the
 mean shift algorithm with inter-frame scale-invariant SURF
 feature tracking to enhance the robustness of our system.
 The experimental evaluation shows that our system
 outperforms, in the FIR domain, the state-of-the-art
 Haar-like Adaboost-cascade, histogram of oriented gradients
 (HOG)/linear SVM (linSVM) and MultiFtrpedestrian detectors,
 trained on the FIR images.},
  Doi                      = {10.3390/s150408570},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Besbes et al. - 2015 -
  Pedestrian Detection in Far-Infrared Daytime Images Using a
  Hierarchical Codebook of SURF.pdf:pdf},
  ISSN                     = {14248220},
  Keywords                 = {Far-infrared images,Hierarchical codebook,Pedestrian classification and trackings,Pedestrian detection,SURF,SVM,Scale-invariant feature matching},
  Pmid                     = {25871724},
  Publisher                = {Multidisciplinary Digital Publishing Institute},
  Url                      = {http://www.mdpi.com/1424-8220/15/4/8570/}
}

@Article{blanc2004obstacle,
  Title                    = {Obstacle detection and tracking by millimeter wave radar},
  Author                   = {Blanc, Christophe and Aufrere, Romuald and Malaterre, Laurent and Gallice, Jean and Alizon, Joseph},
  Journal                  = {IFAC Proceedings Volumes},
  Year                     = {2004},
  Number                   = {8},
  Pages                    = {322--327},
  Volume                   = {37},

  Publisher                = {Elsevier}
}

@InProceedings{blanc2007larasidecam,
  Title                    = {LaRASideCam: A fast and robust vision-based blindspot detection system},
  Author                   = {Blanc, Nicolas and Steux, Bruno and Hinz, Thomas},
  Booktitle                = {Intelligent Vehicles Symposium, 2007 IEEE},
  Year                     = {2007},
  Organization             = {IEEE},
  Pages                    = {480--485}
}

@InProceedings{broggi1998,
  Title                    = {{the Experience of the ARGO Autonomous Vehicle}},
  Author                   = {Broggi, Alberto and Bertozzi, Massimo and Fascioli, Alessandra and Conte, Gianni},
  Booktitle                = {Enhanced and Synthetic Vision},
  Year                     = {1998},
  Editor                   = {Verly, Jacques G.},
  Month                    = {jul},
  Pages                    = {218--229},
  Publisher                = {International Society for Optics and Photonics},
  Volume                   = {3364},

  Abstract                 = {This paper presents and discusses the first results
 obtained by the GOLD (Generic Obstacle and Lane Detection)
 system as an automatic driver of ARGO. ARGO is a Lancia
 Thema passenger car equipped with a vision-based system
 that allows to extract road and environmental information
 from the acquired scene. By means of stereo vision,
 obstacles on the road are detected and localized, while the
 processing of a single monocular image allows to extract
 the road geometry in front of the vehicle. The generality
 of the underlying approach allows to detect generic
 obstacles (without constraints on shape, color, or
 symmetry) and to detect lane markings even in dark and in
 strong shadow conditions. The hardware system consists of a
 PC Pentium 200 Mhz with MMX technology and a frame-grabber
 board able to acquire 3 b/w images simultaneously; the
 result of the processing (position of obstacles and
 geometry of the road) is used to drive an actuator on the
 steering wheel, while debug information are presented to
 the user on an on-board monitor and a led-based control
 panel.},
  Doi                      = {10.1117/12.317473},
  ISBN                     = {9810237200},
  ISSN                     = {0277786X},
  Keywords                 = {algorithms and the architectures,argo is the experimental,autonomous vehicle,autonomous vehicle developed at,computer vision,conducted over the last,dell,few years on the,for vision based automatic,informazione of the,it integrates the main,italy,lane detection,obstacle detection,results of the research,road vehicles guidance,the dipartimento di ingegneria,university of parma},
  Url                      = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=947967
 http://www.worldscientific.com/doi/pdf/10.1142/9789812814937{\_}bmatter}
}

@Book{broggi1999,
  Title                    = {{Automatic Vehicle Guidance : the Experience of the ARGO Autonomous Vehicle}},
  Author                   = {Broggi, Alberto. and Bertozzi, Massimo and Fascioli, Alessandra and Conte, Gianni},
  Publisher                = {World Scientific},
  Year                     = {1999},

  Booktitle                = {World Scientific Co., Singapore},
  ISBN                     = {9810237200},
  Pages                    = {242},
  Url                      = {http://ftp.utcluj.ro/pub/docs/imaging/Autonomous{\_}driving/Articole
 sortate/Lazar
 Mircea/AutoDriving2/parma/www.ce.unipr.it/people/broggi/publications/argo.pdf}
}

@InProceedings{broggi2011,
  Title                    = {{Stereo obstacle detection in challenging environments: The VIAC experience}},
  Author                   = {Broggi, Alberto and Buzzoni, Michele and Felisa, Mirko and Zani, Paolo},
  Booktitle                = {IEEE International Conference on Intelligent Robots and Systems},
  Year                     = {2011},
  Month                    = {sep},
  Pages                    = {1599--1604},
  Publisher                = {IEEE},

  Abstract                 = {Obstacle detection by means of stereo-vision is a
 fundamental task in computer vision, which has spurred a
 lot of research over the years, especially in the field of
 vehicular robotics. The information provided by this class
 of algorithms is used both in driving assistance systems
 and in autonomous vehicles, so the quality of the results
 and the processing times become critical, as detection
 failures or delays can have serious consequences. The
 obstacle detection system presented in this paper has been
 extensively tested during VIAC, the VisLab Intercontinental
 Autonomous Challenge [1], [2], which has offered a unique
 chance to face a number of different scenarios along the
 roads of two continents, in a variety of conditions; data
 collected during the expedition has also become a reference
 benchmark for further algorithm improvements.},
  Doi                      = {10.1109/IROS.2011.6048211},
  ISBN                     = {9781612844541},
  ISSN                     = {2153-0858},
  Url                      = {http://ieeexplore.ieee.org/document/6094535/}
}

@Article{broggi2013,
  Title                    = {{Applications of computer vision to vehicles: An extreme test}},
  Author                   = {Broggi, Alberto and Cattani, Stefano and Medici, Paolo and Zani, Paolo},
  Journal                  = {Studies in Computational Intelligence},
  Year                     = {2013},
  Pages                    = {215--250},
  Volume                   = {411},

  Doi                      = {10.1007/978-3-642-28661-2_9},
  File                     = {::},
  ISBN                     = {9783642286605},
  ISSN                     = {1860949X},
  Url                      = {https://link.springer.com/content/pdf/10.1007/978-3-642-28661-2{\_}9.pdf}
}

@Article{broggi2012a,
  Title                    = {{The VisLab Intercontinental Autonomous Challenge: an extensive test for a platoon of intelligent vehicles}},
  Author                   = {Broggi, Alberto and Cerri, Pietro and Felisa, Mirko and Laghi, Maria Chiara and Mazzei, Luca and Porta, Pier Paolo},
  Journal                  = {International Journal of Vehicle Autonomous Systems},
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {147},
  Volume                   = {10},

  Abstract                 = {This paper presents the VisLab Intercontinental Autonomous
 Challenge (VIAC), an autonomous vehicles test carried out
 from Parma to Shanghai between July and October 2010 by the
 VisLab team. The vehicle equipment is explained introducing
 the sensing systems which were tested during the journey.
 Trip details and the fi rst statistics are presented as
 well.},
  Doi                      = {10.1504/IJVAS.2012.051250},
  ISSN                     = {1471-0226},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.591.8385
 http://www.inderscience.com/link.php?id=51250}
}

@Article{broggi2012,
  Title                    = {{Autonomous vehicles control in the VisLab Intercontinental Autonomous Challenge}},
  Author                   = {Broggi, A. and Medici, P. and Zani, P. and Coati, A. and Panciroli, M.},
  Journal                  = {Annual Reviews in Control},
  Year                     = {2012},

  Month                    = {apr},
  Number                   = {1},
  Pages                    = {161--171},
  Volume                   = {36},

  Abstract                 = {Autonomous driving is one of the most interesting fields
 of research, with a number of important applications, like
 agricultural, military and, most significantly, safety.
 This paper addresses the problem of designing a general
 purpose path planner and its associated low level control
 for autonomous vehicles operating in unknown environments.
 Different kinds of inputs, like the results of obstacle
 detection, ditch localization, lane detection, and global
 path planning information are merged together using
 potential fields to build a representation of the
 environment in real-time; kinematically feasible
 trajectories, based on vehicle dynamics, are generated on a
 cost map. This approach demonstrated both flexibility and
 reliability for vehicle driving in very different
 environments, including extreme road conditions. This
 controller was extensively tested during VIAC, the VisLab
 Intercontinental Autonomous Challenge, a 13,000 km long
 test for intelligent vehicle applications. The results,
 collected during the development stage and the experiment
 itself, are presented in the final part of this article.
 {\textcopyright} 2012 Published by Elsevier Ltd. All rights
 reserved.},
  Doi                      = {10.1016/j.arcontrol.2012.03.012},
  File                     = {::},
  ISBN                     = {1367-5788},
  ISSN                     = {13675788},
  Keywords                 = {Autonomous navigation,Path planning},
  Publisher                = {Pergamon},
  Url                      = {https://www.sciencedirect.com/science/article/pii/S1367578812000132}
}

@InProceedings{Brookner2016,
  Title                    = {{Metamaterial Advances for Radar and Communications}},
  Author                   = {Brookner, Eli},
  Booktitle                = {IEEE International Symposium on Phased Array Systems and Technology (PAST)},
  Year                     = {2016},
  Month                    = {oct},
  Pages                    = {1--9},
  Publisher                = {IEEE},

  Doi                      = {10.1109/ARRAY.2016.7832577},
  ISBN                     = {978-1-5090-1447-7},
  Keywords                 = {aesa,antenna,cloaking,conformal,ebg,esa,fractals,invisibility,magnetic dielectric,magnetic ground plane,metamaterial,phased arrays,radar,stealth,waim},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Url                      = {http://ieeexplore.ieee.org/document/7832577/}
}

@Article{castilloaguilar2015,
  Title                    = {Robust road condition detection system using in-vehicle standard sensors},
  Author                   = {{Castillo Aguilar}, Juan Jes{\'{u}}s and {Cabrera Carrillo}, Juan Antonio and {Guerra Fern{\'{a}}ndez}, Antonio Jes{\'{u}}s and {Carabias Acosta}, Enrique},
  Journal                  = {Sensors (Switzerland)},
  Year                     = {2015},

  Month                    = {dec},
  Number                   = {12},
  Pages                    = {32056--32078},
  Volume                   = {15},

  Abstract                 = {The appearance of active safety systems, such as Anti-lock
 Braking System, Traction Control System, Stability Control
 System, etc., represents a major evolution in road safety.
 In the automotive sector, the term vehicle active safety
 systems refers to those whose goal is to help avoid a crash
 or to reduce the risk of having an accident. These systems
 safeguard us, being in continuous evolution and
 incorporating new capabilities continuously. In order for
 these systems and vehicles to work adequately, they need to
 know some fundamental information: the road condition on
 which the vehicle is circulating. This early road detection
 is intended to allow vehicle control systems to act faster
 and more suitably, thus obtaining a substantial advantage.
 In this work, we try to detect the road condition the
 vehicle is being driven on, using the standard sensors
 installed in commercial vehicles. Vehicle models were
 programmed in on-board systems to perform real-time
 estimations of the forces of contact between the wheel and
 road and the speed of the vehicle. Subsequently, a fuzzy
 logic block is used to obtain an index representing the
 road condition. Finally, an artificial neural network was
 used to provide the optimal slip for each surface.
 Simulations and experiments verified the proposed method.},
  Doi                      = {10.3390/s151229908},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Castillo Aguilar et al. -
  2015 - Robust Road Condition Detection System Using
  In-Vehicle Standard Sensors.pdf:pdf},
  ISSN                     = {14248220},
  Keywords                 = {Friction estimation,Normal driving,Optimal slip estimation,Standard vehicle sensor},
  Pmid                     = {26703605},
  Publisher                = {Multidisciplinary Digital Publishing Institute},
  Url                      = {http://www.mdpi.com/1424-8220/15/12/29908}
}

@InProceedings{censi2014,
  Title                    = {{Low-Latency Event-Based Visual Odometry}},
  Author                   = {Censi, Andrea and Scaramuzza, Davide},
  Booktitle                = {IEEE International Conference on Robotics and Automation (ICRA)},
  Year                     = {2014},

  Address                  = {Hong Kong},
  Pages                    = {703--710},

  Abstract                 = {— The agility of a robotic system is ultimately limited
 by the speed of its processing pipeline. The use of a
 Dynamic Vision Sensors (DVS), a sensor producing
 asynchronous events as luminance changes are perceived by
 its pixels, makes it pos-sible to have a sensing pipeline
 of a theoretical latency of a few microseconds. However,
 several challenges must be overcome: a DVS does not provide
 the grayscale value but only changes in the luminance; and
 because the output is composed by a sequence of events,
 traditional frame-based visual odometry methods are not
 applicable. This paper presents the first visual odometry
 system based on a DVS plus a normal CMOS camera to provide
 the absolute brightness values. The two sources of data are
 automatically spatiotemporally calibrated from logs taken
 during normal operation. We design a visual odometry method
 that uses the DVS events to estimate the relative
 displacement since the previous CMOS frame by processing
 each event individually. Experiments show that the rotation
 can be estimated with surprising accuracy, while the
 translation can be estimated only very noisily, because it
 produces few events due to very small apparent motion.},
  Doi                      = {10.1109/ICRA.2014.6906931},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Censi, Scaramuzza - 2014 -
  Low-Latency Event-Based Visual Odometry.pdf:pdf},
  Url                      = {http://rpg.ifi.uzh.ch/docs/ICRA14{\_}Censi.pdf}
}

@InProceedings{chang2008real,
  Title                    = {Real-time side vehicle tracking using parts-based boosting},
  Author                   = {Chang, Wen-Chung and Cho, Chih-Wei},
  Booktitle                = {Systems, Man and Cybernetics, 2008. SMC 2008. IEEE International Conference on},
  Year                     = {2008},
  Organization             = {IEEE},
  Pages                    = {3370--3375}
}

@Misc{chapell2016,
  Title                    = {{The Big Bang of autonomous driving}},

  Author                   = {Chapell, Lindsay},
  Year                     = {2016},

  Abstract                 = {DARPA Challenge brought lab tests out into the real world,
 and started a movement},
  Booktitle                = {Automotive News},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Url                      = {http://www.autonews.com/article/20161219/OEM06/312199908/the-big-bang-of-autonomous-driving},
  Urldate                  = {2018-10-08}
}

@Article{chen2017turn,
  Title                    = {Turn signal detection during nighttime by CNN detector and perceptual hashing tracking},
  Author                   = {Chen, Long and Hu, Xuemin and Xu, Tong and Kuang, Hulin and Li, Qingquan},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2017},
  Number                   = {12},
  Pages                    = {3303--3314},
  Volume                   = {18},

  Publisher                = {IEEE}
}

@Article{chun2008,
  Title                    = {{Suppressing rolling-shutter distortion of CMOS image sensors by motion vector detection}},
  Author                   = {Chun, Jung Bum and Jung, Hunjoon and Kyung, Chong Min},
  Journal                  = {IEEE Transactions on Consumer Electronics},
  Year                     = {2008},

  Month                    = {nov},
  Number                   = {4},
  Pages                    = {1479--1487},
  Volume                   = {54},

  Abstract                 = {This paper focuses on the rolling shutter distortion of
 CMOS image sensor coming from its unique readout mechanism
 as the main cause for image degradation when there are
 fast-moving objects. This paper proposes a post image
 processing scheme based on motion vector detection to
 suppress the rolling shutter distortion. Motion vector
 detection is performed based on an optical flow method at a
 reasonable computational complexity. A practical
 implementation scheme is also described.},
  Doi                      = {10.1109/TCE.2008.4711190},
  ISSN                     = {00983063},
  Keywords                 = {CMOS image sensor,Post-processing technique,Rolling-shutter distortion},
  Url                      = {http://ieeexplore.ieee.org/document/4711190/}
}

@Article{cornick2016,
  Title                    = {{Localizing Ground Penetrating RADAR: A Step Toward Robust Autonomous Ground Vehicle Localization}},
  Author                   = {Cornick, Matthew and Koechling, Jeffrey and Stanley, Byron and Zhang, Beijia},
  Journal                  = {Journal of Field Robotics},
  Year                     = {2016},

  Month                    = {jan},
  Number                   = {1},
  Pages                    = {82--102},
  Volume                   = {33},

  Abstract                 = {It is anticipatedthat theMars ScienceLaboratory rover,
 namedCuriosity,will traverse 10–20 kmon the surface of
 Mars during its primary mission. In preparation for this
 traverse, Earth-based tests were performed using Mars
 weight vehicles. These vehicles were driven over Mars
 analog bedrock, cohesive soil, and cohesionless sand at
 various slopes. Vehicle slip was characterized on each of
 these terrains versus slope for direct upslope driving.
 Results show that slopes up to 22 degrees are traversable
 on smooth bedrock and that slopes up to 28 degrees are
 traversable on some cohesive soils. In cohesionless sand,
 results show a sharp transition between moderate slip on 10
 degree slopes and vehicle embedding at 17 degrees. For
 cohesionless sand, data are also presented showing the
 relationship between vehicle slip and wheel sinkage. Side
 by side testing of the Mars Exploration Rover test vehicle
 and the Mars Science Laboratory test vehicle show how
 increased wheel diameter leads to better slope climbing
 ability in sand for vehicles with nearly identical ground
 pressure. Lastly, preliminary data from Curiosity's initial
 driving on Mars are presented and compared to the
 Earth-based testing, showing good agreement for the driving
 done during the first 250 Martian days.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {10.1.1.91.5767},
  Doi                      = {10.1002/rob.21605},
  Eprint                   = {10.1.1.91.5767},
  File                     = {::},
  ISBN                     = {9783902661623},
  ISSN                     = {15564967},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Pmid                     = {22164016},
  Publisher                = {Wiley-Blackwell},
  Url                      = {http://doi.wiley.com/10.1002/rob.21605}
}

@Article{coronado2012detection,
  Title                    = {Detection and classification of road signs for automatic inventory systems using computer vision},
  Author                   = {Coronado, Gustavo A Pel{\'a}ez and Mu{\~n}oz, Mar{\'\i}a Romero and Armingol, Jos{\'e} Mar{\'\i}a and de la Escalera, Arturo and Mu{\~n}oz, Juan Jes{\'u}s and van Bijsterveld, Wouter and Bola{\~n}o, Juan Antonio},
  Journal                  = {Integrated Computer-Aided Engineering},
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {285--298},
  Volume                   = {19},

  Publisher                = {IOS Press}
}

@InProceedings{Dagan2004,
  Title                    = {Forward collision warning with a single camera},
  Author                   = {Dagan, E. and Mano, O. and Stein, G.P. and Shashua, A.},
  Booktitle                = {IEEE Intelligent Vehicles Symposium, 2004},
  Year                     = {2004},
  Pages                    = {37--42},
  Publisher                = {IEEE},

  Abstract                 = {The large number of rear end collisions due to driver inattention has been identified as a major automotive safety issue. Even a short advance warning can significantly reduce the number and severity of the collisions. This paper describes a vision based forward collision warning (FCW) system for highway safety. The algorithm described in this paper computes time to contact (TTC) and possible collision course directly from the size and position of the vehicles in the image - which are the natural measurements for a vision based system - without having to compute a 3D representation of the scene. The use of a single low cost image sensor results in an affordable system which is simple to install. The system has been implemented on real-time hardware and has been test driven on highways. Collision avoidance tests have also been performed on test tracks.},
  Doi                      = {10.1109/IVS.2004.1336352},
  ISBN                     = {0-7803-8310-9},
  Url                      = {http://ieeexplore.ieee.org/document/1336352/}
}

@Article{depontemuller2017,
  Title                    = {{Survey on Ranging Sensors and Cooperative Techniques for Relative Positioning of Vehicles}},
  Author                   = {{de Ponte M{\"{u}}ller}, Fabian and Fabian},
  Journal                  = {Sensors},
  Year                     = {2017},

  Month                    = {jan},
  Number                   = {2},
  Pages                    = {271},
  Volume                   = {17},

  Abstract                 = {Future driver assistance systems will rely on accurate,
 reliable and continuous knowledge on the position of other
 road participants, including pedestrians, bicycles and
 other vehicles. The usual approach to tackle this
 requirement is to use on-board ranging sensors inside the
 vehicle. Radar, laser scanners or vision-based systems are
 able to detect objects in their line-of-sight. In contrast
 to these non-cooperative ranging sensors, cooperative
 approaches follow a strategy in which other road
 participants actively support the estimation of the
 relative position. The limitations of on-board ranging
 sensors regarding their detection range and angle of view
 and the facility of blockage can be approached by using a
 cooperative approach based on vehicle-to-vehicle
 communication. The fusion of both, cooperative and
 non-cooperative strategies, seems to offer the largest
 benefits regarding accuracy, availability and robustness.
 This survey offers the reader a comprehensive review on
 different techniques for vehicle relative positioning. The
 reader will learn the important performance indicators when
 it comes to relative positioning of vehicles, the different
 technologies that are both commercially available and
 currently under research, their expected performance and
 their intrinsic limitations. Moreover, the latest research
 in the area of vision-based systems for vehicle detection,
 as well as the latest work on GNSS-based vehicle
 localization and vehicular communication for relative
 positioning of vehicles, are reviewed. The survey also
 includes the research work on the fusion of cooperative and
 non-cooperative approaches to increase the reliability and
 the availability.},
  Doi                      = {10.3390/s17020271},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/de Ponte M{\"{u}}ller,
  Fabian - 2017 - Survey on Ranging Sensors and Cooperative
  Techniques for Relative Positioning of Vehicles.pdf:pdf},
  ISSN                     = {1424-8220},
  Keywords                 = {GNSS,cooperative,laser scanner,localization,relative positioning,to,vehicle,vehicle sensors},
  Publisher                = {Multidisciplinary Digital Publishing Institute},
  Url                      = {http://www.mdpi.com/1424-8220/17/2/271}
}

@Article{dickmanns1987,
  Title                    = {{Autonomous High Speed Road Vehicle Guidance by Computer Vision 1}},
  Author                   = {Dickmanns, E.D. and Zapp, A.},
  Journal                  = {IFAC Proceedings Volumes},
  Year                     = {1987},

  Month                    = {jul},
  Number                   = {5},
  Pages                    = {221--226},
  Volume                   = {20},

  Abstract                 = {A visual feedback control system has been developed which
 is able to guide road vehicles on well structured roads at
 high speeds. The road boundary markings are tracked by a
 multiprocessor image processing system using contour
 correlation and curvature models together with the laws of
 perspective projection. Feature position data are the input
 into Kalman filters to estimate both the vehicle state
 vector relative to the driving lane and road curvature
 parameters. Velocity is measured conventionally.
 Longitudinal control by throttle and braking is geared to
 lateral acceleration due to road curvature; lateral control
 has an anticipatory feed forward and a compensatory
 feedback component. The control system has been tested with
 a CCD TV-camera and image sequence processing hardware in a
 real time simulation loop and with our experimental
 vehicle, a 5 ton-van equipped with sensors, onboard
 computers and actuators for autonomous driving},
  Doi                      = {10.1016/S1474-6670(17)55320-3},
  File                     = {::},
  ISSN                     = {14746670},
  Publisher                = {Elsevier},
  Url                      = {https://www.sciencedirect.com/science/article/pii/S1474667017553203
 http://linkinghub.elsevier.com/retrieve/pii/S1474667017553203}
}

@InProceedings{duthon2016,
  Title                    = {{Visual saliency on the road: model and database dependent detection}},
  Author                   = {Duthon, Pierre and Colomb, Mich{\`{e}}le and Kuntzmann, Laboratoire Jean},
  Booktitle                = {Actes du 20{\`{e}}me congr{\`{e}}s national sur la Reconnaissance des Formes et l'Intelligence Artificielle},
  Year                     = {2016},
  Month                    = {jun},

  Abstract                 = {In the road context, objects of interest (salient or not)
 must be efficiently detected under any condition to ensure
 safety, for both driver assistance systems and autonomous
 vehicles. Nine representative state-of-the-art saliency
 models are evaluated on driving databases (human perception
 vs. robotics). Although not sufficient for robust
 detection, bottom-up saliency provides important
 information, especially when controlling for the classical
 biases.},
  File                     = {::},
  Keywords                 = {road context,saliency,target detection,visual attention},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Url                      = {https://hal.archives-ouvertes.fr/hal-01359997}
}

@Misc{edelstein2018,
  Title                    = {{Intel/Mobileye Self-Driving Cars Begin Testing in Jerusalem - The Drive}},

  Author                   = {Edelstein, Stephen},
  Year                     = {2018},

  Booktitle                = {The Drive},
  Url                      = {http://www.thedrive.com/tech/20919/intel-mobileye-self-driving-cars-begin-testing-in-jerusalem},
  Urldate                  = {2018-10-06}
}

@Misc{eldada2017,
  Title                    = {{LiDAR and the Autonomous Vehicle Revolution for Truck and Ride Sharing Fleets}},

  Author                   = {Eldada, Louay},
  Year                     = {2017},

  Address                  = {San Francisco},
  Booktitle                = {Automated Vehicles Symposium},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Eldada - 2017 - LiDAR and
  the Autonomous Vehicle Revolution for Truck and Ride
  Sharing Fleets.pdf:pdf},
  Url                      = {https://higherlogicdownload.s3.amazonaws.com/AUVSI/14c12c18-fde1-4c1d-8548-035ad166c766/UploadedImages/2017/PDFs/Proceedings/ESS/Wednesday
 1330-1400{\_}Louay Eldada.pdf}
}

@Article{engel2018,
  Title                    = {{Direct Sparse Odometry}},
  Author                   = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2018},

  Month                    = {jul},
  Number                   = {3},
  Pages                    = {611--625},
  Volume                   = {40},

  Abstract                 = {We propose a novel direct sparse visual odometry
 formulation. It combines a fully direct probabilistic model
 (minimizing a photometric error) with consistent, joint
 optimization of all model parameters, including geometry --
 represented as inverse depth in a reference frame -- and
 camera motion. This is achieved in real time by omitting
 the smoothness prior used in other direct methods and
 instead sampling pixels evenly throughout the images. Since
 our method does not depend on keypoint detectors or
 descriptors, it can naturally sample pixels from across all
 image regions that have intensity gradient, including edges
 or smooth intensity variations on mostly white walls. The
 proposed model integrates a full photometric calibration,
 accounting for exposure time, lens vignetting, and
 non-linear response functions. We thoroughly evaluate our
 method on three different datasets comprising several hours
 of video. The experiments show that the presented approach
 significantly outperforms state-of-the-art direct and
 indirect methods in a variety of real-world settings, both
 in terms of tracking accuracy and robustness.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1607.02565},
  Doi                      = {10.1109/TPAMI.2017.2658577},
  Eprint                   = {1607.02565},
  File                     = {::},
  ISBN                     = {0162-8828 VO - PP},
  ISSN                     = {01628828},
  Keywords                 = {3D reconstruction,SLAM,Visual odometry,structure from motion},
  Pmid                     = {28060704},
  Url                      = {http://arxiv.org/abs/1607.02565}
}

@InProceedings{engel2014,
  Title                    = {{LSD-SLAM: Large-Scale Direct monocular SLAM}},
  Author                   = {Engel, Jakob and Sch{\"{o}}ps, Thomas and Cremers, Daniel},
  Booktitle                = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  Year                     = {2014},
  Number                   = {PART 2},
  Pages                    = {834--849},
  Publisher                = {Springer, Cham},
  Volume                   = {8690 LNCS},

  Abstract                 = {We propose a direct (feature-less) monocular SLAM
 algorithm which, in contrast to current state-of-the-art
 regarding direct meth- ods, allows to build large-scale,
 consistent maps of the environment. Along with highly
 accurate pose estimation based on direct image alignment,
 the 3D environment is reconstructed in real-time as
 pose-graph of keyframes with associated semi-dense depth
 maps. These are obtained by filtering over a large number
 of pixelwise small-baseline stereo comparisons. The
 explicitly scale-drift aware formulation allows the
 approach to operate on challenging sequences including
 large variations in scene scale. Major enablers are two key
 novelties: (1) a novel direct tracking method which
 operates on sim(3), thereby explicitly detecting
 scale-drift, and (2) an elegant probabilistic solution to
 include the effect of noisy depth values into tracking. The
 resulting direct monocular SLAM system runs in real-time on
 a CPU.},
  Doi                      = {10.1007/978-3-319-10605-2_54},
  ISBN                     = {9783319106045},
  ISSN                     = {16113349},
  Pmid                     = {638263},
  Url                      = {http://link.springer.com/10.1007/978-3-319-10605-2{\_}54}
}

@InProceedings{frejlichowski2015application,
  Title                    = {Application of the Polar--Fourier Greyscale Descriptor to the Automatic Traffic Sign Recognition},
  Author                   = {Frejlichowski, Dariusz},
  Booktitle                = {International Conference Image Analysis and Recognition},
  Year                     = {2015},
  Organization             = {Springer},
  Pages                    = {506--513}
}

@TechReport{frost&sullivan2014,
  Title                    = {{2011 European Consumers' Desirability and Willingness to Pay for Advanced Driver Assistance and Driving Dynamics Technologies Market Research}},
  Author                   = {{Frost {\&} Sullivan}},
  Year                     = {2014},

  Url                      = {https://store.frost.com/2011-european-consumers-desirability-and-willingness-to-pay-for-advanced-driver-assistance-and-driving-dynamics-technologies.html?}
}

@TechReport{frost&sullivan2010,
  Title                    = {{2009 European Consumers Desirability and Willingness to Pay for Advanced Safety and Driver Assistance Systems Market Research}},
  Author                   = {{Frost {\&} Sullivan}},
  Year                     = {2010},

  Url                      = {https://store.frost.com/2009-european-consumers-desirability-and-willingness-to-pay-for-advanced-safety-and-driver-assistance-systems.html}
}

@InProceedings{funke2012,
  Title                    = {{Up to the limits: Autonomous Audi TTS}},
  Author                   = {Funke, Joseph and Theodosis, Paul and Hindiyeh, Rami and Stanek, Ganymed and Kritatakirana, Krisada and Gerdes, Chris and Langer, Dirk and Hernandez, Marcial and M{\"{u}}ller-Bessler, Bernhard and Huhnke, Burkhard},
  Booktitle                = {IEEE Intelligent Vehicles Symposium, Proceedings},
  Year                     = {2012},
  Month                    = {jun},
  Pages                    = {541--547},
  Publisher                = {IEEE},

  Abstract                 = {This paper presents a novel approach to au- tonomous
 driving at the vehicle's handling limits. Such a system
 requires a high speed, consistent control signal as well as
 numerous safety features capable of monitoring and stopping
 the vehicle. When operating, the system's high level
 controller utilizes a highly accurate differential GPS and
 known friction values to drive a precomputed path at the
 friction limits of the vehicle. The system was tested in a
 variety of road conditions, including the challenging Pikes
 Peak Hill climb. Results from this work can be extended to
 improve driving safety and accident avoidance in
 vehicles.},
  Doi                      = {10.1109/IVS.2012.6232212},
  ISBN                     = {9781467321198},
  ISSN                     = {1931-0587},
  Url                      = {http://ieeexplore.ieee.org/document/6232212/}
}

@InProceedings{gohring2011radar,
  Title                    = {Radar/lidar sensor fusion for car-following on highways},
  Author                   = {G{\"o}hring, Daniel and Wang, Miao and Schn{\"u}rmacher, Michael and Ganjineh, Tinosch},
  Booktitle                = {Automation, Robotics and Applications (ICARA), 2011 5th International Conference on},
  Year                     = {2011},
  Organization             = {IEEE},
  Pages                    = {407--412}
}

@Article{gandhi2006vehicle,
  Title                    = {Vehicle surround capture: Survey of techniques and a novel omni-video-based approach for dynamic panoramic surround maps},
  Author                   = {Gandhi, Tarak and Trivedi, Mohan M},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2006},
  Number                   = {3},
  Pages                    = {293--308},
  Volume                   = {7},

  Publisher                = {IEEE}
}

@InProceedings{gao2015learning,
  Title                    = {Learning local histogram representation for efficient traffic sign recognition},
  Author                   = {Gao, Jinlu and Fang, Yuqiang and Li, Xingwei},
  Booktitle                = {Image and Signal Processing (CISP), 2015 8th International Congress on},
  Year                     = {2015},
  Organization             = {IEEE},
  Pages                    = {631--635}
}

@InProceedings{garcia2012data,
  Title                    = {Data fusion for overtaking vehicle detection based on radar and optical flow},
  Author                   = {Garcia, Fernando and Cerri, Pietro and Broggi, Alberto and de la Escalera, Arturo and Armingol, Jos{\'e} Mar{\'\i}a},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2012 IEEE},
  Year                     = {2012},
  Organization             = {IEEE},
  Pages                    = {494--499}
}

@Article{garcia2018,
  Title                    = {{Bioinspired polarization imager with high dynamic range}},
  Author                   = {Garcia, Missael and Davis, Tyler and Blair, Steven and Cui, Nan and Gruev, Viktor},
  Journal                  = {Optica},
  Year                     = {2018},

  Month                    = {oct},
  Number                   = {10},
  Pages                    = {1240},
  Volume                   = {5},

  Abstract                 = {Polarization is one of the three fundamental properties of
 light, along with color and intensity, yet most vertebrate
 species, including humans, are blind with respect to this
 light modality. In contrast, many invertebrates, including
 insects, spiders, cephalopods, and stomatopods, have
 evolved to detect polarization information with
 high-dynamic-range photosensitive cells and utilize this
 information in visually guided behavior. In this paper, we
 present a high-dynamic-range polarization imaging sensor
 inspired by the visual system of the mantis shrimp. Our
 bioinspired imager achieves 140 dB dynamic range and 61 dB
 maximum signal-to-noise ratio across 384 × 288 pixels
 equipped with logarithmic photodiodes. Contrary to
 state-of-the-art active pixel sensors, where photodiodes in
 individual pixels operate in reverse bias mode and yield up
 to ∼60 dB dynamic range, our pixel has a logarithmic
 response by operating individual photodiodes in forward
 bias mode. This novel pixel circuitry is monolithically
 integrated with pixelated polarization filters composed of
 250-nm-tall × 75-nm-wide aluminum nanowires to enable
 snapshot polarization imaging at 30 frames per second. This
 sensor can enable many automotive and remote sensing
 applications, where high-dynamic-range imaging augmented
 with polarization information can provide critical
 information during hazy or rainy conditions.},
  Doi                      = {10.1364/OPTICA.5.001240},
  File                     = {::},
  ISSN                     = {2334-2536},
  Keywords                 = {(1105405) Polarimetric imaging,(1305440) Polarization-selective devices,(2804788) Optical sensing and sensors,OCIS codes: (2605430) Polarization},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Publisher                = {Optical Society of America},
  Url                      = {https://www.osapublishing.org/abstract.cfm?URI=optica-5-10-1240
 https://doi.org/10.1364/OPTICA.5.001240}
}

@Article{gargoum2017automated,
  Title                    = {Automated highway sign extraction using lidar data},
  Author                   = {Gargoum, Suliman and El-Basyouny, Karim and Sabbagh, Joseph and Froese, Kenneth},
  Journal                  = {Transportation Research Record: Journal of the Transportation Research Board},
  Year                     = {2017},
  Number                   = {2643},
  Pages                    = {1--8},

  Publisher                = {Transportation Research Board of the National Academies}
}

@Article{gatziolis2008,
  Title                    = {{A Guide to LIDAR Data Acquisition and Processing for the Forests of the Pacific Northwest}},
  Author                   = {Gatziolis, Demetrios and Andersen, Hans Erik},
  Journal                  = {General Technical Report PNW-GTR-768},
  Year                     = {2008},
  Number                   = {July},
  Pages                    = {1--40},
  Volume                   = {768},

  Abstract                 = {Light detection and ranging (LIDAR) is an emerging
 remote-sensing technology with promising potential to
 assist in mapping, monitoring, and assessment of forest
 resources. Continuous technological advancement and
 substantial reductions in data acquisition cost have
 enabled acquisition of laser data over entire states and
 regions. These developments have triggered an explosion of
 interest in LIDAR technology. Despite a growing body of
 peer-reviewed literature documenting the merits of LIDAR
 for forest assessment, management, and planning, there
 seems to be little information describing in detail the
 acquisition, quality assessment, and processing of laser
 data for forestry applications. This report addresses this
 information deficit by providing a foundational knowledge
 base containing answers to the most frequently asked
 questions. Keywords: LIDAR, Pacific Northwest, FIA, forest
 inventory, laser, absolute and relative accuracy,
 precision, registration, stand penetration, DEM, canopy
 surface, resolution, data storage, data quality assessment,
 topography, scanning.},
  Doi                      = {Gen. Tech. Rep. PNW-GTR-768},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Gatziolis, Andersen - 2008
  - A guide to LIDAR data acquisition and processing for the
  forests of the Pacific Northwest.pdf:pdf},
  Keywords                 = {DEM,FIA,LIDAR,Pacific Northwest,absolute and relative accuracy,canopy surface,data quality assessment,data storage,forest inventory,laser,precision,registration,resolution,scanning.,stand penetration,topography},
  Url                      = {https://www.fs.usda.gov/treesearch/pubs/30652
 http://www.arlis.org/docs/vol1/A/276932054.pdf}
}

@Article{glennie2010,
  Title                    = {{Static calibration and analysis of the velodyne HDL-64E S2 for high accuracy mobile scanning}},
  Author                   = {Glennie, Craig and Lichti, Derek D.},
  Journal                  = {Remote Sensing},
  Year                     = {2010},

  Month                    = {jun},
  Number                   = {6},
  Pages                    = {1610--1624},
  Volume                   = {2},

  Abstract                 = {The static calibration and analysis of the Velodyne
 HDL-64E S2 scanning LiDAR system is presented and analyzed.
 The mathematical model for measurements for the HDL-64E S2
 scanner is derived and discussed. A planar feature based
 least squares adjustment approach is presented and utilized
 in a minimally constrained network in order to derive an
 optimal solution for the laser's internal calibration
 parameters. Finally, the results of the adjustment along
 with a detailed examination of the adjustment residuals are
 given. A three-fold improvement in the planar misclosure
 residual RMSE over the standard factory calibration model
 was achieved by the proposed calibration. Results also
 suggest that there may still be some unmodelled distortions
 in the range measurements from the scanner. However,
 despite this, the overall precision of the adjusted laser
 scanner data appears to make it a viable choice for high
 accuracy mobile scanning applications.},
  Doi                      = {10.3390/rs2061610},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Glennie, Lichti - 2010 -
  Static Calibration and Analysis of the Velodyne HDL-64E S2
  for High Accuracy Mobile Scanning.pdf:pdf},
  ISBN                     = {2072-4292},
  ISSN                     = {20724292},
  Keywords                 = {Accuracy,Error analysis,Lidar,System calibration},
  Publisher                = {Molecular Diversity Preservation International},
  Url                      = {http://www.mdpi.com/2072-4292/2/6/1610}
}

@Article{gregor2002,
  Title                    = {{EMS-Vision: A Perceptual System for Autonomous Vehicles}},
  Author                   = {Gregor, Rudolf and L{\"{u}}tzeler, M. and Pellkofer, M. and Siedersberger, K. H. and Dickmanns, Ernst Dieter},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2002},

  Month                    = {mar},
  Number                   = {1},
  Pages                    = {48--59},
  Volume                   = {3},

  Abstract                 = {The paper gives a survey on the new Expectation-based
 Multifocal$\backslash$nSaccadic vision (EMS-vision) system
 for autonomous vehicle guidance$\backslash$ndeveloped at
 the Universitat der Bundeswehr Munchen (UBM). EMS-Vision
 is$\backslash$nthe third generation dynamic vision system
 following the 4-D approach.$\backslash$nIts core element is
 a new camera arrangement, mounted on a
 high$\backslash$nbandwidth pan-tilt head for active gaze
 control. Central knowledge$\backslash$nrepresentation and a
 hierarchical system architecture allow
 efficient$\backslash$nactivation and control of behavioral
 capabilities for perception and$\backslash$naction. The
 system has been implemented on commercial
 off-the-shelf$\backslash$n(COTS) hardware components in
 both UBM test vehicles VaMoRs and VaMP$\backslash$nResults
 from autonomous turnoff maneuvers, performed on army
 proving$\backslash$ngrounds, are discussed},
  Doi                      = {10.1109/6979.994795},
  ISBN                     = {0-7803-6363-9},
  ISSN                     = {15249050},
  Keywords                 = {Active vision system,Autonomous vehicles,Control of perception and action,Dynamic machine vision,Knowledge representation,System architecture},
  Url                      = {http://ieeexplore.ieee.org/document/994795/}
}

@InProceedings{gu2011traffic,
  Title                    = {Traffic sign detection in dual-focal active camera system},
  Author                   = {Gu, Yanlei and Yendo, Tomohiro and Tehrani, Mehrdad Panahpour and Fujii, Toshiaki and Tanimoto, Masayuki},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2011 IEEE},
  Year                     = {2011},
  Organization             = {IEEE},
  Pages                    = {1054--1059}
}

@Article{guan2018robust,
  Title                    = {Robust Traffic-Sign Detection and Classification Using Mobile LiDAR Data With Digital Images},
  Author                   = {Guan, Haiyan and Yan, Wanqian and Yu, Yongtao and Zhong, Liang and Li, Dilong},
  Journal                  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  Year                     = {2018},

  Publisher                = {IEEE}
}

@Article{hillel2014recent,
  Title                    = {Recent progress in road and lane detection: a survey},
  Author                   = {Hillel, Aharon Bar and Lerner, Ronen and Levi, Dan and Raz, Guy},
  Journal                  = {Machine vision and applications},
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {727--745},
  Volume                   = {25},

  Publisher                = {Springer}
}

@Article{hosseinyalamdary2017bayesian,
  Title                    = {A Bayesian approach to traffic light detection and mapping},
  Author                   = {Hosseinyalamdary, Siavash and Yilmaz, Alper},
  Journal                  = {ISPRS journal of photogrammetry and remote sensing},
  Year                     = {2017},
  Pages                    = {184--192},
  Volume                   = {125},

  Publisher                = {Elsevier}
}

@InProceedings{Ieng2003,
  Title                    = {Merging lateral cameras information with proprioceptive sensors in vehicle location gives centimetric precision},
  Author                   = {Ieng, S-S and Gruyer, D},
  Booktitle                = {Proceedings of the 18th International Technical Conference on the Enhanced Safety of Vehicles},
  Year                     = {2003},

  Address                  = {Nagoya, Japan},
  Month                    = {may},
  Publisher                = {National Highway Traffic Safety Administration},

  Url                      = {https://trid.trb.org/view.aspx?id=750799}
}

@InProceedings{janda2013,
  Title                    = {Road boundary detection for run-off road prevention based on the fusion of video and radar},
  Author                   = {Janda, Florian and Pangerl, Sebastian and Lang, Eva and Fuchs, Erich},
  Booktitle                = {IEEE Intelligent Vehicles Symposium, Proceedings},
  Year                     = {2013},
  Month                    = {jun},
  Pages                    = {1173--1178},
  Publisher                = {IEEE},

  Abstract                 = {An approach for detecting the road boundary on different
 types of roads without any preliminary knowledge is
 presented. We fuse information obtained from an algorithm
 which detects road markings and road edges in images
 acquired by a video camera as well as data from a radar
 sensor. Each road marking, each road edge and each road
 barrier is tracked individually. Hence we can even capture
 exits or laybys. We use an edge image for road marking
 detection and texture information for road edge detection.
 Additional data provided by a radar sensor is used to
 measure targets referring to static barriers along the road
 side such as guardrails. The output of each processing unit
 is fused into a Kalman filter framework, where the
 confidence of each subsystem influences the innovation of
 the overall system. The underlying geometric road model
 comprises parameters for multiple lanes, the flanking road
 edge as well as the vehicle's relative pose. The work is
 part of the project Interactive.},
  Doi                      = {10.1109/IVS.2013.6629625},
  ISBN                     = {9781467327558},
  ISSN                     = {1931-0587},
  Url                      = {http://ieeexplore.ieee.org/document/6629625/}
}

@Article{kohler2013,
  Title                    = {{Feasibility of automotive radar at frequencies beyond 100 GHz}},
  Author                   = {K{\"{o}}hler, Mike and Hasch, J{\"{u}}rgen and Bl{\"{o}}cher, Hans Ludwig and Schmidt, Lorenz Peter},
  Journal                  = {International Journal of Microwave and Wireless Technologies},
  Year                     = {2013},

  Month                    = {feb},
  Number                   = {1},
  Pages                    = {49--54},
  Volume                   = {5},

  Doi                      = {10.1017/S175907871200075X},
  ISSN                     = {17590787},
  Keywords                 = {Automotive radar},
  Url                      = {http://www.journals.cambridge.org/abstract{\_}S175907871200075X}
}

@Article{kaliyaperumal2001algorithm,
  Title                    = {An algorithm for detecting roads and obstacles in radar images},
  Author                   = {Kaliyaperumal, Kesav and Lakshmanan, Sridhar and Kluge, Karl},
  Journal                  = {IEEE Transactions on Vehicular Technology},
  Year                     = {2001},
  Number                   = {1},
  Pages                    = {170--182},
  Volume                   = {50},

  Publisher                = {IEEE}
}

@InProceedings{kim2015lane,
  Title                    = {Lane map building and localization for automated driving using 2D laser rangefinder},
  Author                   = {Kim, Dongwook and Chung, Taeyoung and Yi, Kyongsu},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2015 IEEE},
  Year                     = {2015},
  Organization             = {IEEE},
  Pages                    = {680--685}
}

@Article{kishida2015,
  Title                    = {{79 GHz-Band High-Resolution Millimeter- Wave Radar}},
  Author                   = {Kishida, Masayuki and Ohguchi, Katsuyuki and Shono, Masayoshi},
  Journal                  = {FUJITSU Sci. Tech. J},
  Year                     = {2015},
  Number                   = {4},
  Pages                    = {55--59},
  Volume                   = {51},

  Abstract                 = {High-resolution millimeter-wave radar that operates in the
 79 GHz band is expected to achieve a significant increase
 in the distance resolution of radar systems because of the
 availability of a wide frequency bandwidth of 4 GHz as
 compared with 0.5 GHz of the existing 77 GHz-band
 mil-limeter-wave radar. For this reason, it has the
 potential to distinguish between a vehicle and a human,
 which was conventionally difficult, and recognize their
 movements. Therefore it raises expectations for use as a
 surrounding monitoring radar in driving safety support and
 auto-matic driving. As one of FUJITSU TEN's efforts
 regarding sensing technologies for driving safety support
 and automatic driving, it has been developing 79 GHz-band
 high-resolution millimeter-wave radar. This paper presents
 specifications of radar for application to systems that
 assist in safe driving and automatic driving and the
 results of testing a prototype for a wider bandwidth that
 is required to accomplish the technology's purpose. This
 radar increases the ability to de-tect a pedestrian in the
 surroundings of a vehicle, which was difficult to do with
 the existing 77 GHz-band radar. Furthermore, this paper
 also describes how the newly developed radar of-fers the
 possibility of improving the performance of a sensor for
 automatic driving and systems to assist in safe driving.},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Kishida, Ohguchi, Shono -
  2015 - 79 GHz-Band High-Resolution Millimeter-Wave
  Radar.pdf:pdf},
  Url                      = {https://www.fujitsu.com/global/documents/about/resources/publications/fstj/archives/vol51-4/paper09.pdf}
}

@Article{koch2011,
  Title                    = {Pothole detection in asphalt pavement images},
  Author                   = {Koch, Christian and Brilakis, Ioannis},
  Journal                  = {Advanced Engineering Informatics},
  Year                     = {2011},

  Month                    = {aug},
  Number                   = {3},
  Pages                    = {507--515},
  Volume                   = {25},

  Abstract                 = {Pavement condition assessment is essential when developing
 road network maintenance programs. In practice, the data
 collection process is to a large extent automated. However,
 pavement distress detection (cracks, potholes, etc.) is
 mostly performed manually, which is labor-intensive and
 time-consuming. Existing methods either rely on complete 3D
 surface reconstruction, which comes along with high
 equipment and computation costs, or make use of
 acceleration data, which can only provide preliminary and
 rough condition surveys. In this paper we present a method
 for automated pothole detection in asphalt pavement images.
 In the proposed method an image is first segmented into
 defect and non-defect regions using histogram shape-based
 thresholding. Based on the geometric properties of a defect
 region the potential pothole shape is approximated
 utilizing morphological thinning and elliptic regression.
 Subsequently, the texture inside a potential defect shape
 is extracted and compared with the texture of the
 surrounding non-defect pavement in order to determine if
 the region of interest represents an actual pothole. This
 methodology has been implemented in a MATLAB prototype,
 trained and tested on 120 pavement images. The results show
 that this method can detect potholes in asphalt pavement
 images with reasonable accuracy. {\textcopyright} 2011
 Elsevier Ltd. All rights reserved.},
  Doi                      = {10.1016/j.aei.2011.01.002},
  ISBN                     = {1474-0346},
  ISSN                     = {14740346},
  Keywords                 = {Image processing,Pavement assessment,Pothole detection,Visual sensing},
  Publisher                = {Elsevier},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1474034611000036}
}

@Article{krotosky2007color,
  Title                    = {On color-, infrared-, and multimodal-stereo approaches to pedestrian detection},
  Author                   = {Krotosky, Stephen J and Trivedi, Mohan Manubhai},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2007},
  Number                   = {4},
  Pages                    = {619--629},
  Volume                   = {8},

  Publisher                = {IEEE}
}

@InProceedings{kum2013lane,
  Title                    = {Lane detection system with around view monitoring for intelligent vehicle},
  Author                   = {Kum, Chang-Hoon and Cho, Dong-Chan and Ra, Moon-Soo and Kim, Whoi-Yul},
  Booktitle                = {SoC Design Conference (ISOCC), 2013 International},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {215--218}
}

@InProceedings{lee2017avm,
  Title                    = {AVM/LiDAR sensor based lane marking detection method for automated driving on complex urban roads},
  Author                   = {Lee, Hyunsung and Kim, Seonwook and Park, Sungyoul and Jeong, Yonghwan and Lee, Hojoon and Yi, Kyongsu},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2017 IEEE},
  Year                     = {2017},
  Organization             = {IEEE},
  Pages                    = {1434--1439}
}

@PhdThesis{levinson2011a,
  Title                    = {{Automatic Laser Calibration, Mapping, and Localization for Autonomous Vehicles}},
  Author                   = {Levinson, Jesse},
  Year                     = {2011},

  Abstract                 = {This dissertation presents several related algorithms that
 enable$\backslash$nimportant capabilities for self-driving
 vehicles. Using a rotating$\backslash$nmulti-beam laser
 rangefinder to sense the world, our vehicle
 scans$\backslash$nmillions of 3D points every second.
 Calibrating these sensors plays$\backslash$na crucial role
 in accurate perception, but manual calibration
 is$\backslash$nunreasonably tedious, and generally
 inaccurate. As an alternative,$\backslash$nwe present an
 unsupervised algorithm for automatically
 calibrating$\backslash$nboth the intrinsics and extrinsics
 of the laser unit from only seconds$\backslash$nof driving
 in an arbitrary and unknown environment. We show
 that$\backslash$nthe results are not only vastly easier to
 obtain than traditional$\backslash$ncalibration techniques,
 they are also more accurate. A second
 key$\backslash$nchallenge in autonomous navigation is
 reliable localization in the$\backslash$nface of
 uncertainty. Using our calibrated sensors, we obtain
 high$\backslash$nresolution infrared reflectivity readings
 of the world. From these,$\backslash$nwe build large-scale
 self-consistent probabilistic laser maps
 of$\backslash$nurban scenes, and show that we can reliably
 localize a vehicle against$\backslash$nthese maps to within
 centimeters, even in dynamic environments,
 by$\backslash$nfusing noisy GPS and IMU readings with the
 laser in realtime. We$\backslash$nalso present a
 localization algorithm that was used in the
 DARPA$\backslash$nUrban Challenge, which operated without a
 prerecorded laser map,$\backslash$nand allowed our vehicle
 to complete the entire six-hour course
 without$\backslash$na single localization failure. Finally,
 we present a collection of$\backslash$nalgorithms for the
 mapping and detection of traffic lights in
 realtime.$\backslash$nThese methods use a combination of
 computer-vision techniques and$\backslash$nprobabilistic
 approaches to incorporating uncertainty in order
 to$\backslash$nallow our vehicle to reliably ascertain the
 state of traffic-light-controlled$\backslash$nintersections.},
  Pages                    = {153},
  Url                      = {https://searchworks.stanford.edu/view/9275866
 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:AUTOMATIC+LASER+CALIBRATION+,+MAPPING+,+AND+LOCALIZATION+FOR+AUTONOMOUS+VEHICLES+A+DISSERTATION+SUBMITTED+TO+THE+DEPARTMENT+OF+COMPUTER+SCIENCE+AND}
}

@InProceedings{levinson2011,
  Title                    = {{Towards fully autonomous driving: Systems and algorithms}},
  Author                   = {Levinson, Jesse and Askeland, Jake and Becker, Jan and Dolson, Jennifer and Held, David and Kammel, Soeren and Kolter, J. Zico and Langer, Dirk and Pink, Oliver and Pratt, Vaughan and Sokolsky, Michael and Stanek, Ganymed and Stavens, David and Teichman, Alex and Werling, Moritz and Thrun, Sebastian},
  Booktitle                = {IEEE Intelligent Vehicles Symposium, Proceedings},
  Year                     = {2011},
  Month                    = {jun},
  Pages                    = {163--168},
  Publisher                = {IEEE},

  Abstract                 = {In order to achieve autonomous operation of a vehicle in
 urban situations with unpredictable traffic, several
 realtime systems must interoperate, including environment
 perception, localization, planning, and control. In
 addition, a robust vehicle platform with appropriate
 sensors, computational hardware, networking, and software
 infrastructure is essential.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1702.06827},
  Doi                      = {10.1109/IVS.2011.5940562},
  Eprint                   = {1702.06827},
  ISBN                     = {9781457708909},
  ISSN                     = {1931-0587},
  Pmid                     = {11317986},
  Url                      = {http://ieeexplore.ieee.org/document/5940562/}
}

@Article{li2016vehicle,
  Title                    = {Vehicle detection from 3d lidar using fully convolutional network},
  Author                   = {Li, Bo and Zhang, Tianlei and Xia, Tian},
  Journal                  = {arXiv preprint arXiv:1608.07916},
  Year                     = {2016}
}

@InProceedings{li2013new,
  Title                    = {A new 3D LIDAR-based lane markings recognition approach},
  Author                   = {Li, Tan and Zhidong, Deng},
  Booktitle                = {Robotics and Biomimetics (ROBIO), 2013 IEEE International Conference on},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {2197--2202}
}

@Article{chia-kailiang2008,
  Title                    = {{Analysis and compensation of rolling shutter effect}},
  Author                   = {Liang, Chia Kai and Chang, Li Wen and Chen, Homer H.},
  Journal                  = {IEEE Transactions on Image Processing},
  Year                     = {2008},

  Month                    = {aug},
  Number                   = {8},
  Pages                    = {1323--1330},
  Volume                   = {17},

  Abstract                 = {Due to the sequential-readout structure of complementary
 metal-oxide semiconductor image sensor array, each scanline
 of the acquired image is exposed at a different time,
 resulting in the so-called electronic rolling shutter that
 induces geometric image distortion when the object or the
 video camera moves during image capture. In this paper, we
 propose an image processing technique using a planar motion
 model to address the problem. Unlike previous methods that
 involve complex 3-D feature correspondences, a simple
 approach to the analysis of inter- and intraframe
 distortions is presented. The high-resolution velocity
 estimates used for restoring the image are obtained by
 global motion estimation, BEzier curve fitting, and local
 motion estimation without resort to correspondence
 identification. Experimental results demonstrate the
 effectiveness of the algorithm.},
  Doi                      = {10.1109/TIP.2008.925384},
  ISBN                     = {1057-7149 (Print)},
  ISSN                     = {10577149},
  Keywords                 = {Complementary metal-oxide semiconductor (CMOS) sen,Motion analysis,Rolling shutter},
  Pmid                     = {18632342},
  Url                      = {http://ieeexplore.ieee.org/document/4549748/}
}

@Article{liu2014traffic,
  Title                    = {Traffic sign recognition using group sparse coding},
  Author                   = {Liu, Huaping and Liu, Yulong and Sun, Fuchun},
  Journal                  = {Information Sciences},
  Year                     = {2014},
  Pages                    = {75--89},
  Volume                   = {266},

  Publisher                = {Elsevier}
}

@Article{liu2015,
  Title                    = {An ultrasonic sensor system based on a two-dimensional state method for highway vehicle violation detection applications},
  Author                   = {Liu, Jun and Han, Jiuqiang and Lv, Hongqiang and Li, Bing},
  Journal                  = {Sensors (Switzerland)},
  Year                     = {2015},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {9000--9021},
  Volume                   = {15},

  Abstract                 = {With the continuing growth of highway construction and
 vehicle use expansion all over the world, highway vehicle
 traffic rule violation (TRV) detection has become more and
 more important so as to avoid traffic accidents and
 injuries in intelligent transportation systems (ITS) and
 vehicular ad hoc networks (VANETs). Since very few works
 have contributed to solve the TRV detection problem by
 moving vehicle measurements and surveillance devices, this
 paper develops a novel parallel ultrasonic sensor system
 that can be used to identify the TRV behavior of a host
 vehicle in real-time. Then a two-dimensional state method
 is proposed, utilizing the spacial state and time
 sequential states from the data of two parallel ultrasonic
 sensors to detect and count the highway vehicle violations.
 Finally, the theoretical TRV identification probability is
 analyzed, and actual experiments are conducted on different
 highway segments with various driving speeds, which
 indicates that the identification accuracy of the proposed
 method can reach about 90.97{\%}.},
  Doi                      = {10.3390/s150409000},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - An
  Ultrasonic Sensor System Based on a Two-Dimensional State
  Method for Highway Vehicle Violation Detection
  Applicat.pdf:pdf},
  ISSN                     = {14248220},
  Keywords                 = {Highway vehicle traffic rule violation detection,Intelligent transportation systems,Two-dimensional state method,Ultrasonic sensor system},
  Pmid                     = {1603305},
  Publisher                = {Multidisciplinary Digital Publishing Institute},
  Url                      = {http://www.mdpi.com/1424-8220/15/4/9000/}
}

@InProceedings{liu2007rear,
  Title                    = {Rear vehicle detection and tracking for lane change assist},
  Author                   = {Liu, Wei and Wen, XueZhi and Duan, Bobo and Yuan, Huai and Wang, Nan},
  Booktitle                = {Intelligent Vehicles Symposium, 2007 IEEE},
  Year                     = {2007},
  Organization             = {IEEE},
  Pages                    = {252--257}
}

@Article{llinas2000,
  Title                    = {{Revisiting the JDL data fusion model II}},
  Author                   = {Llinas, James and Bowman, Christopher and Rogova, Galina and Steinberg, Alan},
  Journal                  = {Space and Naval Warfare Systems Command},
  Year                     = {2004},
  Number                   = {7},
  Pages                    = {1--14},
  Volume                   = {1},

  Abstract                 = {This paper suggests refinements and extensions of the JDL
 Data Fusion Model, the standard process model used for a
 multiplicity of community purposes. However, this Model has
 not been reviewed in accordance with (a) the dynamics of
 world events and (b) the changes, discoveries, and new
 methods in both the data fusion research and development
 community and related IT technologies. This paper suggests
 ways to revise and extend this important model. Proposals
 are made regarding (a) improvements in the understanding of
 internal processing within a fusion node and (b) extending
 the model to include (1) remarks on issues related to
 quality control, reliability, and consistency in DF
 processing, (2) assertions about the need for co-processing
 of abductive/ inductive and deductive inferencing
 processes, (3) remarks about the need for and exploitation
 of an onto logicallybased approach to DF process design,
 and (4) extensions to account for the case of Distributed
 Data Fusion (DDF).},
  ISBN                     = {917056115X},
  Keywords                 = {data fusion,fusion,information fusion,jdl model},
  Url                      = {http://oai.dtic.mil/oai/oai?verb=getRecord{\&}metadataPrefix=html{\&}identifier=ADA525721}
}

@Article{lundquist2011,
  Title                    = {Joint ego-motion and road geometry estimation},
  Author                   = {Lundquist, Christian and Sch{\"{o}}n, Thomas B.},
  Journal                  = {Information Fusion},
  Year                     = {2011},

  Month                    = {oct},
  Number                   = {4},
  Pages                    = {253--263},
  Volume                   = {12},

  Abstract                 = {We provide a sensor fusion framework for solving the
 problem of joint ego-motion and road geometry estimation.
 More specifically we employ a sensor fusion framework to
 make systematic use of the measurements from a forward
 looking radar and camera, steering wheel angle sensor,
 wheel speed sensors and inertial sensors to compute good
 estimates of the road geometry and the motion of the ego
 vehicle on this road. In order to solve this problem we
 derive dynamical models for the ego vehicle, the road and
 the leading vehicles. The main difference to existing
 approaches is that we make use of a new dynamic model for
 the road. An extended Kalman filter is used to fuse data
 and to filter measurements from the camera in order to
 improve the road geometry estimate. The proposed solution
 has been tested and compared to existing algorithms for
 this problem, using measurements from authentic traffic
 environments on public roads in Sweden. The results clearly
 indicate that the proposed method provides better
 estimates. ?? 2011 Elsevier B.V. All rights reserved.},
  Doi                      = {10.1016/j.inffus.2010.06.007},
  ISBN                     = {1566-2535},
  ISSN                     = {15662535},
  Keywords                 = {Bicycle model,Extended Kalman filter,Road geometry estimation,Sensor fusion,Single track model},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S1566253510000709}
}

@Article{ma2000simultaneous,
  Title                    = {Simultaneous detection of lane and pavement boundaries using model-based multisensor fusion},
  Author                   = {Ma, Bing and Lakshmanan, Sridhar and Hero, Alfred O},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2000},
  Number                   = {3},
  Pages                    = {135--147},
  Volume                   = {1},

  Publisher                = {IEEE}
}

@Article{Maddalena2005,
  Title                    = {{Automotive CMOS image sensors}},
  Author                   = {Maddalena, S. and Darmon, A. and Diels, R.},
  Journal                  = {Advanced Microsystems for Automotive {\ldots}},
  Year                     = {2005},
  Pages                    = {401--412},

  Abstract                 = {After penetrating over a decade the consumer and industrial world, digital imaging is slowly but inevitably gaining marketshare in the automotive world. Cameras will become a key sensor in increasing car safety, driving assistance and driving comfort. The image sensors for automotive will be dominated by CMOS sensors as the requirements are different from the consumer market or the industrial or medical markets. Dynamic range, temperature range, cost, speed and many others are key parameters that need to be optimized. For this reason, automotive sensors differ from the other market's sensors and need to use different design and processing techniques in order to achieve the automotive specifications. This paper will show how Melexis has developed two CMOS imagers to target the automotive safety market and automotive CMOS imagers in general.},
  Address                  = {Berlin/Heidelberg},
  Doi                      = {10.1007/3-540-27463-4_29},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maddalena, Darmon, Diels - 2005 - Automotive CMOS Image Sensors.pdf:pdf},
  Mendeley-groups          = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
  Publisher                = {Springer-Verlag},
  Url                      = {http://link.springer.com/10.1007/3-540-27463-4{\_}29 http://www.springerlink.com/index/M354109378G10242.pdf}
}

@InProceedings{Maqueda2018,
  Title                    = {{Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars}},
  Author                   = {Maqueda, Ana I and Loquercio, Antonio and Gallego, Guillermo and Garcia, Narciso and Scaramuzza, Davide},
  Booktitle                = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2018},

  Address                  = {Salt Lake City},

  Abstract                 = {Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, filtering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle's steering angle. To make the best out of this sensor-algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset ({\~{}}1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1804.01310},
  Doi                      = {10.1109/CVPR.2018.00568},
  Eprint                   = {1804.01310},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maqueda et al. - 2018 - Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars.pdf:pdf},
  Keywords                 = {Computer Vision,Machine Learning},
  Mendeley-groups          = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
  Url                      = {http://rpg.ifi.uzh.ch/docs/CVPR18{\_}Maqueda.pdf http://arxiv.org/abs/1804.01310}
}

@Article{massot1999,
  Title                    = {{Praxitele: Preliminary results from the Saint-Quentin station-car experiment}},
  Author                   = {Massot, M.-H.a and Allouche, J.-F.b and B{\'{e}}n{\'{e}}jam, E.c and Parent, M.d},
  Journal                  = {Transportation Research Record},
  Year                     = {1999},

  Month                    = {jan},
  Number                   = {1666},
  Pages                    = {125--132},
  Volume                   = {1666},

  Abstract                 = {The Praxitele system is the first large-scale,
 operational, public individual-transportation system - or
 station-car system - using self-service electric vehicles.
 It was developed in France by a consortium of industrial
 companies and research institutes, formed in 1993. Its
 operation started in the city of Saint-Quentin-en-Yvelines,
 a high-tech center near Paris, at the end of 1997 with 50
 electric vehicles from Renault. At midterm in the
 experiment, close to 500 participants were using the
 system. This is the first report on the experiment, which
 continued until the end of 1998. Preliminary conclusions
 show that users have expressed a high level of satisfaction
 and a desire to expand the system. However, no conclusion
 can be drawn on yet the economics of such a system, which
 remains expensive and underutilized.},
  Doi                      = {10.3141/1666-15},
  ISSN                     = {03611981},
  Keywords                 = {Economic and social effects,Electric vehicles,Mass transportation,Self service electric vehicles,Station-car syste},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Publisher                = {Transportation Research Board of the National Academies},
  Url                      = {http://trrjournalonline.trb.org/doi/10.3141/1666-15
 http://www.scopus.com/inward/record.url?eid=2-s2.0-0033285652{\&}partnerID=40{\&}md5=2c0ce78034a4e8db551c9b336d44bd1e}
}

@Article{McManamon1996,
  Title                    = {{Optical phased array technology}},
  Author                   = {Mcmanamon, Paul F. and Dorschner, Terry A. and Corkum, David L. and Friedman, Larry J. and Hobbs, Douglas S. and Holz, Michael and Liberman, Sergey and Nguyen, Huy Q. and Resler, Daniel P. and Sharp, Richard C. and Watson, Edward A. and Dorschner, T. A. and Friedman, L. J. and Hobbs, D. S. and Holz, M. and Resler, D. P. and Sharp, R. C.},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {1996},

  Month                    = {feb},
  Number                   = {2},
  Pages                    = {268--298},
  Volume                   = {84},

  Abstract                 = {{Optical phased arrays represent an enabling new technology that makes possible simple affordable, lightweight, optical sensors offering very precise stabilization, random-access pointing programmable multiple simultaneous beams, a dynamic focus/defocus capability, and moderate to excellent optical power handling capability. These new arrays steer or otherwise operate on an already formed beam. A phase profile is imposed on an optical beam as it is either transmitted through or reflected from the phase shifter array. The imposed phase profile steers, focuses, fans out, or corrects phase aberrations on the beam. The array of optical phase shifters is realized through lithographic patterning of an electrical addressing network on the superstrate of a liquid crystal waveplate. Refractive index changes sufficiently large to realize full-wave differential phase shifts can be effected using low ({\textless}10 V) voltages applied to the liquid crystal phase plate electrodes. High efficiency large-angle steering with phased arrays requires phase shifter spacing on the order of a wavelength or less; consequently addressing issues make 1-D optical arrays much more practical than 2-D arrays. Orthogonal oriented 1-D phased arrays are used to deflect a beam in both dimensions. Optical phased arrays with apertures on the order of 4 cm by 4 cm have been fabricated for steering green, red, 1.06 $\mu$m, and 10.6 $\mu$m radiation. System concepts that include a passive acquisition sensor as well as a laser radar are presented{\}}, keywords={\{}aberrations;arrays;liquid crystal devices;lithography;optical radar;optical sensors;phase shifters;phased array radar;refractive index;1.06 micrometre;10 V;10.6 micrometre;1D optical arrays;dynamic focus/defocus capability;electrical addressing network;full-wave differential phase shifts;large-angle steering;laser radar;liquid crystal waveplate;lithographic patterning;optical phased array technology;optical power handling capability;optical sensors;passive acquisition sensor;phase aberrations;phase profile;phase shifter array;programmable multiple simultaneous beams;random-access pointing;refractive index changes;Laser radar;Liquid crystals;Optical arrays;Optical beams;Optical refraction;Optical sensors;Optical variables control;Phase shifters;Phased arrays;Sensor arrays}},
  Doi                      = {10.1109/5.482231},
  ISSN                     = {00189219},
  Mendeley-groups          = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
  Url                      = {http://ieeexplore.ieee.org/document/482231/}
}

@Article{miyata2017automatic,
  Title                    = {Automatic Recognition of Speed Limits on Speed-Limit Signs by Using Machine Learning},
  Author                   = {Miyata, Shigeharu},
  Journal                  = {Journal of Imaging},
  Year                     = {2017},
  Number                   = {3},
  Pages                    = {25},
  Volume                   = {3},

  Publisher                = {Multidisciplinary Digital Publishing Institute}
}

@Article{mizumachi2014robust,
  Title                    = {Robust sensing of approaching vehicles relying on acoustic cues},
  Author                   = {Mizumachi, Mitsunori and Kaminuma, Atsunobu and Ono, Nobutaka and Ando, Shigeru},
  Journal                  = {Sensors},
  Year                     = {2014},
  Number                   = {6},
  Pages                    = {9546--9561},
  Volume                   = {14},

  Publisher                = {Multidisciplinary Digital Publishing Institute}
}

@Misc{mobileye2018,
  Title                    = {{Our Technology - Mobileye}},

  Author                   = {Mobileye},
  Year                     = {2018},

  Booktitle                = {Mobileye webpage},
  Url                      = {https://www.mobileye.com/our-technology/},
  Urldate                  = {2018-10-06}
}

@Article{montemerlo2008,
  Title                    = {{Junior: the Stanford entry in the Urban Challenge}},
  Author                   = {Montemerlo, Michael and Becker, Jan and Bhat, Suhrid and Dahlkamp, Hendrik and Dolgov, Dmitri and Ettinger, Scott and Haehnel, Dirk and Hilden, Tim and Hoffmann, Gabe and Huhnke, Burkhard and Johnston, Doug and Klumpp, Stefan and Langer, Dirk and Levandowski, Anthony and Levinson, Jesse and Marcil, Julien and Orenstein, David and Paefgen, Johannes and Penny, Isaac and Petrovskaya, Anna and Pflueger, Mike and Stanek, Ganymed and Stavens, David and Vogt, Antone and Thrun, Sebastian},
  Journal                  = {Journal of Field Robotics},
  Year                     = {2008},

  Month                    = {sep},
  Number                   = {9},
  Pages                    = {569--597},
  Volume                   = {25},

  Abstract                 = {This article presents the architecture of Junior, a
 robotic vehicle capable of navigating ur-ban environments
 autonomously. In doing so, the vehicle is able to select
 its own routes, perceive and interact with other traffic,
 and execute various urban driving skills including lane
 changes, U-turns, parking, and merging into moving traffic.
 The vehicle successfully finished and won second place in
 the DARPA Urban Challenge, a robot competition or-ganized
 by the U.},
  Doi                      = {10.1002/rob.20258},
  File                     = {::},
  ISSN                     = {15564959},
  Publisher                = {Wiley-Blackwell},
  Url                      = {http://doi.wiley.com/10.1002/rob.20258
 https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20258{\%}0Ahttp://dx.doi.org/10.1002/rob.20258}
}

@PhdThesis{montemerlo2003a,
  Title                    = {{FastSLAM: A factored solution to the simultaneous localization and mapping problem}},
  Author                   = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
  School                   = {Carnegie Mellon University},
  Year                     = {2003},

  Abstract                 = {The ability to simultaneously localize a robot and
 accurately map its surroundings is considered by many to be
 a key prerequisite of truly autonomous robots. However, few
 approaches to this problem scale up to handle the very
 large number of landmarks present in real environments.
 Kalman filter-based algorithms, for example, require time
 quadratic in the number of landmarks to incorporate each
 sensor observation. This paper presents FastSLAM, an
 algorithm that recursively estimates the full posterior
 distribution over robot pose and landmark locations, yet
 scales logarithmically with the number of landmarks in the
 map. This algorithm is based on a factorization of the
 posterior into a product of conditional landmark
 distributions and a distribution over robot paths. The
 algorithm has been run successfully on as many as 50,000
 landmarks, environments far beyond the reach of previous
 approaches. Experimental results demonstrate the advantages
 and limitations of the FastSLAM algorithm on both simulated
 and real-world data.},
  Annote                   = {NULL},
  Doi                      = {10.1.1.16.2153},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Montemerlo et al. - 2003 -
  FastSLAM A factored solution to the simultaneous
  localization and mapping problem.pdf:pdf},
  ISBN                     = {0262511290},
  Url                      = {https://www.ri.cmu.edu/pub{\_}files/pub4/montemerlo{\_}michael{\_}2003{\_}1/montemerlo{\_}michael{\_}2003{\_}1.pdf
 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:FastSLAM+:+A+Factored+Solution+to+the+Simultaneous+Localization+and+Mapping+Problem{\#}0}
}

@InProceedings{mueggler2014,
  Title                    = {{Event-based, 6-DOF pose tracking for high-speed maneuvers}},
  Author                   = {Mueggler, Elias and Huber, Basil and Scaramuzza, Davide},
  Booktitle                = {IEEE International Conference on Intelligent Robots and Systems},
  Year                     = {2014},
  Month                    = {sep},
  Pages                    = {2761--2768},
  Publisher                = {IEEE},

  Abstract                 = {In the last few years, we have witnessed impres- sive
 demonstrations of aggressive flights and acrobatics using
 quadrotors. However, those robots are actually blind. They
 do not see by themselves, but through the “eyes” of an
 external motion capture system. Flight maneuvers using
 onboard sensors are still slow compared to those attainable
 with motion capture systems. At the current state, the
 agility of a robot is limited by the latency of its
 perception pipeline. To obtain more agile robots, we need
 to use faster sensors. In this paper, we present the first
 onboard perception system for 6-DOF localization during
 high-speed maneuvers using a Dynamic Vision Sensor (DVS).
 Unlike a standard CMOS camera, a DVS does not wastefully
 send full image frames at a fixed frame rate. Conversely,
 similar to the human eye, it only transmits pixel-level
 brightness changes at the time they occur with microsecond
 resolution, thus, offering the possibility to create a
 perception pipeline whose latency is negligible compared to
 the dynamics of the robot. We exploit these characteristics
 to estimate the pose of a quadrotor with respect to a known
 pattern during high-speed maneuvers, such as flips, with
 rotational speeds up to 1,200 ◦/s. Additionally, we
 provide a versatile method to capture ground-truth data
 using a DVS},
  Doi                      = {10.1109/IROS.2014.6942940},
  ISBN                     = {9781479969340},
  ISSN                     = {21530866},
  Url                      = {http://ieeexplore.ieee.org/document/6942940/}
}

@Article{mukhtar2015vehicle,
  Title                    = {Vehicle Detection Techniques for Collision Avoidance Systems: A Review.},
  Author                   = {Mukhtar, Amir and Xia, Likun and Tang, Tong Boon},
  Journal                  = {IEEE Trans. Intelligent Transportation Systems},
  Year                     = {2015},
  Number                   = {5},
  Pages                    = {2318--2338},
  Volume                   = {16}
}

@Article{nguyen2013,
  Title                    = {1-Point Ransac Based Robust Visual Odometry},
  Author                   = {Nguyen, Van Cuong and Heo, Moon Beom and Jee, Gyu-In},
  Journal                  = {Journal of Positioning, Navigation, and Timing},
  Year                     = {2013},

  Month                    = {apr},
  Number                   = {1},
  Pages                    = {81--89},
  Volume                   = {2},

  Doi                      = {10.11003/JKGS.2013.2.1.081},
  ISSN                     = {2288-8187},
  Keywords                 = {1-point method,Ackermann's principle,Bundle Adjustment,kpubs,kpubs.org,rotation estimation},
  Publisher                = {The Korean GNSS Society},
  Url                      = {http://koreascience.or.kr/journal/view.jsp?kj=HOHSB0{\&}py=2013{\&}vnc=v2n1{\&}sp=81}
}

@TechReport{NHTSA2016,
  Title                    = {{Federal Automated Vehicles Policy}},
  Author                   = {NHTSA},
  Institution              = {NHTSA},
  Year                     = {2016},
  Number                   = {September},

  Abstract                 = {Technology in transportation is not new. In fact, the airplane, the automobile, the train and the horse-drawn carriage all introduced new opportunities and new complications to the safe movement of people and goods.$\backslash$r$\backslash$n$\backslash$r$\backslash$nAs the digital era increasingly reaches deeper into transportation, our task at the U.S. Department of Transportation is not only to keep pace, but to ensure public safety while establishing a strong foundation such that the rules of the road can be known, understood, and responded to by industry and the public. The self-driving car raises more possibilities and more questions than perhaps any other transportation innovation under present discussion. That is as it should be. Possessing the potential to uproot personal mobility as we know it, to make it safer and even more ubiquitous than conventional automobiles and perhaps even more efficient, self-driving cars have become the archetype of our future transportation. Still, important concerns emerge. Will they fully replace the human driver? What ethical judgments will they be called upon to make? What socioeconomic impacts flow from such a dramatic change? Will they disrupt the nature of privacy and security?$\backslash$r$\backslash$n$\backslash$r$\backslash$nMany of these larger questions will require longer and more thorough dialogue with government, industry, academia and, most importantly, the public.},
  File                     = {:D$\backslash$:/Users/109123/Desktop/Federal automated vehicles policy.pdf:pdf},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Pages                    = {1--116},
  Url                      = {https://www.transportation.gov/AV{\%}0Apapers3://publication/uuid/2E47DA05-79E3-4DA5-BE3B-B172900149A1}
}

@InProceedings{nie2012camera,
  Title                    = {Camera and lidar fusion for road intersection detection},
  Author                   = {Nie, Yiming and Chen, Qingyang and Chen, Tongtong and Sun, Zhenping and Dai, Bin},
  Booktitle                = {Electrical \& Electronics Engineering (EEESYM), 2012 IEEE Symposium on},
  Year                     = {2012},
  Organization             = {IEEE},
  Pages                    = {273--276}
}

@Article{nordin2004,
  Title                    = {{Optical Frequency Modulated Continuous Wave (FMCW) Range and Velocity Measurements}},
  Author                   = {Nordin, Daniel},
  Journal                  = {Thesis},
  Year                     = {2004},
  Pages                    = {110},

  File                     = {::},
  Institution              = {Lulea University of Technology, Sweden},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Url                      = {https://www.diva-portal.org/smash/get/diva2:999065/FULLTEXT01.pdf
 http://epubl.luth.se/1402-1544/2004/43/LTU-DT-0443-SE.pdf}
}

@Article{Nowakowski2015,
  Title                    = {{Development of California Regulations to Govern Testing and Operation of Automated Driving Systems}},
  Author                   = {Nowakowski, Christopher and Shladover, Steven E and Chan, Ching-Yao and Tan, Han-Shue},
  Journal                  = {Transportation Research Record: Journal of the Transportation Research Board},
  Year                     = {2015},
  Pages                    = {137--144},
  Volume                   = {2489},

  Abstract                 = {Higher levels of road vehicle automation pose a regulatory challenge in the U.S. At the national level, the National Highway Traffic Safety Administration (NHTSA) has been responsible for ensuring vehicle safety through the mandatory Federal Motor Vehicle Safety Standards (FMVSS) and the voluntary New Car Assessment Program (NCAP). Although NHTSA typically regulates the technology aspects of vehicle safety, state agencies, such as a Department of Motor Vehicles (DMV), are responsible for the regulations governing training, evaluation, and licensing of drivers, and the registration of vehicles. Automation that allows drivers to disengage from monitoring and control tasks introduces safety concerns related to the vehicle technology (typically regulated by NHTSA) and to the Automated Vehicle's (AV) driving behavior and compliance with vehicle codes (typically regulated by states). This paper details a variety of issues that need to be addressed in support of state regulations for manufacturers' testing of AVs on public roads and the general public's operation of automated driving systems. The key challenges for these regulations are how to ensure public safety without discouraging technical innovations and how to define meaningful requirements in the absence of existing technical standards for automated driving systems. The topics covered in this paper include regulatory models of certification, driver training and licensing requirements, and a discussion of the distinction between behavioral competency (how well the automation handles the environment) and functional safety (how well the vehicle handles internal faults and failures). This information is reported here so that other jurisdictions and institutions that will need to grapple with the same issues will be able to benefit from what we have learned in the process of developing the first comprehensive set of state regulations governing road vehicle automation.},
  Doi                      = {10.3141/2489-16},
  File                     = {::},
  ISBN                     = {5106653673},
  ISSN                     = {0361-1981},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Url                      = {https://pdfs.semanticscholar.org/1ce1/ea57551a98a36f44bf9c45607da4462bfadc.pdf http://trrjournalonline.trb.org/doi/10.3141/2489-16}
}

@TechReport{NTSB2018,
  Title                    = {{PRELIMINARY REPORT - HIGHWAY - HWY18MH010}},
  Author                   = {NTSB},
  Institution              = {National Transportation Safety Board},
  Year                     = {2018},
  Number                   = {May},

  Abstract                 = {About 9:58 p.m., on Sunday, March 18, 2018, an Uber Technologies, Inc. test vehicle, based on a modified 2017 Volvo XC90 and operating with a self-driving system in computer control mode, struck a pedestrian on northbound Mill Avenue, in Tempe, Maricopa County, Arizona. The Uber test vehicle was occupied by one vehicle operator, a 44-year-old female. No passengers were in the vehicle.},
  File                     = {::},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Pages                    = {4},
  Url                      = {https://www.ntsb.gov/investigations/AccidentReports/Reports/HWY18MH010-prelim.pdf}
}

@Misc{NTSB2017,
  Title                    = {{NTSB Opens Docket on Tesla Crash}},

  Author                   = {NTSB},
  Year                     = {2017},

  Booktitle                = {National Transportation Safety Board Office of Public Affairs},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Pages                    = {702},
  Url                      = {https://www.ntsb.gov/news/press-releases/Pages/PR20170619.aspx},
  Urldate                  = {2018-10-16}
}

@Article{omalley2008,
  Title                    = {{A review of automotive infrared pedestrian detection techniques}},
  Author                   = {O'Malley, R. and Glavin, Martin and Jones, E.},
  Journal                  = {Signals and Systems Conference, 208.(ISSC 2008). IET Irish},
  Year                     = {2008},
  Pages                    = {168--173},

  Abstract                 = {Abstract In automotive design, the issue of safety remains
 a growing priority. Recently the focus has extended beyond
 the occupants of the vehicle and has turned towards other
 Vulnerable Road Users (VRU). Simple night vision systems
 have already become an ...},
  Doi                      = {10.1049/cp:20080657},
  ISBN                     = {978-0-86341-931-7},
  Keywords                 = {- pedestrian detection,active safety,agery,driver assist,infrared,obstacle detection,thermal im-},
  Pmid                     = {4780948},
  Publisher                = {IEE},
  Url                      = {http://digital-library.theiet.org/content/conferences/10.1049/cp{\_}20080657
 papers2://publication/uuid/FFD556E9-8AF3-48F2-9C9B-3B57247056B7{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4780948{\%}5Cnpapers2://publication/uuid/FC2526CD-D332-4959-B833}
}

@TechReport{leddartech2016,
  Title                    = {{LEDDAR optical Time-of-Flight sensing technology: A new approach to detection and ranging}},
  Author                   = {Olivier, Pierre},
  Institution              = {LeddarTech},
  Year                     = {2016},

  Address                  = {Quebec},

  Abstract                 = {Remote sensing consists of acquiring information about a
 specific object in the vicinity of a sensor without making
 physical contact with the object. Countless applications
 such as automotive driver assistance systems and autonomous
 driving, drone and robot collision avoidance and
 navigation, traffic management and level sensing exist
 thanks to this technique. Multiple technology options are
 available for remote sensing; we can divide them into three
 broad applications: Presence or proximity detection, where
 the absence or presence of an object in a general area is
 the only information that is required (e.g., for security
 applications). This is the simplest form of remote sensing;
 Speed measurement, where the exact position of an object
 does not need to be known but where its accurate speed is
 required (e.g., for law enforcement applications); and
 Detection and ranging, where the position of an object
 relative to the sensor needs to be precisely and accurately
 determined. This paper will concentrate on technologies
 capable of providing a detection and ranging functionality,
 as it is the most complex of the three applications. From
 the position information, presence and speed can be
 retrieved so technologies capable of detection and ranging
 can be universally applied to all remote sensing
 applications.},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Olivier - 2016 - Leddar
  Optical Time-of-Flight sensing Technology A new approach to
  detection and ranging.pdf:pdf},
  Pages                    = {13},
  Url                      = {https://leddartech.com/app/uploads/dlm{\_}uploads/2016/02/Leddar-Optical-Time-of-Flight-Sensing-Technology-1.pdf
 https://d1wx5us9wukuh0.cloudfront.net/app/uploads/dlm{\_}uploads/2016/02/Leddar-Optical-Time-of-Flight-Sensing-Technology-1.pdf}
}

@Article{olmeda2013pedestrian,
  Title                    = {Pedestrian detection in far infrared images},
  Author                   = {Olmeda, Daniel and Premebida, Cristiano and Nunes, Urbano and Armingol, Jose Maria and de la Escalera, Arturo},
  Journal                  = {Integrated Computer-Aided Engineering},
  Year                     = {2013},
  Number                   = {4},
  Pages                    = {347--360},
  Volume                   = {20},

  Publisher                = {IOS Press}
}

@Book{world2015global,
  Title                    = {Global status report on road safety 2015},
  Author                   = {World Health Organization},
  Publisher                = {World Health Organization},
  Year                     = {2015}
}

@Article{ozgunalp2017multiple,
  Title                    = {Multiple lane detection algorithm based on novel dense vanishing point estimation},
  Author                   = {Ozgunalp, Umar and Fan, Rui and Ai, Xiao and Dahnoun, Naim},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2017},
  Number                   = {3},
  Pages                    = {621--632},
  Volume                   = {18},

  Publisher                = {IEEE}
}

@Book{ozguner2007,
  Title                    = {{Autonomous Ground Vehicles}},
  Author                   = {Ozguner, Umit and Acarman, Tankut and Redmill, Keith},
  Publisher                = {Artech House},
  Year                     = {2007},

  Abstract                 = {In the near future, we will witness vehicles with the
 ability to provide drivers with several advanced safety and
 performance assistance features. Autonomous technology in
 ground vehicles will afford us capabilities like
 intersection collision warning, lane change warning, backup
 parking, parallel parking aids, and bus precision parking.
 Providing you with a practical understanding of this
 technology area, this innovative resource focuses on basic
 autonomous control and feedback for stopping and steering
 ground vehicles.Covering sensors, estimation, and sensor
 fusion to percept the vehicle motion and surrounding
 objects, this unique book explains the key aspects that
 makes autonomous vehicle behavior possible. Moreover, you
 find detailed examples of fusion and Kalman filtering. From
 maps, path planning, and obstacle avoidance scenarios...to
 cooperative mobility among autonomous vehicles,
 vehicle-to-vehicle communication, and
 vehicle-to-infrastructure communication, this
 forward-looking book presents the most critical topics in
 the field today.},
  ISBN                     = {978-1608071920},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Pages                    = {292},
  Url                      = {http://us.artechhouse.com/Autonomous-Ground-Vehicles-P1476.aspx}
}

@InCollection{perez2016,
  Title                    = {{Vehicle Control in ADAS Applications: State of the Art}},
  Author                   = {P{\'{e}}rez, Joshu{\'{e}} and Gonzalez, David and Milan{\'{e}}s, Vicente},
  Booktitle                = {Intelligent Transport Systems: Technologies and Applications},
  Publisher                = {John Wiley {\&} Sons, Ltd},
  Year                     = {2016},

  Address                  = {Chichester, UK},
  Month                    = {oct},
  Pages                    = {206--219},

  Abstract                 = {... driver models previously validated are compared with
 current driver behaviour [4]. The ... dated$\backslash$nand
 demonstrated important intermediate steps towards highly
 automated driving for passenger ...$\backslash$nissues
 remained open, eg managing between manual and fully
 autonomous driving. ... $\backslash$n},
  Doi                      = {10.1002/9781118894774.ch11},
  ISBN                     = {9781118894774},
  Keywords                 = {Advanced driver assistance system,Autonomous driving,Intelligent transportation systems,Lateral control,Longitudinal control,Vehicle control},
  Url                      = {http://doi.wiley.com/10.1002/9781118894774.ch11}
}

@Article{palazzi2018,
  Title                    = {{Predicting the Driver's Focus of Attention: the DR(eye)VE Project}},
  Author                   = {Palazzi, Andrea and Abati, Davide and Calderara, Simone and Solera, Francesco and Cucchiara, Rita},
  Journal                  = {(preprint) IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2018},

  Month                    = {may},

  Abstract                 = {In this work we aim to predict the driver{\&}{\#}x0027;s
 focus of attention. The goal is to estimate what a person
 would pay attention to while driving, and which part of the
 scene around the vehicle is more critical for the task. To
 this end we propose a new computer vision model based on a
 multi-branch deep architecture that integrates three
 sources of information: raw video, motion and scene
 semantics. We also introduce DR(eye)VE, the largest dataset
 of driving scenes for which eye-tracking annotations are
 available. This dataset features more than 500,000
 registered frames, matching ego-centric views (from glasses
 worn by drivers) and car-centric views (from roof-mounted
 camera), further enriched by other sensors measurements.
 Results highlight that several attention patterns are
 shared across drivers and can be reproduced to some extent.
 The indication of which elements in the scene are likely to
 capture the driver{\&}{\#}x0027;s attention may benefit
 several applications in the context of human-vehicle
 interaction and driver attention analysis.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1705.03854},
  Doi                      = {10.1109/TPAMI.2018.2845370},
  Eprint                   = {1705.03854},
  File                     = {::},
  ISSN                     = {01628828},
  Keywords                 = {Cameras,Computational modeling,Predictive models,Semantics,Task analysis,Vehicles,Visualization,driver{\&}{\#}x0027,focus of attention,gaze prediction,s attention},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Url                      = {http://arxiv.org/abs/1705.03854}
}

@InProceedings{parent1993,
  Title                    = {{Automatic Driving For Small Public Urban Vehicles}},
  Author                   = {Parent, M. and Daviet, P.},
  Booktitle                = {Proceedings of the Intelligent Vehicles '93 Symposium},
  Year                     = {1993},
  Pages                    = {402--407},
  Publisher                = {IEEE},

  Doi                      = {10.1109/IVS.1993.697360},
  ISBN                     = {0-7803-1370-4},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Url                      = {http://ieeexplore.ieee.org/document/697360/}
}

@Article{Pendleton2017,
  Title                    = {{Perception, Planning, Control, and Coordination for Autonomous Vehicles}},
  Author                   = {Pendleton, Scott and Andersen, Hans and Du, Xinxin and Shen, Xiaotong and Meghjani, Malika and Eng, You and Rus, Daniela and Ang, Marcelo},
  Journal                  = {Machines},
  Year                     = {2017},

  Month                    = {feb},
  Number                   = {1},
  Pages                    = {6},
  Volume                   = {5},

  Abstract                 = {Autonomous vehicles are expected to play a key role in the future of urban transportation systems, as they offer potential for additional safety, increased productivity, greater accessibility, better road efﬁciency, and positive impact on the environment. Research in autonomous systems has seen dramatic advances in recent years, due to the increases in available computing power and reduced cost in sensing and computing technologies, resulting in maturing technological readiness level of fully autonomous vehicles. The objective of this paper is to provide a general overview of the recent developments in the realm of autonomous vehicle software systems. Fundamental components of autonomous vehicle software are reviewed, and recent developments in each area are discussed.},
  Doi                      = {10.3390/machines5010006},
  File                     = {::},
  ISBN                     = {01411926 (ISSN)},
  ISSN                     = {2075-1702},
  Keywords                 = {automotive control,autonomous vehicles,localization,multi,perception,planning,vehicle cooperation},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Publisher                = {Multidisciplinary Digital Publishing Institute},
  Url                      = {http://www.mdpi.com/2075-1702/5/1/6}
}

@Article{phillips2017,
  Title                    = {{When the Dust Settles: The Four Behaviors of LiDAR in the Presence of Fine Airborne Particulates}},
  Author                   = {Phillips, Tyson Govan and Guenther, Nicky and McAree, Peter Ross},
  Journal                  = {Journal of Field Robotics},
  Year                     = {2017},

  Month                    = {aug},
  Number                   = {5},
  Pages                    = {985--1009},
  Volume                   = {34},

  Abstract                 = {It is anticipatedthat theMars ScienceLaboratory rover,
 namedCuriosity,will traverse 10–20 kmon the surface of
 Mars during its primary mission. In preparation for this
 traverse, Earth-based tests were performed using Mars
 weight vehicles. These vehicles were driven over Mars
 analog bedrock, cohesive soil, and cohesionless sand at
 various slopes. Vehicle slip was characterized on each of
 these terrains versus slope for direct upslope driving.
 Results show that slopes up to 22 degrees are traversable
 on smooth bedrock and that slopes up to 28 degrees are
 traversable on some cohesive soils. In cohesionless sand,
 results show a sharp transition between moderate slip on 10
 degree slopes and vehicle embedding at 17 degrees. For
 cohesionless sand, data are also presented showing the
 relationship between vehicle slip and wheel sinkage. Side
 by side testing of the Mars Exploration Rover test vehicle
 and the Mars Science Laboratory test vehicle show how
 increased wheel diameter leads to better slope climbing
 ability in sand for vehicles with nearly identical ground
 pressure. Lastly, preliminary data from Curiosity's initial
 driving on Mars are presented and compared to the
 Earth-based testing, showing good agreement for the driving
 done during the first 250 Martian days.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {10.1.1.91.5767},
  Doi                      = {10.1002/rob.21701},
  Eprint                   = {10.1.1.91.5767},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Phillips, Guenther, McAree
  - 2017 - When the Dust Settles The Four Behaviors of LiDAR
  in the Presence of Fine Airborne Particulates.pdf:pdf},
  ISBN                     = {9783902661623},
  ISSN                     = {15564967},
  Pmid                     = {22164016},
  Publisher                = {Wiley-Blackwell},
  Url                      = {http://doi.wiley.com/10.1002/rob.21701}
}

@InProceedings{pinchon2018,
  Title                    = {{All-Weather Vision for Automotive Safety: Which Spectral Band?}},
  Author                   = {Pinchon, Nicolas and Cassignol, Olivier and Nicolas, Adrien and Bernardin, Fr{\'{e}}d{\'{e}}ric and Leduc, Patrick and Tarel, Jean-Philippe and Br{\'{e}}mond, Roland and Bercier, Emmanuel and Brunet, Johann},
  Booktitle                = {Advanced Microsystems for Automotive Applications 2018},
  Year                     = {2018},
  Month                    = {sep},
  Pages                    = {3--15},
  Publisher                = {Springer, Cham},

  Doi                      = {10.1007/978-3-319-99762-9_1},
  Url                      = {http://link.springer.com/10.1007/978-3-319-99762-9{\_}1}
}

@InProceedings{polychronopoulos2006,
  Title                    = {{Revisiting JDL model for automotive safety applications: The PF2 functional model}},
  Author                   = {Polychronopoulos, A. and Amditis, A. and Scheunert, U. and Tatschke, T.},
  Booktitle                = {2006 9th International Conference on Information Fusion, FUSION},
  Year                     = {2006},
  Month                    = {jul},
  Pages                    = {1--7},
  Publisher                = {IEEE},

  Abstract                 = {The question raised in this paper, for the first time, is
 how the JDL model can be applied in multi-sensor automotive
 safety systems, since new sensors are integrated on-board,
 while new functions support the driver, intervene and
 control the vehicle. The paper proposes a hybrid
 hierarchical structure and develops a suitable functional
 model, namely the ProFusion2 (PF2) model; PF2 serves the
 broad automotive sensor data fusion community as a
 conceptual framework of common understanding and it
 provides recommendations and guidelines for implementation
 of fusion systems. Reference implementations are given as
 complete examples from the major automotive research
 initiative in Europe (PReVENT project).},
  Doi                      = {10.1109/ICIF.2006.301681},
  File                     = {:E$\backslash$:/publicaciones/2018{\_}review{\_}sensores{\_}percepcion{\_}ADAS/doc/polychronopoulos2006.pdf:pdf},
  ISBN                     = {1424409535},
  ISSN                     = {1-4244-0953-5},
  Keywords                 = {Environment model,JDL model,Object refinement,PF2 functional model,Situation refinement},
  Url                      = {http://ieeexplore.ieee.org/document/4085967/}
}

@InProceedings{poulton2016,
  Title                    = {{Frequency-modulated Continuous-wave LIDAR Module in Silicon Photonics}},
  Author                   = {Poulton, Christopher V and Cole, David B and Yaacobi, Ami and Watts, Michael R},
  Booktitle                = {Optical Fiber Communication Conference},
  Year                     = {2016},

  Address                  = {Anaheim},
  Number                   = {c},
  Pages                    = {4--6},

  Abstract                 = {Frequency-modulated continuous-wave LIDAR is demonstrated
 with a silicon photonic device consisting of transmitting
 and receiving waveguides and photodetectors. A 20 mm
 resolution and 2 m range is shown. Simultaneous distance
 and velocity measurements are achieved.},
  Doi                      = {10.1364/OFC.2016.W4E.3},
  ISBN                     = {9781943580071},
  Keywords                 = {3,Integra,Remote sensing and sensors: 280.3640 Lidar,pdf},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Url                      = {https://ieeexplore.ieee.org/document/7537823}
}

@Article{premebida2007lidar,
  Title                    = {A lidar and vision-based approach for pedestrian and vehicle detection and tracking},
  Author                   = {Premebida, Cristiano and Monteiro, Gon{\c{c}}alo and Nunes, Urbano and Peixoto, Paulo},
  Journal                  = {rn},
  Year                     = {2007},
  Pages                    = {2},
  Volume                   = {10}
}

@Article{pueo2016,
  Title                    = {{High speed cameras for motion analysis in sports science}},
  Author                   = {Pueo, Basilio},
  Journal                  = {Journal of Human Sport and Exercise},
  Year                     = {2016},

  Month                    = {dec},
  Number                   = {1},
  Pages                    = {53--73},
  Volume                   = {11},

  Abstract                 = {Video analysis can be a qualitative or quantitative
 process to analyze motion occurring in a single plane using
 one camera (two-dimensional or 2D) or in more than one
 plane using two or more cameras simultaneously
 (three-dimensional or 3D). Quantitative 2D video analysis
 is performed through a digitizing process that converts
 body segments or sport implements into digital horizontal
 and vertical coordinates in the computer. In order for
 these measurements to be accurate, image capture by means
 of video cameras must be sharp and motion blur-free,
 especially in high speed motions. In this paper, a detailed
 introduction to factors affecting image quality will be
 presented. Furthermore, selection of the most appropriate
 camera setting to undertake high speed motion analysis with
 the best quality possible, both spatially (focus and
 resolution) and temporally (frame rate, motion blur,
 shutter options and lighting), will be discussed. Rather
 than considering commercial criteria, the article will
 focus on key features to choose the most convenient model
 both from technical and economical perspectives. Then, a
 revision of available cameras on the market as of 2015 will
 be carried out, with selected models grouped into three
 categories: high-, mid- and low-range, according to their
 maximum performance in relation to high speed features.
 Finally, a suggested recording procedure to minimize
 perspective errors and produce high quality video
 recordings will be presented. This guideline starts with
 indications for camera selection prior to purchase or for
 testing if a given camera would fulfil the minimum
 features. A good video recording dramatically improves the
 analysis quality and enables digitizing software to produce
 accurate measurements},
  Doi                      = {10.14198/jhse.2016.111.05},
  ISSN                     = {19885202},
  Keywords                 = {Biomechanics,Frame rate,Motion blur,Performance,Shutter speed},
  Url                      = {http://hdl.handle.net/10045/61909}
}

@Article{reina2015,
  Title                    = {Radar sensing for intelligent vehicles in urban environments},
  Author                   = {Reina, Giulio and Johnson, David and Underwood, James},
  Journal                  = {Sensors (Switzerland)},
  Year                     = {2015},

  Month                    = {jun},
  Number                   = {6},
  Pages                    = {14661--14678},
  Volume                   = {15},

  Abstract                 = {Radar overcomes the shortcomings of laser, stereovision,
 and sonar because it can operate successfully in dusty,
 foggy, blizzard-blinding, and poorly lit scenarios. This
 paper presents a novel method for ground and obstacle
 segmentation based on radar sensing. The algorithm operates
 directly in the sensor frame, without the need for a
 separate synchronised navigation source, calibration
 parameters describing the location of the radar in the
 vehicle frame, or the geometric restrictions made in the
 previous main method in the field. Experimental results are
 presented in various urban scenarios to validate this
 approach, showing its potential applicability for advanced
 driving assistance systems and autonomous vehicle
 operations.},
  Doi                      = {10.3390/s150614661},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Reina, Johnson, Underwood
  - 2015 - Radar Sensing for Intelligent Vehicles in Urban
  Environments.pdf:pdf},
  ISSN                     = {14248220},
  Keywords                 = {Navigation systems,Perception in urban environment,Radar sensing,Robotic intelligent vehicles,Unmanned ground vehicles},
  Publisher                = {Multidisciplinary Digital Publishing Institute},
  Url                      = {http://www.mdpi.com/1424-8220/15/6/14661/}
}

@Article{santana2016,
  Title                    = {Learning a Driving Simulator},
  Author                   = {Santana, Eder and Hotz, George},
  Year                     = {2016},

  Month                    = {aug},

  Abstract                 = {Comma.ai's approach to Artificial Intelligence for
 self-driving cars is based on an agent that learns to clone
 driver behaviors and plans maneuvers by simulating future
 events in the road. This paper illustrates one of our
 research approaches for driving simulation. One where we
 learn to simulate. Here we investigate variational
 autoencoders with classical and learned cost functions
 using generative adversarial networks for embedding road
 frames. Afterwards, we learn a transition model in the
 embedded space using action conditioned Recurrent Neural
 Networks. We show that our approach can keep predicting
 realistic looking video for several frames despite the
 transition model being optimized without a cost function in
 the pixel space.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1608.01230},
  Eprint                   = {1608.01230},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Santana, Hotz - 2016 -
  Learning a Driving Simulator.pdf:pdf},
  Url                      = {http://arxiv.org/abs/1608.01230}
}

@Article{scaramuzza2008,
  Title                    = {Appearance-Guided Monocular Omnidirectional Visual Odometry for Outdoor Ground Vehicles},
  Author                   = {Scaramuzza, D. and Siegwart, R.},
  Journal                  = {IEEE Transactions on Robotics},
  Year                     = {2008},

  Month                    = {oct},
  Number                   = {5},
  Pages                    = {1015--1026},
  Volume                   = {24},

  Doi                      = {10.1109/TRO.2008.2004490},
  ISSN                     = {1552-3098},
  Url                      = {http://ieeexplore.ieee.org/document/4625958/}
}

@Misc{scheer2018,
  Title                    = {{Exclusive: Intel's Mobileye gets self-driving tech deal for 8 million cars}},

  Author                   = {Scheer, Steven},
  Year                     = {2018},

  Booktitle                = {Reuters},
  Url                      = {https://www.reuters.com/article/us-israel-tech-intel-mobileye-exclusive/exclusive-intels-mobileye-gets-self-driving-tech-deal-for-8-million-cars-idUSKCN1II0K7},
  Urldate                  = {2018-10-06}
}

@InProceedings{schreiber2013laneloc,
  Title                    = {Laneloc: Lane marking based localization using highly accurate maps},
  Author                   = {Schreiber, Markus and Kn{\"o}ppel, Carsten and Franke, Uwe},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2013 IEEE},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {449--454}
}

@Misc{shalev-shwartz2017,
  Title                    = {{On a Formal Model of Safe and Scalable Self-driving Cars}},

  Author                   = {Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  Month                    = {aug},
  Year                     = {2017},

  Abstract                 = {In recent years, car makers and tech companies have been
 racing towards self driving cars. It seems that the main
 parameter in this race is who will have the first car on
 the road. The goal of this paper is to add to the equation
 two additional crucial parameters. The first is
 standardization of safety assurance --- what are the
 minimal requirements that every self-driving car must
 satisfy, and how can we verify these requirements. The
 second parameter is scalability --- engineering solutions
 that lead to unleashed costs will not scale to millions of
 cars, which will push interest in this field into a niche
 academic corner, and drive the entire field into a "winter
 of autonomous driving". In the first part of the paper we
 propose a white-box, interpretable, mathematical model for
 safety assurance, which we call Responsibility-Sensitive
 Safety (RSS). In the second part we describe a design of a
 system that adheres to our safety assurance requirements
 and is scalable to millions of cars.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1708.06374},
  Doi                      = {1708.06374v2},
  Eprint                   = {1708.06374},
  File                     = {::},
  Url                      = {http://arxiv.org/abs/1708.06374}
}

@TechReport{shalev-shwartz2016,
  Title                    = {{Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving}},
  Author                   = {Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  Year                     = {2016},
  Month                    = {oct},

  Abstract                 = {Autonomous driving is a multi-agent setting where the host
 vehicle must apply sophisticated negotiation skills with
 other road users when overtaking, giving way, merging,
 taking left and right turns and while pushing ahead in
 unstructured urban roadways. Since there are many possible
 scenarios, manually tackling all possible cases will likely
 yield a too simplistic policy. Moreover, one must balance
 between unexpected behavior of other drivers/pedestrians
 and at the same time not to be too defensive so that normal
 traffic flow is maintained. In this paper we apply deep
 reinforcement learning to the problem of forming long term
 driving strategies. We note that there are two major
 challenges that make autonomous driving different from
 other robotic tasks. First, is the necessity for ensuring
 functional safety-something that machine learning has
 difficulty with given that performance is optimized at the
 level of an expectation over many instances. Second, the
 Markov Decision Process model often used in robotics is
 problematic in our case because of unpredictable behavior
 of other agents in this multi-agent scenario. We make three
 contributions in our work. First, we show how policy
 gradient iterations can be used, and the variance of the
 gradient estimation using stochastic gradient ascent can be
 minimized, without Markovian assumptions. Second, we
 decompose the problem into a composition of a Policy for
 Desires (which is to be learned) and trajectory planning
 with hard constraints (which is not learned). The goal of
 Desires is to enable comfort of driving, while hard
 constraints guarantees the safety of driving. Third, we
 introduce a hierarchical temporal abstraction we call an
 "Option Graph" with a gating mechanism that significantly
 reduces the effective horizon and thereby reducing the
 variance of the gradient estimation even further. The
 Option Graph plays a similar role to "structured
 prediction" in supervised learning, thereby reducing sample
 complexity, while also playing a similar role to LSTM
 gating mechanisms used in supervised deep networks.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1610.03295v1},
  Booktitle                = {Arxiv},
  Eprint                   = {1610.03295v1},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Shalev-shwartz, Shammah,
  Shashua - 2016 - Safe, Multi-Agent, Reinforcement Learning
  for Autonomous Driving(2).pdf:pdf},
  Url                      = {https://cdn.mobileye.com/wp-content/uploads/2016/10/Safe-Multi-Agent-Reinforcement-Learning-for-Autonomous-Driving.pdf
 http://arxiv.org/abs/1610.03295}
}

@Article{sivaraman2013looking,
  Title                    = {Looking at vehicles on the road: A survey of vision-based vehicle detection, tracking, and behavior analysis},
  Author                   = {Sivaraman, Sayanan and Trivedi, Mohan Manubhai},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2013},
  Number                   = {4},
  Pages                    = {1773--1795},
  Volume                   = {14},

  Publisher                = {IEEE}
}

@Article{Sleasman2017,
  Title                    = {{Experimental Synthetic Aperture Radar with Dynamic Metasurfaces}},
  Author                   = {Sleasman, Timothy and Boyarsky, Michael and Pulido-Mancera, Laura and Fromenteze, Thomas and Imani, Mohammadreza F. and Reynolds, Matthew S. and Smith, David R.},
  Journal                  = {IEEE Transactions on Antennas and Propagation},
  Year                     = {2017},

  Month                    = {feb},
  Number                   = {12},
  Pages                    = {6864--6877},
  Volume                   = {65},

  Abstract                 = {We investigate the use of a dynamic metasurface as the transmitting antenna for a synthetic aperture radar (SAR) imaging system. The dynamic metasurface consists of a one-dimensional microstrip waveguide with complementary electric resonator (cELC) elements patterned into the upper conductor. Integrated into each of the cELCs are two diodes that can be used to shift each cELC resonance out of band with an applied voltage. The aperture is designed to operate at K band frequencies (17.5 to 20.3 GHz), with a bandwidth of 2.8 GHz. We experimentally demonstrate imaging with a fabricated metasurface aperture using existing SAR modalities, showing image quality comparable to traditional antennas. The agility of this aperture allows it to operate in spotlight and stripmap SAR modes, as well as in a third modality inspired by computational imaging strategies. We describe its operation in detail, demonstrate high-quality imaging in both 2D and 3D, and examine various trade-offs governing the integration of dynamic metasurfaces in future SAR imaging platforms.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1703.00072},
  Doi                      = {10.1109/TAP.2017.2758797},
  Eprint                   = {1703.00072},
  File                     = {::},
  ISBN                     = {0018-926X},
  ISSN                     = {0018926X},
  Keywords                 = {Beam steering,microwave imaging,radar imaging,reconfigurable antennas,synthetic aperture radar (SAR),waveguide antennas},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Url                      = {http://arxiv.org/abs/1703.00072 http://dx.doi.org/10.1109/TAP.2017.2758797}
}

@Article{soilan2016traffic,
  Title                    = {Traffic sign detection in MLS acquired point clouds for geometric and image-based semantic inventory},
  Author                   = {Soil{\'a}n, Mario and Riveiro, Bel{\'e}n and Mart{\'\i}nez-S{\'a}nchez, Joaqu{\'\i}n and Arias, Pedro},
  Journal                  = {ISPRS Journal of Photogrammetry and Remote Sensing},
  Year                     = {2016},
  Pages                    = {92--101},
  Volume                   = {114},

  Publisher                = {Elsevier}
}

@InProceedings{song2007lateral,
  Title                    = {Lateral driving assistance using optical flow and scene analysis},
  Author                   = {Song, Kai-Tai and Chen, Hung-Yi},
  Booktitle                = {Intelligent Vehicles Symposium, 2007 IEEE},
  Year                     = {2007},
  Organization             = {IEEE},
  Pages                    = {624--629}
}

@InProceedings{Stein2003,
  Title                    = {Vision-based ACC with a single camera: Bounds on range and range rate accuracy},
  Author                   = {Stein, Gideon P. and Mano, Ofer and Shashua, Amnon},
  Booktitle                = {IEEE Intelligent Vehicles Symposium, Proceedings},
  Year                     = {2003},
  Pages                    = {120--125},
  Publisher                = {IEEE},

  Abstract                 = {This paper describes a vision-based adaptive cruise control (ACC) system which uses a single camera as input. In particular, we discuss how to compute the range and range-rate from a single camera and discuss how the imaging geometry affects the range and range rate accuracy. We determine the bound on the accuracy given a particular configuration. These bounds in turn determine what steps must be made to achieve good performance. The system has been implemented on a test vehicle and driven on various highways over thousands of miles.},
  Doi                      = {10.1109/IVS.2003.1212895},
  ISBN                     = {0780378482},
  Url                      = {http://ieeexplore.ieee.org/document/1212895/}
}

@Article{strobel2013,
  Title                    = {{High dynamic range CMOS (HDRC) imagers for safety systems}},
  Author                   = {Strobel, Markus and D{\"{o}}ttling, Dietmar},
  Journal                  = {Advanced Optical Technologies},
  Year                     = {2013},

  Month                    = {jan},
  Number                   = {2},
  Pages                    = {147--157},
  Volume                   = {2},

  Doi                      = {10.1515/aot-2012-0081},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Strobel, D{\"{o}}ttling -
  2013 - High dynamic range CMOS (HDRC) imagers for safety
  systems.pdf:pdf},
  ISSN                     = {2192-8584},
  Keywords                 = {CMOS image sensor,OCIS codes: 110.4850,SafetyEYE,global shutter,high dynamic range CMOS (HDRC),safe camera system},
  Publisher                = {THOSS Media {\&} De Gruyter},
  Url                      = {https://www.degruyter.com/view/j/aot.2013.2.issue-2/aot-2012-0081/aot-2012-0081.xml}
}

@Article{sun2014application,
  Title                    = {Application of BW-ELM model on traffic sign recognition},
  Author                   = {Sun, Zhan-Li and Wang, Han and Lau, Wai-Shing and Seet, Gerald and Wang, Danwei},
  Journal                  = {Neurocomputing},
  Year                     = {2014},
  Pages                    = {153--159},
  Volume                   = {128},

  Publisher                = {Elsevier}
}

@Article{tan2016weakly,
  Title                    = {Weakly supervised metric learning for traffic sign recognition in a LIDAR-equipped vehicle},
  Author                   = {Tan, Min and Wang, Baoyuan and Wu, Zhaohui and Wang, Jingdong and Pan, Gang},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2016},
  Number                   = {5},
  Pages                    = {1415--1427},
  Volume                   = {17},

  Publisher                = {IEEE}
}

@Misc{tartanracing2005,
  Title                    = {{Boss at a glance}},

  Author                   = {{Tartan Racing}},
  Year                     = {2005},

  File                     = {::},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Url                      = {http://www.tartanracing.org/press/boss-glance.pdf}
}

@InProceedings{tatschke2006,
  Title                    = {{ProFusion2 - Towards a modular, robust and reliable fusion architecture for automotive environment perception}},
  Author                   = {Tatschke, Thomas and Park, S. B. and Amditis, Angelos and Polychronopoulos, Aristomenis and Scheunert, Ullrich and Aycard, Olivier},
  Booktitle                = {Advanced Microsystems for Automotive Applications 2006},
  Year                     = {2006},

  Address                  = {Berlin/Heidelberg},
  Pages                    = {451--469},
  Publisher                = {Springer-Verlag},

  Abstract                 = {This publication focuses on a modular architecture for
 sensor data fusion regarding to research work of common
 interest related to sensors and sensor data fusion. This
 architecture will be based on an extended environment model
 and representation, consisting of a set of common data
 structures for sensor, object and situation refinement data
 and algorithms as well as the corresponding models. The aim
 of such research is to contribute to a measurable
 enhancement of the output performance provided by
 multi-sensor systems in terms of actual availability,
 reliability, accuracy and precision of the perception
 results. In this connection, investigations towards fusion
 concepts and paradigms, such as
 {\^{a}}€˜redundant{\^{a}}€™ and
 {\^{a}}€˜complementary{\^{a}}€™, as well as
 {\^{a}}€˜early{\^{a}}€™ and track-based sensor data
 fusion approaches, are conducted, in order to significantly
 enhance the overall performance of the perception system.},
  Doi                      = {10.1007/3-540-33410-6_32},
  ISBN                     = {3540334092},
  Keywords                 = {Early fusion,Environment modelling,Environment perception,Fusion feedback,Fusion framework,Grid-based fusion,Multi level fusion,ProFusion2,Sensor data fusion,Track-based fusion},
  Url                      = {http://link.springer.com/10.1007/3-540-33410-6{\_}32
 http://lig-membres.imag.fr/aycard/html/Publications/2006/Tatschke06.pdf}
}

@Article{Thomas2013,
  Title                    = {{Identifying the causes of road crashes in Europe.}},
  Author                   = {Thomas, Pete and Morris, Andrew and Talbot, Rachel and Fagerlind, Helen},
  Journal                  = {Annals of advances in automotive medicine. Association for the Advancement of Automotive Medicine. Scientific Conference},
  Year                     = {2013},
  Pages                    = {13--22},
  Volume                   = {57},

  Abstract                 = {This research applies a recently developed model of accident causation, developed to investigate industrial accidents, to a specially gathered sample of 997 crashes investigated in-depth in 6 countries. Based on the work of Hollnagel the model considers a collision to be a consequence of a breakdown in the interaction between road users, vehicles and the organisation of the traffic environment. 54{\%} of road users experienced interpretation errors while 44{\%} made observation errors and 37{\%} planning errors. In contrast to other studies only 11{\%} of drivers were identified as distracted and 8{\%} inattentive. There was remarkably little variation in these errors between the main road user types. The application of the model to future in-depth crash studies offers the opportunity to identify new measures to improve safety and to mitigate the social impact of collisions. Examples given include the potential value of co-driver advisory technologies to reduce observation errors and predictive technologies to avoid conflicting interactions between road users.},
  File                     = {::},
  ISSN                     = {1943-2461},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Pmid                     = {24406942},
  Url                      = {http://www.ncbi.nlm.nih.gov/pubmed/24406942{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3861814}
}

@Misc{thorpe1997,
  Title                    = {{1997 Automated highway free agent demonstration}},

  Author                   = {Thorpe, Chuck and Jochem, Todd and Pomerleau, Dean},
  Year                     = {1997},

  Abstract                 = {In August of 1997, The US National Automated Highway
 System Consortium (NAHSC) presented a proof of technical
 feasibility demonstration of automated driving. The 97 Demo
 took place on car-pool lanes on I-15, in San Diego,
 California. Members of the Consortium demonstrated many
 different functions: Vision-based road following, Following
 magnetic nails, Following a radar reflective strip,
 Radar-based headway maintenance, Ladar-based headway
 maintenance, Evolutionary systems, Close vehicle following
 (platooning), Cooperative maneuvering, Obstacle detection
 and avoidance, Mixed automated and manual driving, Mixed
 automated cars and buses, and Semi-automated maintenance.
 CMU led the effort to build one of the seven demonstration
 scenarios, the Free Agent Demonstration (FAD). The FAD
 involved two fully automated cars, one partially automated
 car, and two fully automated city buses. The scenario
 demonstrates lane entry, speed and headway control, lane
 following, lane changing, obstacle detection, and
 cooperative maneuvers. This paper describes the free agent
 demonstration itself, the technology that made the
 demonstration possible, and the future work to analyze the
 feasibility of turning the demonstration system into a
 practical prototype.},
  Booktitle                = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
  Doi                      = {10.1109/ITSC.1997.660524},
  ISBN                     = {0-7803-4269-0},
  Pages                    = {496--501},
  Publisher                = {IEEE},
  Url                      = {http://ieeexplore.ieee.org/document/660524/
 http://www.scopus.com/inward/record.url?eid=2-s2.0-0031366442{\&}partnerID=tZOtx3y1}
}

@Article{thrun2006,
  Title                    = {{The robot that won the DARPA Grand Challenge.}},
  Author                   = {Thrun, Sebastian and Montemerlo, Mike and Dahlkamp, Hendrik and Et, Al.},
  Journal                  = {Journal of Field Robotics},
  Year                     = {2006},
  Number                   = {9},
  Pages                    = {661--692},
  Volume                   = {23-9},

  Abstract                 = {This article describes the robot Stanley, which won the
 2005 DARPA Grand Challenge. Stanley was developed for
 high-speed desert driving without manual intervention. The
 robot's software system relied predominately on
 state-of-the-art artificial intelligence technologies, such
 as machine learning and probabilistic reasoning. This paper
 describes the major components of this architecture, and
 discusses the results of the Grand Challenge race.},
  Doi                      = {10.1002/rob.20147},
  File                     = {::},
  ISSN                     = {1556-4967},
  Url                      = {www.interscience.wiley.com
 http://doi.wiley.com/10.1002/rob.20147}
}

@Article{timofte2014multi,
  Title                    = {Multi-view traffic sign detection, recognition, and 3d localisation},
  Author                   = {Timofte, Radu and Zimmermann, Karel and Van Gool, Luc},
  Journal                  = {Machine vision and applications},
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {633--647},
  Volume                   = {25},

  Publisher                = {Springer}
}

@TechReport{urmson2007,
  Title                    = {{DARPA Urban Challenge Final Report for Tartan Racing}},
  Author                   = {Urmson, Chris},
  Year                     = {2007},

  File                     = {::},
  Url                      = {https://pdfs.semanticscholar.org/3116/38e299acef3cbd3423649b77ef73c2a94fc1.pdf}
}

@Article{velez2017,
  Title                    = {{Embedding vision-based advanced driver assistance systems: a survey}},
  Author                   = {Velez, Gorka and Otaegui, Oihana},
  Journal                  = {IET Intelligent Transport Systems},
  Year                     = {2017},

  Month                    = {apr},
  Number                   = {3},
  Pages                    = {103--112},
  Volume                   = {11},

  Doi                      = {10.1049/iet-its.2016.0026},
  ISSN                     = {1751-956X},
  Url                      = {http://digital-library.theiet.org/content/journals/10.1049/iet-its.2016.0026}
}

@Article{vidal2017,
  Title                    = {{Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenarios}},
  Author                   = {Vidal, Antoni Rosinol and Rebecq, Henri and Horstschaefer, Timo and Scaramuzza, Davide},
  Journal                  = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  Year                     = {2017},
  Number                   = {2},
  Volume                   = {3},

  Abstract                 = {Event cameras are bio-inspired vision sensors that output
 pixel-level brightness changes instead of standard
 intensity frames. These cameras do not suffer from motion
 blur and have a very high dynamic range, which enables them
 to provide reliable visual information during high speed
 motions or in scenes characterized by high dynamic range.
 However, event cameras output only little information when
 the amount of motion is limited, such as in the case of
 almost still motion. Conversely, standard cameras provide
 instant and rich information about the environment most of
 the time (in low-speed and good lighting scenarios), but
 they fail severely in case of fast motions, or difficult
 lighting such as high dynamic range or low light scenes. In
 this paper, we present the first state estimation pipeline
 that leverages the complementary advantages of these two
 sensors by fusing in a tightly-coupled manner events,
 standard frames, and inertial measurements. We show on the
 publicly available Event Camera Dataset that our hybrid
 pipeline leads to an accuracy improvement of 130{\%} over
 event-only pipelines, and 85{\%} over standard-frames-only
 visual-inertial systems, while still being computationally
 tractable. Furthermore, we use our pipeline to demonstrate
 - to the best of our knowledge - the first autonomous
 quadrotor flight using an event camera for state
 estimation, unlocking flight scenarios that were not
 reachable with traditional visual-inertial odometry, such
 as low-light environments and high-dynamic range scenes.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1709.06310},
  Doi                      = {10.1109/LRA.2018.2793357},
  Eprint                   = {1709.06310},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Vidal et al. - 2017 -
  Ultimate SLAM Combining Events, Images, and IMU for Robust
  Visual SLAM in HDR and High Speed Scenarios.pdf:pdf},
  ISSN                     = {2377-3766},
  Url                      = {http://rpg.ifi.uzh.ch/docs/RAL18{\_}VidalRebecq.pdf
 http://arxiv.org/abs/1709.06310{\%}0Ahttp://dx.doi.org/10.1109/LRA.2018.2793357}
}

@Article{villalon2017traffic,
  Title                    = {Traffic sign detection system for locating road intersections and roundabouts: the Chilean case},
  Author                   = {Villal{\'o}n-Sep{\'u}lveda, Gabriel and Torres-Torriti, Miguel and Flores-Calero, Marco},
  Journal                  = {Sensors},
  Year                     = {2017},
  Number                   = {6},
  Pages                    = {1207},
  Volume                   = {17},

  Publisher                = {Multidisciplinary Digital Publishing Institute}
}

@Article{wali2015automatic,
  Title                    = {An automatic traffic sign detection and recognition system based on colour segmentation, shape matching, and svm},
  Author                   = {Wali, Safat B and Hannan, Mahammad A and Hussain, Aini and Samad, Salina A},
  Journal                  = {Mathematical Problems in Engineering},
  Year                     = {2015},
  Volume                   = {2015},

  Publisher                = {Hindawi}
}

@Article{wang2008,
  Title                    = {{Study on the measurement of the atmospheric extinction of fog and rain by forward-scattering near infrared spectroscopy}},
  Author                   = {Wang, M. and Liu, W.-Q. and Lu, Y.-H. and Zhao, X.-S. and Song, B.-C. and Zhang, Y.-J. and Wang, Y.-P. and Lian, C.-H. and Chen, Jun and Cheng, Yin and Liu, J.-G. and Wei, Q.-N.},
  Journal                  = {Guang Pu Xue Yu Guang Pu Fen Xi/Spectroscopy and Spectral Analysis},
  Year                     = {2008},

  Month                    = {aug},
  Number                   = {8},
  Pages                    = {1776--80},
  Volume                   = {28},

  Abstract                 = {In the visible and near IR, absorption is negligible so
 that the atmospheric extinction can be derived by
 atmospheric scattering which is mainly contributed by fog
 droplet, rain droplet, another types of droplet and small
 articles. The forward-scattering visibility meter (FVM)
 works by illuminating with near IR light a small sample
 volume of about 100 mL of air and measuring the intensity
 scattered in the angular range of 30° to 36° degrees. The
 scattered intensity is proportional to the extinction
 coefficient regardless of the article size distribution and
 after wavelength calibration. The ratio of scattered signal
 to extinction coefficient of fog and haze can be achieved
 by comparative test of FVM outputs and manual observations.
 Nevertheless, as a result of the application of the
 measurement during rain with the ratio of fog and haze, an
 unacceptable error is raised. To obtain an accuracy
 extinction measurement during rain, an appropriated ratio
 of scattered signal to extinction coefficient of rain would
 be found. The calculation for different size distributions
 of fog and rain with Mie theory has been made in this
 paper. And a comparison of extinction measurements made
 with two FVMs and manual observations during fog and rain
 has been made. The result shows that during rain the FVM
 extinction coefficient is from 20{\%} to 60{\%} greater
 than that of manual observations. This result can be used
 to define correction factors so that the FVM using
 forward-scattering near IR spectroscopy not only can be
 used to estimate extinction during fog and haze as well as
 during rain.},
  ISSN                     = {10000593},
  Keywords                 = {Atmospheric extinction,Extinction coefficient,Forward-scattering,Near IR,Phase function,Size distribution,Visibility},
  Pmid                     = {18975801},
  Url                      = {http://www.ncbi.nlm.nih.gov/pubmed/18975801}
}

@TechReport{Waymo2017,
  Title                    = {{On the Road to Fully Self-Driving}},
  Author                   = {Waymo},
  Institution              = {Waymo},
  Year                     = {2017},

  Abstract                 = {Waymo's mission is to bring self-driving technology to the world, making it safe and easy for people and things to move around. We believe our technology can improve mobility by giving people the freedom to get around, and save thousands of lives now lost to traffic crashes. o u r m i s s i o n 2 3 Waymo Safety Report On The Road to Fully Self-Driving Self-driving vehicles hold the promise to improve road safety and offer new mobility options to millions of people. Whether they're saving lives or helping people run errands, commute to work, or drop kids off at school, fully self-driving vehicles hold enormous potential to transform people's lives for the better. Safety is at the core of Waymo's mission — it's why we were founded over eight years ago as the Google self-driving car project. Every year, 1.2 million lives are lost to traffic crashes around the world, and in the U.S. the number of tragedies is growing. A common element of these crashes is that 94{\%} involve human error. Driving is not as safe or as easy as it should be, while distracted driving is on the rise. We believe our technology could save thousands of lives now lost to traffic crashes every year. Our commitment to safety is reflected in everything we do, from our company culture to how we design and test our technology. In this, our first Safety Report on Waymo's fully self-driving technology, we detail Waymo's work on — and our commitment to — safety. This overview of our safety program underscores the important lessons learned through the 3.5 million miles Waymo's vehicles have self-driven on public roads and through our billions of miles of simulated driving.},
  File                     = {:D$\backslash$:/Users/109123/Desktop/waymo-safety-report-2017-10.pdf:pdf},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Pages                    = {43},
  Url                      = {https://storage.googleapis.com/sdc-prod/v1/safety-report/waymo-safety-report-2017-10.pdf}
}

@InProceedings{weng2016road,
  Title                    = {Road traffic sign detection and classification from mobile LiDAR point clouds},
  Author                   = {Weng, Shengxia and Li, Jonathan and Chen, Yiping and Wang, Cheng},
  Booktitle                = {2nd ISPRS International Conference on Computer Vision in Remote Sensing (CVRS 2015)},
  Year                     = {2016},
  Organization             = {International Society for Optics and Photonics},
  Pages                    = {99010A},
  Volume                   = {9901}
}

@Article{yang2012automated,
  Title                    = {Automated extraction of road markings from mobile LiDAR point clouds},
  Author                   = {Yang, Bisheng and Fang, Lina and Li, Qingquan and Li, Jonathan},
  Journal                  = {Photogrammetric Engineering \& Remote Sensing},
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {331--338},
  Volume                   = {78},

  Publisher                = {American Society for Photogrammetry and Remote Sensing}
}

@Article{yang2016towards,
  Title                    = {Towards real-time traffic sign detection and classification},
  Author                   = {Yang, Yi and Luo, Hengliang and Xu, Huarong and Wu, Fuchao},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2016},
  Number                   = {7},
  Pages                    = {2022--2031},
  Volume                   = {17},

  Publisher                = {IEEE}
}

@Article{yebes2015,
  Title                    = {Visual Object Recognition with 3D-Aware Features in KITTI Urban Scenes},
  Author                   = {Yebes, J. Javier and Bergasa, Luis M. and Garc{\'{i}}a-Garrido, Miguel {\'{A}}ngel},
  Journal                  = {Sensors (Basel, Switzerland)},
  Year                     = {2015},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {9228--9250},
  Volume                   = {15},

  Abstract                 = {Driver assistance systems and autonomous robotics rely on
 the deployment of several sensors for environment
 perception. Compared to LiDAR systems, the inexpensive
 vision sensors can capture the 3D scene as perceived by a
 driver in terms of appearance and depth cues. Indeed,
 providing 3D image understanding capabilities to vehicles
 is an essential target in order to infer scene semantics in
 urban environments. One of the challenges that arises from
 the navigation task in naturalistic urban scenarios is the
 detection of road participants (e.g., cyclists, pedestrians
 and vehicles). In this regard, this paper tackles the
 detection and orientation estimation of cars, pedestrians
 and cyclists, employing the challenging and naturalistic
 KITTI images. This work proposes 3D-aware features computed
 from stereo color images in order to capture the appearance
 and depth peculiarities of the objects in road scenes. The
 successful part-based object detector, known as DPM, is
 extended to learn richer models from the 2.5D data (color
 and disparity), while also carrying out a detailed analysis
 of the training pipeline. A large set of experiments
 evaluate the proposals, and the best performing approach is
 ranked on the KITTI website. Indeed, this is the first work
 that reports results with stereo data for the KITTI object
 challenge, achieving increased detection ratios for the
 classes car and cyclist compared to a baseline DPM.},
  Doi                      = {10.3390/s150409228},
  ISBN                     = {14248220},
  ISSN                     = {14248220},
  Keywords                 = {2.5D},
  Mendeley-tags            = {2.5D},
  Pmid                     = {102279770},
  Url                      = {http://www.ncbi.nlm.nih.gov/pubmed/25903553
 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4431302
 http://www.mdpi.com/1424-8220/15/4/9228/}
}

@PhdThesis{Yenkanchi2016,
  Title                    = {{Multi Sensor Data Fusion for Autonomous Vehicles}},
  Author                   = {Yenkanchi, Shashibushan},
  School                   = {University of Windsor},
  Year                     = {2016},

  File                     = {::},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Pages                    = {68},
  Url                      = {https://scholar.uwindsor.ca/etd/5680}
}

@Article{zaklouta2014real,
  Title                    = {Real-time traffic sign recognition in three stages},
  Author                   = {Zaklouta, Fatin and Stanciulescu, Bogdan},
  Journal                  = {Robotics and autonomous systems},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {16--24},
  Volume                   = {62},

  Publisher                = {Elsevier}
}

@PhdThesis{zhang2016,
  Title                    = {{Rapid Inspection of Pavement Markings Using Mobile Laser Scanning Point Clouds}},
  Author                   = {Zhang, Haocheng},
  School                   = {University of Waterloo},
  Year                     = {2016},
  Month                    = {mar},

  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Zhang - 2016 - Rapid
  Inspection of Pavement Markings Using Mobile Laser Scanning
  Point Clouds.pdf:pdf},
  Url                      = {https://uwspace.uwaterloo.ca/handle/10012/10343}
}

@Article{zhang2017real,
  Title                    = {A Real-Time Chinese Traffic Sign Detection Algorithm Based on Modified YOLOv2},
  Author                   = {Zhang, Jianming and Huang, Manting and Jin, Xiaokang and Li, Xudong},
  Journal                  = {Algorithms},
  Year                     = {2017},
  Number                   = {4},
  Pages                    = {127},
  Volume                   = {10},

  Publisher                = {Multidisciplinary Digital Publishing Institute}
}

@Article{zhang2016a,
  Title                    = {{Unified saliency detection model using color and texture features}},
  Author                   = {Zhang, Libo and Yang, Lin and Luo, Tiejian},
  Journal                  = {PLoS ONE},
  Year                     = {2016},
  Number                   = {2},
  Pages                    = {e0149328},
  Volume                   = {11},

  Abstract                 = {Saliency detection attracted attention of many researchers
 and had become a very active area of research. Recently,
 many saliency detection models have been proposed and
 achieved excellent performance in various fields. However,
 most of these models only consider low-level features. This
 paper proposes a novel saliency detection model using both
 color and texture features and incorporating higher-level
 priors. The SLIC superpixel algorithm is applied to form an
 over-segmentation of the image. Color saliency map and
 texture saliency map are calculated based on the region
 contrast method and adaptive weight. Higher-level priors
 including location prior and color prior are incorporated
 into the model to achieve a better performance and full
 resolution saliency map is obtained by using the
 up-sampling method. Experimental results on three datasets
 demonstrate that the proposed saliency detection model
 outperforms the state-of-the-art models.},
  Doi                      = {10.1371/journal.pone.0149328},
  ISSN                     = {19326203},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Pmid                     = {26889826},
  Publisher                = {Public Library of Science},
  Url                      = {http://www.ncbi.nlm.nih.gov/pubmed/26889826
 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4758633}
}

@InProceedings{zhao2012curb,
  Title                    = {Curb detection and tracking using 3D-LIDAR scanner},
  Author                   = {Zhao, Gangqiang and Yuan, Junsong},
  Booktitle                = {Image Processing (ICIP), 2012 19th IEEE International Conference on},
  Year                     = {2012},
  Organization             = {IEEE},
  Pages                    = {437--440}
}

@InProceedings{zhou2014lidar,
  Title                    = {LIDAR and vision-based real-time traffic sign detection and recognition algorithm for intelligent vehicle},
  Author                   = {Zhou, Lipu and Deng, Zhidong},
  Booktitle                = {Intelligent Transportation Systems (ITSC), 2014 IEEE 17th International Conference on},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {578--583}
}

@InCollection{ziebinski2016,
  Title                    = {{A Survey of ADAS Technologies for the Future Perspective of Sensor Fusion}},
  Author                   = {Ziebinski, Adam and Cupek, Rafal and Erdogan, Hueseyin and Waechter, Sonja},
  Publisher                = {Springer, Cham},
  Year                     = {2016},
  Month                    = {sep},
  Pages                    = {135--146},

  Doi                      = {10.1007/978-3-319-45246-3_13},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
  Ltd./Mendeley Desktop/Downloaded/Ziebinski et al. - 2016 -
  A Survey of ADAS Technologies for the Future Perspective of
  Sensor Fusion.pdf:pdf},
  Url                      = {http://link.springer.com/10.1007/978-3-319-45246-3{\_}13}
}

@Article{Ziebinski2016a,
  Title                    = {{A survey of ADAS technologies for the future perspective of sensor fusion}},
  Author                   = {Ziebinski, Adam and Cupek, Rafal and Erdogan, Hueseyin and Waechter, Sonja},
  Journal                  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  Year                     = {2016},
  Pages                    = {135--146},
  Volume                   = {9876 LNCS},

  Abstract                 = {A performance analysis of a modified self-regulated self-excited$\backslash$nsingle-phase induction generator is presented. This generator consists$\backslash$nof a three-phase squirrel-cage induction machine Y-connected with three$\backslash$ncapacitors, and one of these capacitors is connected with a single-phase$\backslash$nload. Using a circuit model, the generator's steady-state performance is$\backslash$ntheoretically predicted, and confirmed through experiment},
  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1011.1669v3},
  Doi                      = {10.1007/978-3-319-45246-3_13},
  Eprint                   = {arXiv:1011.1669v3},
  File                     = {::},
  ISBN                     = {978-3-642-40494-8},
  ISSN                     = {16113349},
  Keywords                 = {ADAS,Camera sensor,Lidar sensor,Radar sensor,Sensor fusion},
  Mendeley-groups          = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  Pmid                     = {25246403},
  Publisher                = {Springer, Cham},
  Url                      = {http://link.springer.com/10.1007/978-3-319-45246-3{\_}13}
}

@Article{ziegler2014,
  Title                    = {{Making bertha drive-an autonomous journey on a historic route}},
  Author                   = {Ziegler, Julius and Bender, Philipp and Schreiber, Markus and Lategahn, Henning and Strauss, Tobias and Stiller, Christoph and Dang, Thao and Franke, Uwe and Appenrodt, Nils and Keller, Christoph G. and Kaus, Eberhard and Herrtwich, Ralf G. and Rabe, Clemens and Pfeiffer, David and Lindner, Frank and Stein, Fridtjof and Erbs, Friedrich and Enzweiler, Markus and Knoppel, Carsten and Hipp, Jochen and Haueis, Martin and Trepte, Maximilian and Brenk, Carsten and Tamke, Andreas and Ghanaat, Mohammad and Braun, Markus and Joos, Armin and Fritz, Hans and Mock, Horst and Hein, Martin and Zeeb, Eberhard},
  Journal                  = {IEEE Intelligent Transportation Systems Magazine},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {8--20},
  Volume                   = {6},

  Abstract                 = {125 years after Bertha Benz completed the first overland
 journey in automotive history, the Mercedes Benz S-Class S
 500 INTELLIGENT DRIVE followed the same route from Mannheim
 to Pforzheim, Germany, in fully autonomous manner. The
 autonomous vehicle was equipped with close-to-production
 sensor hardware and relied solely on vision and radar
 sensors in combination with accurate digital maps to obtain
 a comprehensive understanding of complex traffic
 situations. The historic Bertha Benz Memorial Route is
 particularly challenging for autonomous driving. The course
 taken by the autonomous vehicle had a length of 103 km and
 covered rural roads, 23 small villages and major cities
 (e.g. downtown Mannheim and Heidelberg). The route posed a
 large variety of difficult traffic scenarios including
 intersections with and without traffic lights, roundabouts,
 and narrow passages with oncoming traffic. This paper gives
 an overview of the autonomous vehicle and presents details
 on vision and radar-based perception, digital road maps and
 video-based self-localization, as well as motion planning
 in complex urban scenarios.},
  Doi                      = {10.1109/MITS.2014.2306552},
  ISBN                     = {1939-1390},
  ISSN                     = {19391390},
  Url                      = {http://ieeexplore.ieee.org/document/6803933/}
}

@Misc{EULawandPublications2004,
  Title                    = {{2004/545/EC: Commission Decision of 8 July 2004 on the harmonisation of radio spectrum in the 79 GHz range for the use of automotive short-range radar equipment in the Community}},
  Year                     = {2004},

  Booktitle                = {EU Law and Publications},
  Mendeley-groups          = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
  Pages                    = {2},
  Url                      = {https://publications.europa.eu/en/publication-detail/-/publication/9d425670-b54b-4c65-8461-824dbf71facc/language-en},
  Urldate                  = {2018-07-19}
}

@comment{jabref-meta: databaseType:bibtex;}

