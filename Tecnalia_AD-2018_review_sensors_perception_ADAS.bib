% Encoding: UTF-8

@InProceedings{bak2012,
  author    = {Bak, Adrien and Gruyer, Dominique and Bouchafa, Samia and Aubert, Didier},
  title     = {Multi-sensor localization - Visual Odometry as a low cost proprioceptive sensor},
  booktitle = {2012 15th International IEEE Conference on Intelligent Transportation Systems},
  year      = {2012},
  pages     = {1365--1370},
  month     = {sep},
  publisher = {IEEE},
  doi       = {10.1109/ITSC.2012.6338771},
  isbn      = {978-1-4673-3063-3},
  url       = {http://ieeexplore.ieee.org/document/6338771/},
}

@Article{besbes2015,
  author    = {Besbes, Bassem and Rogozan, Alexandrina and Rus, Adela Maria and Bensrhair, Abdelaziz and Broggi, Alberto},
  title     = {Pedestrian detection in far-infrared daytime images using a hierarchical codebook of SURF},
  journal   = {Sensors (Switzerland)},
  year      = {2015},
  volume    = {15},
  number    = {4},
  pages     = {8570--8594},
  month     = {apr},
  issn      = {14248220},
  abstract  = {One of the main challenges in intelligent vehicles
		  concerns pedestrian detection for driving assistance.
		  Recent experiments have showed that state-of-the-art
		  descriptors provide better performances on the far-infrared
		  (FIR) spectrum than on the visible one, even in daytime
		  conditions, for pedestrian classification. In this paper,
		  we propose a pedestrian detector with on-board FIR camera.
		  Our main contribution is the exploitation of the specific
		  characteristics of FIR images to design a fast,
		  scale-invariant and robust pedestrian detector. Our system
		  consists of three modules, each based on speeded-up robust
		  feature (SURF) matching. The first module allows generating
		  regions-of-interest (ROI), since in FIR images of the
		  pedestrian shapes may vary in large scales, but heads
		  appear usually as light regions. ROI are detected with a
		  high recall rate with the hierarchical codebook of SURF
		  features located in head regions. The second module
		  consists of pedestrian full-body classification by using
		  SVM. This module allows one to enhance the precision with
		  low computational cost. In the third module, we combine the
		  mean shift algorithm with inter-frame scale-invariant SURF
		  feature tracking to enhance the robustness of our system.
		  The experimental evaluation shows that our system
		  outperforms, in the FIR domain, the state-of-the-art
		  Haar-like Adaboost-cascade, histogram of oriented gradients
		  (HOG)/linear SVM (linSVM) and MultiFtrpedestrian detectors,
		  trained on the FIR images.},
  doi       = {10.3390/s150408570},
  file      = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Besbes et al. - 2015 -
		  Pedestrian Detection in Far-Infrared Daytime Images Using a
		  Hierarchical Codebook of SURF.pdf:pdf},
  keywords  = {Far-infrared images,Hierarchical codebook,Pedestrian classification and trackings,Pedestrian detection,SURF,SVM,Scale-invariant feature matching},
  pmid      = {25871724},
  publisher = {Multidisciplinary Digital Publishing Institute},
  url       = {http://www.mdpi.com/1424-8220/15/4/8570/},
}

@Article{castilloaguilar2015,
  author    = {{Castillo Aguilar}, Juan Jes{\'{u}}s and {Cabrera Carrillo}, Juan Antonio and {Guerra Fern{\'{a}}ndez}, Antonio Jes{\'{u}}s and {Carabias Acosta}, Enrique},
  title     = {Robust road condition detection system using in-vehicle standard sensors},
  journal   = {Sensors (Switzerland)},
  year      = {2015},
  volume    = {15},
  number    = {12},
  pages     = {32056--32078},
  month     = {dec},
  issn      = {14248220},
  abstract  = {The appearance of active safety systems, such as Anti-lock
		  Braking System, Traction Control System, Stability Control
		  System, etc., represents a major evolution in road safety.
		  In the automotive sector, the term vehicle active safety
		  systems refers to those whose goal is to help avoid a crash
		  or to reduce the risk of having an accident. These systems
		  safeguard us, being in continuous evolution and
		  incorporating new capabilities continuously. In order for
		  these systems and vehicles to work adequately, they need to
		  know some fundamental information: the road condition on
		  which the vehicle is circulating. This early road detection
		  is intended to allow vehicle control systems to act faster
		  and more suitably, thus obtaining a substantial advantage.
		  In this work, we try to detect the road condition the
		  vehicle is being driven on, using the standard sensors
		  installed in commercial vehicles. Vehicle models were
		  programmed in on-board systems to perform real-time
		  estimations of the forces of contact between the wheel and
		  road and the speed of the vehicle. Subsequently, a fuzzy
		  logic block is used to obtain an index representing the
		  road condition. Finally, an artificial neural network was
		  used to provide the optimal slip for each surface.
		  Simulations and experiments verified the proposed method.},
  doi       = {10.3390/s151229908},
  file      = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Castillo Aguilar et al. -
		  2015 - Robust Road Condition Detection System Using
		  In-Vehicle Standard Sensors.pdf:pdf},
  keywords  = {Friction estimation,Normal driving,Optimal slip estimation,Standard vehicle sensor},
  pmid      = {26703605},
  publisher = {Multidisciplinary Digital Publishing Institute},
  url       = {http://www.mdpi.com/1424-8220/15/12/29908},
}

@Article{chen2017turn,
  Title                    = {Turn signal detection during nighttime by CNN detector and perceptual hashing tracking},
  Author                   = {Chen, Long and Hu, Xuemin and Xu, Tong and Kuang, Hulin and Li, Qingquan},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2017},
  Number                   = {12},
  Pages                    = {3303--3314},
  Volume                   = {18},

  Publisher                = {IEEE}
}

@Article{coronado2012detection,
  Title                    = {Detection and classification of road signs for automatic inventory systems using computer vision},
  Author                   = {Coronado, Gustavo A Pel{\'a}ez and Mu{\~n}oz, Mar{\'\i}a Romero and Armingol, Jos{\'e} Mar{\'\i}a and de la Escalera, Arturo and Mu{\~n}oz, Juan Jes{\'u}s and van Bijsterveld, Wouter and Bola{\~n}o, Juan Antonio},
  Journal                  = {Integrated Computer-Aided Engineering},
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {285--298},
  Volume                   = {19},

  Publisher                = {IOS Press}
}

@InProceedings{Dagan2004,
  Title                    = {Forward collision warning with a single camera},
  Author                   = {Dagan, E. and Mano, O. and Stein, G.P. and Shashua, A.},
  Booktitle                = {IEEE Intelligent Vehicles Symposium, 2004},
  Year                     = {2004},
  Pages                    = {37--42},
  Publisher                = {IEEE},

  Abstract                 = {The large number of rear end collisions due to driver inattention has been identified as a major automotive safety issue. Even a short advance warning can significantly reduce the number and severity of the collisions. This paper describes a vision based forward collision warning (FCW) system for highway safety. The algorithm described in this paper computes time to contact (TTC) and possible collision course directly from the size and position of the vehicles in the image - which are the natural measurements for a vision based system - without having to compute a 3D representation of the scene. The use of a single low cost image sensor results in an affordable system which is simple to install. The system has been implemented on real-time hardware and has been test driven on highways. Collision avoidance tests have also been performed on test tracks.},
  Doi                      = {10.1109/IVS.2004.1336352},
  ISBN                     = {0-7803-8310-9},
  Url                      = {http://ieeexplore.ieee.org/document/1336352/}
}

@InProceedings{frejlichowski2015application,
  Title                    = {Application of the Polar--Fourier Greyscale Descriptor to the Automatic Traffic Sign Recognition},
  Author                   = {Frejlichowski, Dariusz},
  Booktitle                = {International Conference Image Analysis and Recognition},
  Year                     = {2015},
  Organization             = {Springer},
  Pages                    = {506--513}
}

@InProceedings{gao2015learning,
  Title                    = {Learning local histogram representation for efficient traffic sign recognition},
  Author                   = {Gao, Jinlu and Fang, Yuqiang and Li, Xingwei},
  Booktitle                = {Image and Signal Processing (CISP), 2015 8th International Congress on},
  Year                     = {2015},
  Organization             = {IEEE},
  Pages                    = {631--635}
}

@Article{gargoum2017automated,
  Title                    = {Automated highway sign extraction using lidar data},
  Author                   = {Gargoum, Suliman and El-Basyouny, Karim and Sabbagh, Joseph and Froese, Kenneth},
  Journal                  = {Transportation Research Record: Journal of the Transportation Research Board},
  Year                     = {2017},
  Number                   = {2643},
  Pages                    = {1--8},

  Publisher                = {Transportation Research Board of the National Academies}
}

@InProceedings{gu2011traffic,
  Title                    = {Traffic sign detection in dual-focal active camera system},
  Author                   = {Gu, Yanlei and Yendo, Tomohiro and Tehrani, Mehrdad Panahpour and Fujii, Toshiaki and Tanimoto, Masayuki},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2011 IEEE},
  Year                     = {2011},
  Organization             = {IEEE},
  Pages                    = {1054--1059}
}

@Article{guan2018robust,
  Title                    = {Robust Traffic-Sign Detection and Classification Using Mobile LiDAR Data With Digital Images},
  Author                   = {Guan, Haiyan and Yan, Wanqian and Yu, Yongtao and Zhong, Liang and Li, Dilong},
  Journal                  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  Year                     = {2018},

  Publisher                = {IEEE}
}

@Article{hillel2014recent,
  Title                    = {Recent progress in road and lane detection: a survey},
  Author                   = {Hillel, Aharon Bar and Lerner, Ronen and Levi, Dan and Raz, Guy},
  Journal                  = {Machine vision and applications},
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {727--745},
  Volume                   = {25},

  Publisher                = {Springer}
}

@Article{hosseinyalamdary2017bayesian,
  Title                    = {A Bayesian approach to traffic light detection and mapping},
  Author                   = {Hosseinyalamdary, Siavash and Yilmaz, Alper},
  Journal                  = {ISPRS journal of photogrammetry and remote sensing},
  Year                     = {2017},
  Pages                    = {184--192},
  Volume                   = {125},

  Publisher                = {Elsevier}
}

@InProceedings{Ieng2003,
  Title                    = {Merging lateral cameras information with proprioceptive sensors in vehicle location gives centimetric precision},
  Author                   = {Ieng, S-S and Gruyer, D},
  Booktitle                = {Proceedings of the 18th International Technical Conference on the Enhanced Safety of Vehicles},
  Year                     = {2003},

  Address                  = {Nagoya, Japan},
  Month                    = {may},
  Publisher                = {National Highway Traffic Safety Administration},

  Url                      = {https://trid.trb.org/view.aspx?id=750799}
}

@InProceedings{janda2013,
  author    = {Janda, Florian and Pangerl, Sebastian and Lang, Eva and Fuchs, Erich},
  title     = {Road boundary detection for run-off road prevention based on the fusion of video and radar},
  booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
  year      = {2013},
  pages     = {1173--1178},
  month     = {jun},
  publisher = {IEEE},
  abstract  = {An approach for detecting the road boundary on different
		  types of roads without any preliminary knowledge is
		  presented. We fuse information obtained from an algorithm
		  which detects road markings and road edges in images
		  acquired by a video camera as well as data from a radar
		  sensor. Each road marking, each road edge and each road
		  barrier is tracked individually. Hence we can even capture
		  exits or laybys. We use an edge image for road marking
		  detection and texture information for road edge detection.
		  Additional data provided by a radar sensor is used to
		  measure targets referring to static barriers along the road
		  side such as guardrails. The output of each processing unit
		  is fused into a Kalman filter framework, where the
		  confidence of each subsystem influences the innovation of
		  the overall system. The underlying geometric road model
		  comprises parameters for multiple lanes, the flanking road
		  edge as well as the vehicle's relative pose. The work is
		  part of the project Interactive.},
  doi       = {10.1109/IVS.2013.6629625},
  isbn      = {9781467327558},
  issn      = {1931-0587},
  url       = {http://ieeexplore.ieee.org/document/6629625/},
}

@Article{kaliyaperumal2001algorithm,
  Title                    = {An algorithm for detecting roads and obstacles in radar images},
  Author                   = {Kaliyaperumal, Kesav and Lakshmanan, Sridhar and Kluge, Karl},
  Journal                  = {IEEE Transactions on Vehicular Technology},
  Year                     = {2001},
  Number                   = {1},
  Pages                    = {170--182},
  Volume                   = {50},

  Publisher                = {IEEE}
}

@InProceedings{kim2015lane,
  Title                    = {Lane map building and localization for automated driving using 2D laser rangefinder},
  Author                   = {Kim, Dongwook and Chung, Taeyoung and Yi, Kyongsu},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2015 IEEE},
  Year                     = {2015},
  Organization             = {IEEE},
  Pages                    = {680--685}
}

@Article{koch2011,
  author    = {Koch, Christian and Brilakis, Ioannis},
  title     = {Pothole detection in asphalt pavement images},
  journal   = {Advanced Engineering Informatics},
  year      = {2011},
  volume    = {25},
  number    = {3},
  pages     = {507--515},
  month     = {aug},
  issn      = {14740346},
  abstract  = {Pavement condition assessment is essential when developing
		  road network maintenance programs. In practice, the data
		  collection process is to a large extent automated. However,
		  pavement distress detection (cracks, potholes, etc.) is
		  mostly performed manually, which is labor-intensive and
		  time-consuming. Existing methods either rely on complete 3D
		  surface reconstruction, which comes along with high
		  equipment and computation costs, or make use of
		  acceleration data, which can only provide preliminary and
		  rough condition surveys. In this paper we present a method
		  for automated pothole detection in asphalt pavement images.
		  In the proposed method an image is first segmented into
		  defect and non-defect regions using histogram shape-based
		  thresholding. Based on the geometric properties of a defect
		  region the potential pothole shape is approximated
		  utilizing morphological thinning and elliptic regression.
		  Subsequently, the texture inside a potential defect shape
		  is extracted and compared with the texture of the
		  surrounding non-defect pavement in order to determine if
		  the region of interest represents an actual pothole. This
		  methodology has been implemented in a MATLAB prototype,
		  trained and tested on 120 pavement images. The results show
		  that this method can detect potholes in asphalt pavement
		  images with reasonable accuracy. {\textcopyright} 2011
		  Elsevier Ltd. All rights reserved.},
  doi       = {10.1016/j.aei.2011.01.002},
  isbn      = {1474-0346},
  keywords  = {Image processing,Pavement assessment,Pothole detection,Visual sensing},
  publisher = {Elsevier},
  url       = {http://www.sciencedirect.com/science/article/pii/S1474034611000036},
}

@InProceedings{kum2013lane,
  Title                    = {Lane detection system with around view monitoring for intelligent vehicle},
  Author                   = {Kum, Chang-Hoon and Cho, Dong-Chan and Ra, Moon-Soo and Kim, Whoi-Yul},
  Booktitle                = {SoC Design Conference (ISOCC), 2013 International},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {215--218}
}

@InProceedings{lee2017avm,
  Title                    = {AVM/LiDAR sensor based lane marking detection method for automated driving on complex urban roads},
  Author                   = {Lee, Hyunsung and Kim, Seonwook and Park, Sungyoul and Jeong, Yonghwan and Lee, Hojoon and Yi, Kyongsu},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2017 IEEE},
  Year                     = {2017},
  Organization             = {IEEE},
  Pages                    = {1434--1439}
}

@InProceedings{li2013new,
  Title                    = {A new 3D LIDAR-based lane markings recognition approach},
  Author                   = {Li, Tan and Zhidong, Deng},
  Booktitle                = {Robotics and Biomimetics (ROBIO), 2013 IEEE International Conference on},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {2197--2202}
}

@Article{liu2014traffic,
  Title                    = {Traffic sign recognition using group sparse coding},
  Author                   = {Liu, Huaping and Liu, Yulong and Sun, Fuchun},
  Journal                  = {Information Sciences},
  Year                     = {2014},
  Pages                    = {75--89},
  Volume                   = {266},

  Publisher                = {Elsevier}
}

@Article{liu2015,
  author    = {Liu, Jun and Han, Jiuqiang and Lv, Hongqiang and Li, Bing},
  title     = {An ultrasonic sensor system based on a two-dimensional state method for highway vehicle violation detection applications},
  journal   = {Sensors (Switzerland)},
  year      = {2015},
  volume    = {15},
  number    = {4},
  pages     = {9000--9021},
  month     = {apr},
  issn      = {14248220},
  abstract  = {With the continuing growth of highway construction and
		  vehicle use expansion all over the world, highway vehicle
		  traffic rule violation (TRV) detection has become more and
		  more important so as to avoid traffic accidents and
		  injuries in intelligent transportation systems (ITS) and
		  vehicular ad hoc networks (VANETs). Since very few works
		  have contributed to solve the TRV detection problem by
		  moving vehicle measurements and surveillance devices, this
		  paper develops a novel parallel ultrasonic sensor system
		  that can be used to identify the TRV behavior of a host
		  vehicle in real-time. Then a two-dimensional state method
		  is proposed, utilizing the spacial state and time
		  sequential states from the data of two parallel ultrasonic
		  sensors to detect and count the highway vehicle violations.
		  Finally, the theoretical TRV identification probability is
		  analyzed, and actual experiments are conducted on different
		  highway segments with various driving speeds, which
		  indicates that the identification accuracy of the proposed
		  method can reach about 90.97{\%}.},
  doi       = {10.3390/s150409000},
  file      = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - An
		  Ultrasonic Sensor System Based on a Two-Dimensional State
		  Method for Highway Vehicle Violation Detection
		  Applicat.pdf:pdf},
  keywords  = {Highway vehicle traffic rule violation detection,Intelligent transportation systems,Two-dimensional state method,Ultrasonic sensor system},
  pmid      = {1603305},
  publisher = {Multidisciplinary Digital Publishing Institute},
  url       = {http://www.mdpi.com/1424-8220/15/4/9000/},
}

@Article{lundquist2011,
  author   = {Lundquist, Christian and Sch{\"{o}}n, Thomas B.},
  title    = {Joint ego-motion and road geometry estimation},
  journal  = {Information Fusion},
  year     = {2011},
  volume   = {12},
  number   = {4},
  pages    = {253--263},
  month    = {oct},
  issn     = {15662535},
  abstract = {We provide a sensor fusion framework for solving the
		  problem of joint ego-motion and road geometry estimation.
		  More specifically we employ a sensor fusion framework to
		  make systematic use of the measurements from a forward
		  looking radar and camera, steering wheel angle sensor,
		  wheel speed sensors and inertial sensors to compute good
		  estimates of the road geometry and the motion of the ego
		  vehicle on this road. In order to solve this problem we
		  derive dynamical models for the ego vehicle, the road and
		  the leading vehicles. The main difference to existing
		  approaches is that we make use of a new dynamic model for
		  the road. An extended Kalman filter is used to fuse data
		  and to filter measurements from the camera in order to
		  improve the road geometry estimate. The proposed solution
		  has been tested and compared to existing algorithms for
		  this problem, using measurements from authentic traffic
		  environments on public roads in Sweden. The results clearly
		  indicate that the proposed method provides better
		  estimates. ?? 2011 Elsevier B.V. All rights reserved.},
  doi      = {10.1016/j.inffus.2010.06.007},
  isbn     = {1566-2535},
  keywords = {Bicycle model,Extended Kalman filter,Road geometry estimation,Sensor fusion,Single track model},
  url      = {http://linkinghub.elsevier.com/retrieve/pii/S1566253510000709},
}

@Article{ma2000simultaneous,
  Title                    = {Simultaneous detection of lane and pavement boundaries using model-based multisensor fusion},
  Author                   = {Ma, Bing and Lakshmanan, Sridhar and Hero, Alfred O},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2000},
  Number                   = {3},
  Pages                    = {135--147},
  Volume                   = {1},

  Publisher                = {IEEE}
}

@Article{miyata2017automatic,
  Title                    = {Automatic Recognition of Speed Limits on Speed-Limit Signs by Using Machine Learning},
  Author                   = {Miyata, Shigeharu},
  Journal                  = {Journal of Imaging},
  Year                     = {2017},
  Number                   = {3},
  Pages                    = {25},
  Volume                   = {3},

  Publisher                = {Multidisciplinary Digital Publishing Institute}
}

@Article{nguyen2013,
  author    = {Nguyen, Van Cuong and Heo, Moon Beom and Jee, Gyu-In},
  title     = {1-Point Ransac Based Robust Visual Odometry},
  journal   = {Journal of Positioning, Navigation, and Timing},
  year      = {2013},
  volume    = {2},
  number    = {1},
  pages     = {81--89},
  month     = {apr},
  issn      = {2288-8187},
  doi       = {10.11003/JKGS.2013.2.1.081},
  keywords  = {1-point method,Ackermann's principle,Bundle Adjustment,kpubs,kpubs.org,rotation estimation},
  publisher = {The Korean GNSS Society},
  url       = {http://koreascience.or.kr/journal/view.jsp?kj=HOHSB0{\&}py=2013{\&}vnc=v2n1{\&}sp=81},
}

@InProceedings{nie2012camera,
  Title                    = {Camera and lidar fusion for road intersection detection},
  Author                   = {Nie, Yiming and Chen, Qingyang and Chen, Tongtong and Sun, Zhenping and Dai, Bin},
  Booktitle                = {Electrical \& Electronics Engineering (EEESYM), 2012 IEEE Symposium on},
  Year                     = {2012},
  Organization             = {IEEE},
  Pages                    = {273--276}
}

@Article{ozgunalp2017multiple,
  Title                    = {Multiple lane detection algorithm based on novel dense vanishing point estimation},
  Author                   = {Ozgunalp, Umar and Fan, Rui and Ai, Xiao and Dahnoun, Naim},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2017},
  Number                   = {3},
  Pages                    = {621--632},
  Volume                   = {18},

  Publisher                = {IEEE}
}

@Article{reina2015,
  author    = {Reina, Giulio and Johnson, David and Underwood, James},
  title     = {Radar sensing for intelligent vehicles in urban environments},
  journal   = {Sensors (Switzerland)},
  year      = {2015},
  volume    = {15},
  number    = {6},
  pages     = {14661--14678},
  month     = {jun},
  issn      = {14248220},
  abstract  = {Radar overcomes the shortcomings of laser, stereovision,
		  and sonar because it can operate successfully in dusty,
		  foggy, blizzard-blinding, and poorly lit scenarios. This
		  paper presents a novel method for ground and obstacle
		  segmentation based on radar sensing. The algorithm operates
		  directly in the sensor frame, without the need for a
		  separate synchronised navigation source, calibration
		  parameters describing the location of the radar in the
		  vehicle frame, or the geometric restrictions made in the
		  previous main method in the field. Experimental results are
		  presented in various urban scenarios to validate this
		  approach, showing its potential applicability for advanced
		  driving assistance systems and autonomous vehicle
		  operations.},
  doi       = {10.3390/s150614661},
  file      = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Reina, Johnson, Underwood
		  - 2015 - Radar Sensing for Intelligent Vehicles in Urban
		  Environments.pdf:pdf},
  keywords  = {Navigation systems,Perception in urban environment,Radar sensing,Robotic intelligent vehicles,Unmanned ground vehicles},
  publisher = {Multidisciplinary Digital Publishing Institute},
  url       = {http://www.mdpi.com/1424-8220/15/6/14661/},
}

@Article{santana2016,
  author        = {Santana, Eder and Hotz, George},
  title         = {Learning a Driving Simulator},
  year          = {2016},
  month         = {aug},
  abstract      = {Comma.ai's approach to Artificial Intelligence for
		  self-driving cars is based on an agent that learns to clone
		  driver behaviors and plans maneuvers by simulating future
		  events in the road. This paper illustrates one of our
		  research approaches for driving simulation. One where we
		  learn to simulate. Here we investigate variational
		  autoencoders with classical and learned cost functions
		  using generative adversarial networks for embedding road
		  frames. Afterwards, we learn a transition model in the
		  embedded space using action conditioned Recurrent Neural
		  Networks. We show that our approach can keep predicting
		  realistic looking video for several frames despite the
		  transition model being optimized without a cost function in
		  the pixel space.},
  archiveprefix = {arXiv},
  arxivid       = {1608.01230},
  eprint        = {1608.01230},
  file          = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Santana, Hotz - 2016 -
		  Learning a Driving Simulator.pdf:pdf},
  url           = {http://arxiv.org/abs/1608.01230},
}

@Article{scaramuzza2008,
  author  = {Scaramuzza, D. and Siegwart, R.},
  title   = {Appearance-Guided Monocular Omnidirectional Visual Odometry for Outdoor Ground Vehicles},
  journal = {IEEE Transactions on Robotics},
  year    = {2008},
  volume  = {24},
  number  = {5},
  pages   = {1015--1026},
  month   = {oct},
  issn    = {1552-3098},
  doi     = {10.1109/TRO.2008.2004490},
  url     = {http://ieeexplore.ieee.org/document/4625958/},
}

@InProceedings{schreiber2013laneloc,
  Title                    = {Laneloc: Lane marking based localization using highly accurate maps},
  Author                   = {Schreiber, Markus and Kn{\"o}ppel, Carsten and Franke, Uwe},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2013 IEEE},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {449--454}
}

@Article{soilan2016traffic,
  Title                    = {Traffic sign detection in MLS acquired point clouds for geometric and image-based semantic inventory},
  Author                   = {Soil{\'a}n, Mario and Riveiro, Bel{\'e}n and Mart{\'\i}nez-S{\'a}nchez, Joaqu{\'\i}n and Arias, Pedro},
  Journal                  = {ISPRS Journal of Photogrammetry and Remote Sensing},
  Year                     = {2016},
  Pages                    = {92--101},
  Volume                   = {114},

  Publisher                = {Elsevier}
}

@InProceedings{Stein2003,
  Title                    = {Vision-based ACC with a single camera: Bounds on range and range rate accuracy},
  Author                   = {Stein, Gideon P. and Mano, Ofer and Shashua, Amnon},
  Booktitle                = {IEEE Intelligent Vehicles Symposium, Proceedings},
  Year                     = {2003},
  Pages                    = {120--125},
  Publisher                = {IEEE},

  Abstract                 = {This paper describes a vision-based adaptive cruise control (ACC) system which uses a single camera as input. In particular, we discuss how to compute the range and range-rate from a single camera and discuss how the imaging geometry affects the range and range rate accuracy. We determine the bound on the accuracy given a particular configuration. These bounds in turn determine what steps must be made to achieve good performance. The system has been implemented on a test vehicle and driven on various highways over thousands of miles.},
  Doi                      = {10.1109/IVS.2003.1212895},
  ISBN                     = {0780378482},
  Url                      = {http://ieeexplore.ieee.org/document/1212895/}
}

@Article{sun2014application,
  Title                    = {Application of BW-ELM model on traffic sign recognition},
  Author                   = {Sun, Zhan-Li and Wang, Han and Lau, Wai-Shing and Seet, Gerald and Wang, Danwei},
  Journal                  = {Neurocomputing},
  Year                     = {2014},
  Pages                    = {153--159},
  Volume                   = {128},

  Publisher                = {Elsevier}
}

@Article{tan2016weakly,
  Title                    = {Weakly supervised metric learning for traffic sign recognition in a LIDAR-equipped vehicle},
  Author                   = {Tan, Min and Wang, Baoyuan and Wu, Zhaohui and Wang, Jingdong and Pan, Gang},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2016},
  Number                   = {5},
  Pages                    = {1415--1427},
  Volume                   = {17},

  Publisher                = {IEEE}
}

@Article{timofte2014multi,
  Title                    = {Multi-view traffic sign detection, recognition, and 3d localisation},
  Author                   = {Timofte, Radu and Zimmermann, Karel and Van Gool, Luc},
  Journal                  = {Machine vision and applications},
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {633--647},
  Volume                   = {25},

  Publisher                = {Springer}
}

@Article{villalon2017traffic,
  Title                    = {Traffic sign detection system for locating road intersections and roundabouts: the Chilean case},
  Author                   = {Villal{\'o}n-Sep{\'u}lveda, Gabriel and Torres-Torriti, Miguel and Flores-Calero, Marco},
  Journal                  = {Sensors},
  Year                     = {2017},
  Number                   = {6},
  Pages                    = {1207},
  Volume                   = {17},

  Publisher                = {Multidisciplinary Digital Publishing Institute}
}

@Article{wali2015automatic,
  Title                    = {An automatic traffic sign detection and recognition system based on colour segmentation, shape matching, and svm},
  Author                   = {Wali, Safat B and Hannan, Mahammad A and Hussain, Aini and Samad, Salina A},
  Journal                  = {Mathematical Problems in Engineering},
  Year                     = {2015},
  Volume                   = {2015},

  Publisher                = {Hindawi}
}

@InProceedings{weng2016road,
  Title                    = {Road traffic sign detection and classification from mobile LiDAR point clouds},
  Author                   = {Weng, Shengxia and Li, Jonathan and Chen, Yiping and Wang, Cheng},
  Booktitle                = {2nd ISPRS International Conference on Computer Vision in Remote Sensing (CVRS 2015)},
  Year                     = {2016},
  Organization             = {International Society for Optics and Photonics},
  Pages                    = {99010A},
  Volume                   = {9901}
}

@Article{yang2012automated,
  Title                    = {Automated extraction of road markings from mobile LiDAR point clouds},
  Author                   = {Yang, Bisheng and Fang, Lina and Li, Qingquan and Li, Jonathan},
  Journal                  = {Photogrammetric Engineering \& Remote Sensing},
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {331--338},
  Volume                   = {78},

  Publisher                = {American Society for Photogrammetry and Remote Sensing}
}

@Article{yang2016towards,
  Title                    = {Towards real-time traffic sign detection and classification},
  Author                   = {Yang, Yi and Luo, Hengliang and Xu, Huarong and Wu, Fuchao},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2016},
  Number                   = {7},
  Pages                    = {2022--2031},
  Volume                   = {17},

  Publisher                = {IEEE}
}

@Article{yebes2015,
  author        = {Yebes, J. Javier and Bergasa, Luis M. and Garc{\'{i}}a-Garrido, Miguel {\'{A}}ngel},
  title         = {Visual Object Recognition with 3D-Aware Features in KITTI Urban Scenes},
  journal       = {Sensors (Basel, Switzerland)},
  year          = {2015},
  volume        = {15},
  number        = {4},
  pages         = {9228--9250},
  month         = {apr},
  issn          = {14248220},
  abstract      = {Driver assistance systems and autonomous robotics rely on
		  the deployment of several sensors for environment
		  perception. Compared to LiDAR systems, the inexpensive
		  vision sensors can capture the 3D scene as perceived by a
		  driver in terms of appearance and depth cues. Indeed,
		  providing 3D image understanding capabilities to vehicles
		  is an essential target in order to infer scene semantics in
		  urban environments. One of the challenges that arises from
		  the navigation task in naturalistic urban scenarios is the
		  detection of road participants (e.g., cyclists, pedestrians
		  and vehicles). In this regard, this paper tackles the
		  detection and orientation estimation of cars, pedestrians
		  and cyclists, employing the challenging and naturalistic
		  KITTI images. This work proposes 3D-aware features computed
		  from stereo color images in order to capture the appearance
		  and depth peculiarities of the objects in road scenes. The
		  successful part-based object detector, known as DPM, is
		  extended to learn richer models from the 2.5D data (color
		  and disparity), while also carrying out a detailed analysis
		  of the training pipeline. A large set of experiments
		  evaluate the proposals, and the best performing approach is
		  ranked on the KITTI website. Indeed, this is the first work
		  that reports results with stereo data for the KITTI object
		  challenge, achieving increased detection ratios for the
		  classes car and cyclist compared to a baseline DPM.},
  doi           = {10.3390/s150409228},
  isbn          = {14248220},
  keywords      = {2.5D},
  mendeley-tags = {2.5D},
  pmid          = {102279770},
  url           = {http://www.ncbi.nlm.nih.gov/pubmed/25903553
		  http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4431302
		  http://www.mdpi.com/1424-8220/15/4/9228/},
}

@Article{zaklouta2014real,
  Title                    = {Real-time traffic sign recognition in three stages},
  Author                   = {Zaklouta, Fatin and Stanciulescu, Bogdan},
  Journal                  = {Robotics and autonomous systems},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {16--24},
  Volume                   = {62},

  Publisher                = {Elsevier}
}

@PhdThesis{zhang2016,
  author = {Zhang, Haocheng},
  title  = {{Rapid Inspection of Pavement Markings Using Mobile Laser Scanning Point Clouds}},
  school = {University of Waterloo},
  year   = {2016},
  month  = {mar},
  file   = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Zhang - 2016 - Rapid
		  Inspection of Pavement Markings Using Mobile Laser Scanning
		  Point Clouds.pdf:pdf},
  url    = {https://uwspace.uwaterloo.ca/handle/10012/10343},
}

@Article{zhang2017real,
  Title                    = {A Real-Time Chinese Traffic Sign Detection Algorithm Based on Modified YOLOv2},
  Author                   = {Zhang, Jianming and Huang, Manting and Jin, Xiaokang and Li, Xudong},
  Journal                  = {Algorithms},
  Year                     = {2017},
  Number                   = {4},
  Pages                    = {127},
  Volume                   = {10},

  Publisher                = {Multidisciplinary Digital Publishing Institute}
}

@InProceedings{zhao2012curb,
  Title                    = {Curb detection and tracking using 3D-LIDAR scanner},
  Author                   = {Zhao, Gangqiang and Yuan, Junsong},
  Booktitle                = {Image Processing (ICIP), 2012 19th IEEE International Conference on},
  Year                     = {2012},
  Organization             = {IEEE},
  Pages                    = {437--440}
}

@InProceedings{zhou2014lidar,
  Title                    = {LIDAR and vision-based real-time traffic sign detection and recognition algorithm for intelligent vehicle},
  Author                   = {Zhou, Lipu and Deng, Zhidong},
  Booktitle                = {Intelligent Transportation Systems (ITSC), 2014 IEEE 17th International Conference on},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {578--583}
}

@InProceedings{censi2014,
  author    = {Censi, Andrea and Scaramuzza, Davide},
  title     = {{Low-Latency Event-Based Visual Odometry}},
  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2014},
  pages     = {703--710},
  address   = {Hong Kong},
  abstract  = {— The agility of a robotic system is ultimately limited
		  by the speed of its processing pipeline. The use of a
		  Dynamic Vision Sensors (DVS), a sensor producing
		  asynchronous events as luminance changes are perceived by
		  its pixels, makes it pos-sible to have a sensing pipeline
		  of a theoretical latency of a few microseconds. However,
		  several challenges must be overcome: a DVS does not provide
		  the grayscale value but only changes in the luminance; and
		  because the output is composed by a sequence of events,
		  traditional frame-based visual odometry methods are not
		  applicable. This paper presents the first visual odometry
		  system based on a DVS plus a normal CMOS camera to provide
		  the absolute brightness values. The two sources of data are
		  automatically spatiotemporally calibrated from logs taken
		  during normal operation. We design a visual odometry method
		  that uses the DVS events to estimate the relative
		  displacement since the previous CMOS frame by processing
		  each event individually. Experiments show that the rotation
		  can be estimated with surprising accuracy, while the
		  translation can be estimated only very noisily, because it
		  produces few events due to very small apparent motion.},
  doi       = {10.1109/ICRA.2014.6906931},
  file      = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Censi, Scaramuzza - 2014 -
		  Low-Latency Event-Based Visual Odometry.pdf:pdf},
  url       = {http://rpg.ifi.uzh.ch/docs/ICRA14{\_}Censi.pdf},
}

@InProceedings{mueggler2014,
  author    = {Mueggler, Elias and Huber, Basil and Scaramuzza, Davide},
  title     = {{Event-based, 6-DOF pose tracking for high-speed maneuvers}},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  year      = {2014},
  pages     = {2761--2768},
  month     = {sep},
  publisher = {IEEE},
  abstract  = {In the last few years, we have witnessed impres- sive
		  demonstrations of aggressive flights and acrobatics using
		  quadrotors. However, those robots are actually blind. They
		  do not see by themselves, but through the “eyes” of an
		  external motion capture system. Flight maneuvers using
		  onboard sensors are still slow compared to those attainable
		  with motion capture systems. At the current state, the
		  agility of a robot is limited by the latency of its
		  perception pipeline. To obtain more agile robots, we need
		  to use faster sensors. In this paper, we present the first
		  onboard perception system for 6-DOF localization during
		  high-speed maneuvers using a Dynamic Vision Sensor (DVS).
		  Unlike a standard CMOS camera, a DVS does not wastefully
		  send full image frames at a fixed frame rate. Conversely,
		  similar to the human eye, it only transmits pixel-level
		  brightness changes at the time they occur with microsecond
		  resolution, thus, offering the possibility to create a
		  perception pipeline whose latency is negligible compared to
		  the dynamics of the robot. We exploit these characteristics
		  to estimate the pose of a quadrotor with respect to a known
		  pattern during high-speed maneuvers, such as flips, with
		  rotational speeds up to 1,200 ◦/s. Additionally, we
		  provide a versatile method to capture ground-truth data
		  using a DVS},
  doi       = {10.1109/IROS.2014.6942940},
  isbn      = {9781479969340},
  issn      = {21530866},
  url       = {http://ieeexplore.ieee.org/document/6942940/},
}

@Article{vidal2017,
  author        = {Vidal, Antoni Rosinol and Rebecq, Henri and Horstschaefer, Timo and Scaramuzza, Davide},
  title         = {{Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenarios}},
  journal       = {IEEE ROBOTICS AND AUTOMATION LETTERS},
  year          = {2017},
  volume        = {3},
  number        = {2},
  issn          = {2377-3766},
  abstract      = {Event cameras are bio-inspired vision sensors that output
		  pixel-level brightness changes instead of standard
		  intensity frames. These cameras do not suffer from motion
		  blur and have a very high dynamic range, which enables them
		  to provide reliable visual information during high speed
		  motions or in scenes characterized by high dynamic range.
		  However, event cameras output only little information when
		  the amount of motion is limited, such as in the case of
		  almost still motion. Conversely, standard cameras provide
		  instant and rich information about the environment most of
		  the time (in low-speed and good lighting scenarios), but
		  they fail severely in case of fast motions, or difficult
		  lighting such as high dynamic range or low light scenes. In
		  this paper, we present the first state estimation pipeline
		  that leverages the complementary advantages of these two
		  sensors by fusing in a tightly-coupled manner events,
		  standard frames, and inertial measurements. We show on the
		  publicly available Event Camera Dataset that our hybrid
		  pipeline leads to an accuracy improvement of 130{\%} over
		  event-only pipelines, and 85{\%} over standard-frames-only
		  visual-inertial systems, while still being computationally
		  tractable. Furthermore, we use our pipeline to demonstrate
		  - to the best of our knowledge - the first autonomous
		  quadrotor flight using an event camera for state
		  estimation, unlocking flight scenarios that were not
		  reachable with traditional visual-inertial odometry, such
		  as low-light environments and high-dynamic range scenes.},
  archiveprefix = {arXiv},
  arxivid       = {1709.06310},
  doi           = {10.1109/LRA.2018.2793357},
  eprint        = {1709.06310},
  file          = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Vidal et al. - 2017 -
		  Ultimate SLAM Combining Events, Images, and IMU for Robust
		  Visual SLAM in HDR and High Speed Scenarios.pdf:pdf},
  url           = {http://rpg.ifi.uzh.ch/docs/RAL18{\_}VidalRebecq.pdf
		  http://arxiv.org/abs/1709.06310{\%}0Ahttp://dx.doi.org/10.1109/LRA.2018.2793357},
}
@inproceedings{Maqueda2018,
abstract = {Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, filtering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle's steering angle. To make the best out of this sensor-algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset ({\~{}}1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras.},
address = {Salt Lake City},
archivePrefix = {arXiv},
arxivId = {1804.01310},
author = {Maqueda, Ana I and Loquercio, Antonio and Gallego, Guillermo and Garcia, Narciso and Scaramuzza, Davide},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2018.00568},
eprint = {1804.01310},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maqueda et al. - 2018 - Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars.pdf:pdf},
keywords = {Computer Vision,Machine Learning},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
title = {{Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars}},
url = {http://rpg.ifi.uzh.ch/docs/CVPR18{\_}Maqueda.pdf http://arxiv.org/abs/1804.01310},
year = {2018}
}

@Article{omalley2008,
  author    = {O'Malley, R. and Glavin, Martin and Jones, E.},
  title     = {{A review of automotive infrared pedestrian detection techniques}},
  journal   = {Signals and Systems Conference, 208.(ISSC 2008). IET Irish},
  year      = {2008},
  pages     = {168--173},
  abstract  = {Abstract In automotive design, the issue of safety remains
		  a growing priority. Recently the focus has extended beyond
		  the occupants of the vehicle and has turned towards other
		  Vulnerable Road Users (VRU). Simple night vision systems
		  have already become an ...},
  doi       = {10.1049/cp:20080657},
  isbn      = {978-0-86341-931-7},
  keywords  = {- pedestrian detection,active safety,agery,driver assist,infrared,obstacle detection,thermal im-},
  pmid      = {4780948},
  publisher = {IEE},
  url       = {http://digital-library.theiet.org/content/conferences/10.1049/cp{\_}20080657
		  papers2://publication/uuid/FFD556E9-8AF3-48F2-9C9B-3B57247056B7{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4780948{\%}5Cnpapers2://publication/uuid/FC2526CD-D332-4959-B833},
}
@article{Maddalena2005,
abstract = {After penetrating over a decade the consumer and industrial world, digital imaging is slowly but inevitably gaining marketshare in the automotive world. Cameras will become a key sensor in increasing car safety, driving assistance and driving comfort. The image sensors for automotive will be dominated by CMOS sensors as the requirements are different from the consumer market or the industrial or medical markets. Dynamic range, temperature range, cost, speed and many others are key parameters that need to be optimized. For this reason, automotive sensors differ from the other market's sensors and need to use different design and processing techniques in order to achieve the automotive specifications. This paper will show how Melexis has developed two CMOS imagers to target the automotive safety market and automotive CMOS imagers in general.},
address = {Berlin/Heidelberg},
author = {Maddalena, S. and Darmon, A. and Diels, R.},
doi = {10.1007/3-540-27463-4_29},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maddalena, Darmon, Diels - 2005 - Automotive CMOS Image Sensors.pdf:pdf},
journal = {Advanced Microsystems for Automotive {\ldots}},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
pages = {401--412},
publisher = {Springer-Verlag},
title = {{Automotive CMOS image sensors}},
url = {http://link.springer.com/10.1007/3-540-27463-4{\_}29 http://www.springerlink.com/index/M354109378G10242.pdf},
year = {2005}
}

@Article{strobel2013,
  author    = {Strobel, Markus and D{\"{o}}ttling, Dietmar},
  title     = {{High dynamic range CMOS (HDRC) imagers for safety systems}},
  journal   = {Advanced Optical Technologies},
  year      = {2013},
  volume    = {2},
  number    = {2},
  pages     = {147--157},
  month     = {jan},
  issn      = {2192-8584},
  doi       = {10.1515/aot-2012-0081},
  file      = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Strobel, D{\"{o}}ttling -
		  2013 - High dynamic range CMOS (HDRC) imagers for safety
		  systems.pdf:pdf},
  keywords  = {CMOS image sensor,OCIS codes: 110.4850,SafetyEYE,global shutter,high dynamic range CMOS (HDRC),safe camera system},
  publisher = {THOSS Media {\&} De Gruyter},
  url       = {https://www.degruyter.com/view/j/aot.2013.2.issue-2/aot-2012-0081/aot-2012-0081.xml},
}

@Article{chun2008,
  author   = {Chun, Jung Bum and Jung, Hunjoon and Kyung, Chong Min},
  title    = {{Suppressing rolling-shutter distortion of CMOS image sensors by motion vector detection}},
  journal  = {IEEE Transactions on Consumer Electronics},
  year     = {2008},
  volume   = {54},
  number   = {4},
  pages    = {1479--1487},
  month    = {nov},
  issn     = {00983063},
  abstract = {This paper focuses on the rolling shutter distortion of
		  CMOS image sensor coming from its unique readout mechanism
		  as the main cause for image degradation when there are
		  fast-moving objects. This paper proposes a post image
		  processing scheme based on motion vector detection to
		  suppress the rolling shutter distortion. Motion vector
		  detection is performed based on an optical flow method at a
		  reasonable computational complexity. A practical
		  implementation scheme is also described.},
  doi      = {10.1109/TCE.2008.4711190},
  keywords = {CMOS image sensor,Post-processing technique,Rolling-shutter distortion},
  url      = {http://ieeexplore.ieee.org/document/4711190/},
}

@Article{chia-kailiang2008,
  author   = {Liang, Chia Kai and Chang, Li Wen and Chen, Homer H.},
  title    = {{Analysis and compensation of rolling shutter effect}},
  journal  = {IEEE Transactions on Image Processing},
  year     = {2008},
  volume   = {17},
  number   = {8},
  pages    = {1323--1330},
  month    = {aug},
  issn     = {10577149},
  abstract = {Due to the sequential-readout structure of complementary
		  metal-oxide semiconductor image sensor array, each scanline
		  of the acquired image is exposed at a different time,
		  resulting in the so-called electronic rolling shutter that
		  induces geometric image distortion when the object or the
		  video camera moves during image capture. In this paper, we
		  propose an image processing technique using a planar motion
		  model to address the problem. Unlike previous methods that
		  involve complex 3-D feature correspondences, a simple
		  approach to the analysis of inter- and intraframe
		  distortions is presented. The high-resolution velocity
		  estimates used for restoring the image are obtained by
		  global motion estimation, BEzier curve fitting, and local
		  motion estimation without resort to correspondence
		  identification. Experimental results demonstrate the
		  effectiveness of the algorithm.},
  doi      = {10.1109/TIP.2008.925384},
  isbn     = {1057-7149 (Print)},
  keywords = {Complementary metal-oxide semiconductor (CMOS) sen,Motion analysis,Rolling shutter},
  pmid     = {18632342},
  url      = {http://ieeexplore.ieee.org/document/4549748/},
}

@Article{pueo2016,
  author   = {Pueo, Basilio},
  title    = {{High speed cameras for motion analysis in sports science}},
  journal  = {Journal of Human Sport and Exercise},
  year     = {2016},
  volume   = {11},
  number   = {1},
  pages    = {53--73},
  month    = {dec},
  issn     = {19885202},
  abstract = {Video analysis can be a qualitative or quantitative
		  process to analyze motion occurring in a single plane using
		  one camera (two-dimensional or 2D) or in more than one
		  plane using two or more cameras simultaneously
		  (three-dimensional or 3D). Quantitative 2D video analysis
		  is performed through a digitizing process that converts
		  body segments or sport implements into digital horizontal
		  and vertical coordinates in the computer. In order for
		  these measurements to be accurate, image capture by means
		  of video cameras must be sharp and motion blur-free,
		  especially in high speed motions. In this paper, a detailed
		  introduction to factors affecting image quality will be
		  presented. Furthermore, selection of the most appropriate
		  camera setting to undertake high speed motion analysis with
		  the best quality possible, both spatially (focus and
		  resolution) and temporally (frame rate, motion blur,
		  shutter options and lighting), will be discussed. Rather
		  than considering commercial criteria, the article will
		  focus on key features to choose the most convenient model
		  both from technical and economical perspectives. Then, a
		  revision of available cameras on the market as of 2015 will
		  be carried out, with selected models grouped into three
		  categories: high-, mid- and low-range, according to their
		  maximum performance in relation to high speed features.
		  Finally, a suggested recording procedure to minimize
		  perspective errors and produce high quality video
		  recordings will be presented. This guideline starts with
		  indications for camera selection prior to purchase or for
		  testing if a given camera would fulfil the minimum
		  features. A good video recording dramatically improves the
		  analysis quality and enables digitizing software to produce
		  accurate measurements},
  doi      = {10.14198/jhse.2016.111.05},
  keywords = {Biomechanics,Frame rate,Motion blur,Performance,Shutter speed},
  url      = {http://hdl.handle.net/10045/61909},
}

@Article{aqel2016,
  author    = {Aqel, Mohammad O A and Marhaban, Mohammad H and Saripan, M Iqbal and Ismail, Napsiah Bt},
  title     = {{Review of visual odometry: types, approaches, challenges, and applications.}},
  journal   = {SpringerPlus},
  year      = {2016},
  volume    = {5},
  number    = {1},
  pages     = {1897},
  issn      = {2193-1801},
  abstract  = {Accurate localization of a vehicle is a fundamental
		  challenge and one of the most important tasks of mobile
		  robots. For autonomous navigation, motion tracking, and
		  obstacle detection and avoidance, a robot must maintain
		  knowledge of its position over time. Vision-based odometry
		  is a robust technique utilized for this purpose. It allows
		  a vehicle to localize itself robustly by using only a
		  stream of images captured by a camera attached to the
		  vehicle. This paper presents a review of state-of-the-art
		  visual odometry (VO) and its types, approaches,
		  applications, and challenges. VO is compared with the most
		  common localization sensors and techniques, such as
		  inertial navigation systems, global positioning systems,
		  and laser sensors. Several areas for future research are
		  also highlighted.},
  doi       = {10.1186/s40064-016-3573-7},
  file      = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Aqel et al. - 2016 -
		  Review of visual odometry types, approaches, challenges,
		  and applications.pdf:pdf},
  keywords  = {Global positioning system,Image stream,Inertial navigation system,Localization sensors,Visual odometry},
  pmid      = {27843754},
  publisher = {Springer},
  url       = {http://www.ncbi.nlm.nih.gov/pubmed/27843754
		  http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5084145},
}

@Article{depontemuller2017,
  author    = {{de Ponte M{\"{u}}ller}, Fabian and Fabian},
  title     = {{Survey on Ranging Sensors and Cooperative Techniques for Relative Positioning of Vehicles}},
  journal   = {Sensors},
  year      = {2017},
  volume    = {17},
  number    = {2},
  pages     = {271},
  month     = {jan},
  issn      = {1424-8220},
  abstract  = {Future driver assistance systems will rely on accurate,
		  reliable and continuous knowledge on the position of other
		  road participants, including pedestrians, bicycles and
		  other vehicles. The usual approach to tackle this
		  requirement is to use on-board ranging sensors inside the
		  vehicle. Radar, laser scanners or vision-based systems are
		  able to detect objects in their line-of-sight. In contrast
		  to these non-cooperative ranging sensors, cooperative
		  approaches follow a strategy in which other road
		  participants actively support the estimation of the
		  relative position. The limitations of on-board ranging
		  sensors regarding their detection range and angle of view
		  and the facility of blockage can be approached by using a
		  cooperative approach based on vehicle-to-vehicle
		  communication. The fusion of both, cooperative and
		  non-cooperative strategies, seems to offer the largest
		  benefits regarding accuracy, availability and robustness.
		  This survey offers the reader a comprehensive review on
		  different techniques for vehicle relative positioning. The
		  reader will learn the important performance indicators when
		  it comes to relative positioning of vehicles, the different
		  technologies that are both commercially available and
		  currently under research, their expected performance and
		  their intrinsic limitations. Moreover, the latest research
		  in the area of vision-based systems for vehicle detection,
		  as well as the latest work on GNSS-based vehicle
		  localization and vehicular communication for relative
		  positioning of vehicles, are reviewed. The survey also
		  includes the research work on the fusion of cooperative and
		  non-cooperative approaches to increase the reliability and
		  the availability.},
  doi       = {10.3390/s17020271},
  file      = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/de Ponte M{\"{u}}ller,
		  Fabian - 2017 - Survey on Ranging Sensors and Cooperative
		  Techniques for Relative Positioning of Vehicles.pdf:pdf},
  keywords  = {GNSS,cooperative,laser scanner,localization,relative positioning,to,vehicle,vehicle sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  url       = {http://www.mdpi.com/1424-8220/17/2/271},
}

@Article{velez2017,
  author  = {Velez, Gorka and Otaegui, Oihana},
  title   = {{Embedding vision-based advanced driver assistance systems: a survey}},
  journal = {IET Intelligent Transport Systems},
  year    = {2017},
  volume  = {11},
  number  = {3},
  pages   = {103--112},
  month   = {apr},
  issn    = {1751-956X},
  doi     = {10.1049/iet-its.2016.0026},
  url     = {http://digital-library.theiet.org/content/journals/10.1049/iet-its.2016.0026},
}

@Article{kohler2013,
  author   = {K{\"{o}}hler, Mike and Hasch, J{\"{u}}rgen and Bl{\"{o}}cher, Hans Ludwig and Schmidt, Lorenz Peter},
  title    = {{Feasibility of automotive radar at frequencies beyond 100 GHz}},
  journal  = {International Journal of Microwave and Wireless Technologies},
  year     = {2013},
  volume   = {5},
  number   = {1},
  pages    = {49--54},
  month    = {feb},
  issn     = {17590787},
  doi      = {10.1017/S175907871200075X},
  keywords = {Automotive radar},
  url      = {http://www.journals.cambridge.org/abstract{\_}S175907871200075X},
}

@Article{kishida2015,
  author   = {Kishida, Masayuki and Ohguchi, Katsuyuki and Shono, Masayoshi},
  title    = {{79 GHz-Band High-Resolution Millimeter- Wave Radar}},
  journal  = {FUJITSU Sci. Tech. J},
  year     = {2015},
  volume   = {51},
  number   = {4},
  pages    = {55--59},
  abstract = {High-resolution millimeter-wave radar that operates in the
		  79 GHz band is expected to achieve a significant increase
		  in the distance resolution of radar systems because of the
		  availability of a wide frequency bandwidth of 4 GHz as
		  compared with 0.5 GHz of the existing 77 GHz-band
		  mil-limeter-wave radar. For this reason, it has the
		  potential to distinguish between a vehicle and a human,
		  which was conventionally difficult, and recognize their
		  movements. Therefore it raises expectations for use as a
		  surrounding monitoring radar in driving safety support and
		  auto-matic driving. As one of FUJITSU TEN's efforts
		  regarding sensing technologies for driving safety support
		  and automatic driving, it has been developing 79 GHz-band
		  high-resolution millimeter-wave radar. This paper presents
		  specifications of radar for application to systems that
		  assist in safe driving and automatic driving and the
		  results of testing a prototype for a wider bandwidth that
		  is required to accomplish the technology's purpose. This
		  radar increases the ability to de-tect a pedestrian in the
		  surroundings of a vehicle, which was difficult to do with
		  the existing 77 GHz-band radar. Furthermore, this paper
		  also describes how the newly developed radar of-fers the
		  possibility of improving the performance of a sensor for
		  automatic driving and systems to assist in safe driving.},
  file     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Kishida, Ohguchi, Shono -
		  2015 - 79 GHz-Band High-Resolution Millimeter-Wave
		  Radar.pdf:pdf},
  url      = {https://www.fujitsu.com/global/documents/about/resources/publications/fstj/archives/vol51-4/paper09.pdf},
}

@misc{EULawandPublications2004,
booktitle = {EU Law and Publications},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
pages = {2},
title = {{2004/545/EC: Commission Decision of 8 July 2004 on the harmonisation of radio spectrum in the 79 GHz range for the use of automotive short-range radar equipment in the Community}},
url = {https://publications.europa.eu/en/publication-detail/-/publication/9d425670-b54b-4c65-8461-824dbf71facc/language-en},
urldate = {2018-07-19},
year = {2004}
}

@Article{gatziolis2008,
  author   = {Gatziolis, Demetrios and Andersen, Hans Erik},
  title    = {{A Guide to LIDAR Data Acquisition and Processing for the Forests of the Pacific Northwest}},
  journal  = {General Technical Report PNW-GTR-768},
  year     = {2008},
  volume   = {768},
  number   = {July},
  pages    = {1--40},
  abstract = {Light detection and ranging (LIDAR) is an emerging
		  remote-sensing technology with promising potential to
		  assist in mapping, monitoring, and assessment of forest
		  resources. Continuous technological advancement and
		  substantial reductions in data acquisition cost have
		  enabled acquisition of laser data over entire states and
		  regions. These developments have triggered an explosion of
		  interest in LIDAR technology. Despite a growing body of
		  peer-reviewed literature documenting the merits of LIDAR
		  for forest assessment, management, and planning, there
		  seems to be little information describing in detail the
		  acquisition, quality assessment, and processing of laser
		  data for forestry applications. This report addresses this
		  information deficit by providing a foundational knowledge
		  base containing answers to the most frequently asked
		  questions. Keywords: LIDAR, Pacific Northwest, FIA, forest
		  inventory, laser, absolute and relative accuracy,
		  precision, registration, stand penetration, DEM, canopy
		  surface, resolution, data storage, data quality assessment,
		  topography, scanning.},
  doi      = {Gen. Tech. Rep. PNW-GTR-768},
  file     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Gatziolis, Andersen - 2008
		  - A guide to LIDAR data acquisition and processing for the
		  forests of the Pacific Northwest.pdf:pdf},
  keywords = {DEM,FIA,LIDAR,Pacific Northwest,absolute and relative accuracy,canopy surface,data quality assessment,data storage,forest inventory,laser,precision,registration,resolution,scanning.,stand penetration,topography},
  url      = {https://www.fs.usda.gov/treesearch/pubs/30652
		  http://www.arlis.org/docs/vol1/A/276932054.pdf},
}

@Article{glennie2010,
  author    = {Glennie, Craig and Lichti, Derek D.},
  title     = {{Static calibration and analysis of the velodyne HDL-64E S2 for high accuracy mobile scanning}},
  journal   = {Remote Sensing},
  year      = {2010},
  volume    = {2},
  number    = {6},
  pages     = {1610--1624},
  month     = {jun},
  issn      = {20724292},
  abstract  = {The static calibration and analysis of the Velodyne
		  HDL-64E S2 scanning LiDAR system is presented and analyzed.
		  The mathematical model for measurements for the HDL-64E S2
		  scanner is derived and discussed. A planar feature based
		  least squares adjustment approach is presented and utilized
		  in a minimally constrained network in order to derive an
		  optimal solution for the laser's internal calibration
		  parameters. Finally, the results of the adjustment along
		  with a detailed examination of the adjustment residuals are
		  given. A three-fold improvement in the planar misclosure
		  residual RMSE over the standard factory calibration model
		  was achieved by the proposed calibration. Results also
		  suggest that there may still be some unmodelled distortions
		  in the range measurements from the scanner. However,
		  despite this, the overall precision of the adjusted laser
		  scanner data appears to make it a viable choice for high
		  accuracy mobile scanning applications.},
  doi       = {10.3390/rs2061610},
  file      = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Glennie, Lichti - 2010 -
		  Static Calibration and Analysis of the Velodyne HDL-64E S2
		  for High Accuracy Mobile Scanning.pdf:pdf},
  isbn      = {2072-4292},
  keywords  = {Accuracy,Error analysis,Lidar,System calibration},
  publisher = {Molecular Diversity Preservation International},
  url       = {http://www.mdpi.com/2072-4292/2/6/1610},
}

@Article{wang2008,
  author   = {Wang, M. and Liu, W.-Q. and Lu, Y.-H. and Zhao, X.-S. and Song, B.-C. and Zhang, Y.-J. and Wang, Y.-P. and Lian, C.-H. and Chen, Jun and Cheng, Yin and Liu, J.-G. and Wei, Q.-N.},
  title    = {{Study on the measurement of the atmospheric extinction of fog and rain by forward-scattering near infrared spectroscopy}},
  journal  = {Guang Pu Xue Yu Guang Pu Fen Xi/Spectroscopy and Spectral Analysis},
  year     = {2008},
  volume   = {28},
  number   = {8},
  pages    = {1776--80},
  month    = {aug},
  issn     = {10000593},
  abstract = {In the visible and near IR, absorption is negligible so
		  that the atmospheric extinction can be derived by
		  atmospheric scattering which is mainly contributed by fog
		  droplet, rain droplet, another types of droplet and small
		  articles. The forward-scattering visibility meter (FVM)
		  works by illuminating with near IR light a small sample
		  volume of about 100 mL of air and measuring the intensity
		  scattered in the angular range of 30° to 36° degrees. The
		  scattered intensity is proportional to the extinction
		  coefficient regardless of the article size distribution and
		  after wavelength calibration. The ratio of scattered signal
		  to extinction coefficient of fog and haze can be achieved
		  by comparative test of FVM outputs and manual observations.
		  Nevertheless, as a result of the application of the
		  measurement during rain with the ratio of fog and haze, an
		  unacceptable error is raised. To obtain an accuracy
		  extinction measurement during rain, an appropriated ratio
		  of scattered signal to extinction coefficient of rain would
		  be found. The calculation for different size distributions
		  of fog and rain with Mie theory has been made in this
		  paper. And a comparison of extinction measurements made
		  with two FVMs and manual observations during fog and rain
		  has been made. The result shows that during rain the FVM
		  extinction coefficient is from 20{\%} to 60{\%} greater
		  than that of manual observations. This result can be used
		  to define correction factors so that the FVM using
		  forward-scattering near IR spectroscopy not only can be
		  used to estimate extinction during fog and haze as well as
		  during rain.},
  keywords = {Atmospheric extinction,Extinction coefficient,Forward-scattering,Near IR,Phase function,Size distribution,Visibility},
  pmid     = {18975801},
  url      = {http://www.ncbi.nlm.nih.gov/pubmed/18975801},
}

@Article{phillips2017,
  author        = {Phillips, Tyson Govan and Guenther, Nicky and McAree, Peter Ross},
  title         = {{When the Dust Settles: The Four Behaviors of LiDAR in the Presence of Fine Airborne Particulates}},
  journal       = {Journal of Field Robotics},
  year          = {2017},
  volume        = {34},
  number        = {5},
  pages         = {985--1009},
  month         = {aug},
  issn          = {15564967},
  abstract      = {It is anticipatedthat theMars ScienceLaboratory rover,
		  namedCuriosity,will traverse 10–20 kmon the surface of
		  Mars during its primary mission. In preparation for this
		  traverse, Earth-based tests were performed using Mars
		  weight vehicles. These vehicles were driven over Mars
		  analog bedrock, cohesive soil, and cohesionless sand at
		  various slopes. Vehicle slip was characterized on each of
		  these terrains versus slope for direct upslope driving.
		  Results show that slopes up to 22 degrees are traversable
		  on smooth bedrock and that slopes up to 28 degrees are
		  traversable on some cohesive soils. In cohesionless sand,
		  results show a sharp transition between moderate slip on 10
		  degree slopes and vehicle embedding at 17 degrees. For
		  cohesionless sand, data are also presented showing the
		  relationship between vehicle slip and wheel sinkage. Side
		  by side testing of the Mars Exploration Rover test vehicle
		  and the Mars Science Laboratory test vehicle show how
		  increased wheel diameter leads to better slope climbing
		  ability in sand for vehicles with nearly identical ground
		  pressure. Lastly, preliminary data from Curiosity's initial
		  driving on Mars are presented and compared to the
		  Earth-based testing, showing good agreement for the driving
		  done during the first 250 Martian days.},
  archiveprefix = {arXiv},
  arxivid       = {10.1.1.91.5767},
  doi           = {10.1002/rob.21701},
  eprint        = {10.1.1.91.5767},
  file          = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Phillips, Guenther, McAree
		  - 2017 - When the Dust Settles The Four Behaviors of LiDAR
		  in the Presence of Fine Airborne Particulates.pdf:pdf},
  isbn          = {9783902661623},
  pmid          = {22164016},
  publisher     = {Wiley-Blackwell},
  url           = {http://doi.wiley.com/10.1002/rob.21701},
}

@article{McManamon1996,
abstract = {{Optical phased arrays represent an enabling new technology that makes possible simple affordable, lightweight, optical sensors offering very precise stabilization, random-access pointing programmable multiple simultaneous beams, a dynamic focus/defocus capability, and moderate to excellent optical power handling capability. These new arrays steer or otherwise operate on an already formed beam. A phase profile is imposed on an optical beam as it is either transmitted through or reflected from the phase shifter array. The imposed phase profile steers, focuses, fans out, or corrects phase aberrations on the beam. The array of optical phase shifters is realized through lithographic patterning of an electrical addressing network on the superstrate of a liquid crystal waveplate. Refractive index changes sufficiently large to realize full-wave differential phase shifts can be effected using low ({\textless}10 V) voltages applied to the liquid crystal phase plate electrodes. High efficiency large-angle steering with phased arrays requires phase shifter spacing on the order of a wavelength or less; consequently addressing issues make 1-D optical arrays much more practical than 2-D arrays. Orthogonal oriented 1-D phased arrays are used to deflect a beam in both dimensions. Optical phased arrays with apertures on the order of 4 cm by 4 cm have been fabricated for steering green, red, 1.06 $\mu$m, and 10.6 $\mu$m radiation. System concepts that include a passive acquisition sensor as well as a laser radar are presented{\}}, keywords={\{}aberrations;arrays;liquid crystal devices;lithography;optical radar;optical sensors;phase shifters;phased array radar;refractive index;1.06 micrometre;10 V;10.6 micrometre;1D optical arrays;dynamic focus/defocus capability;electrical addressing network;full-wave differential phase shifts;large-angle steering;laser radar;liquid crystal waveplate;lithographic patterning;optical phased array technology;optical power handling capability;optical sensors;passive acquisition sensor;phase aberrations;phase profile;phase shifter array;programmable multiple simultaneous beams;random-access pointing;refractive index changes;Laser radar;Liquid crystals;Optical arrays;Optical beams;Optical refraction;Optical sensors;Optical variables control;Phase shifters;Phased arrays;Sensor arrays}},
author = {Mcmanamon, Paul F. and Dorschner, Terry A. and Corkum, David L. and Friedman, Larry J. and Hobbs, Douglas S. and Holz, Michael and Liberman, Sergey and Nguyen, Huy Q. and Resler, Daniel P. and Sharp, Richard C. and Watson, Edward A. and Dorschner, T. A. and Friedman, L. J. and Hobbs, D. S. and Holz, M. and Resler, D. P. and Sharp, R. C.},
doi = {10.1109/5.482231},
issn = {00189219},
journal = {Proceedings of the IEEE},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {feb},
number = {2},
pages = {268--298},
title = {{Optical phased array technology}},
url = {http://ieeexplore.ieee.org/document/482231/},
volume = {84},
year = {1996}
}

@TechReport{leddartech2016,
  author      = {Olivier, Pierre},
  title       = {{LEDDAR optical Time-of-Flight sensing technology: A new approach to detection and ranging}},
  institution = {LeddarTech},
  year        = {2016},
  address     = {Quebec},
  abstract    = {Remote sensing consists of acquiring information about a
		  specific object in the vicinity of a sensor without making
		  physical contact with the object. Countless applications
		  such as automotive driver assistance systems and autonomous
		  driving, drone and robot collision avoidance and
		  navigation, traffic management and level sensing exist
		  thanks to this technique. Multiple technology options are
		  available for remote sensing; we can divide them into three
		  broad applications: Presence or proximity detection, where
		  the absence or presence of an object in a general area is
		  the only information that is required (e.g., for security
		  applications). This is the simplest form of remote sensing;
		  Speed measurement, where the exact position of an object
		  does not need to be known but where its accurate speed is
		  required (e.g., for law enforcement applications); and
		  Detection and ranging, where the position of an object
		  relative to the sensor needs to be precisely and accurately
		  determined. This paper will concentrate on technologies
		  capable of providing a detection and ranging functionality,
		  as it is the most complex of the three applications. From
		  the position information, presence and speed can be
		  retrieved so technologies capable of detection and ranging
		  can be universally applied to all remote sensing
		  applications.},
  file        = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Olivier - 2016 - Leddar
		  Optical Time-of-Flight sensing Technology A new approach to
		  detection and ranging.pdf:pdf},
  pages       = {13},
  url         = {https://leddartech.com/app/uploads/dlm{\_}uploads/2016/02/Leddar-Optical-Time-of-Flight-Sensing-Technology-1.pdf
		  https://d1wx5us9wukuh0.cloudfront.net/app/uploads/dlm{\_}uploads/2016/02/Leddar-Optical-Time-of-Flight-Sensing-Technology-1.pdf},
}

@Misc{eldada2017,
  author    = {Eldada, Louay},
  title     = {{LiDAR and the Autonomous Vehicle Revolution for Truck and Ride Sharing Fleets}},
  year      = {2017},
  address   = {San Francisco},
  booktitle = {Automated Vehicles Symposium},
  file      = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Eldada - 2017 - LiDAR and
		  the Autonomous Vehicle Revolution for Truck and Ride
		  Sharing Fleets.pdf:pdf},
  url       = {https://higherlogicdownload.s3.amazonaws.com/AUVSI/14c12c18-fde1-4c1d-8548-035ad166c766/UploadedImages/2017/PDFs/Proceedings/ESS/Wednesday
		  1330-1400{\_}Louay Eldada.pdf},
}

@Article{blanc2004obstacle,
  author    = {Blanc, Christophe and Aufrere, Romuald and Malaterre, Laurent and Gallice, Jean and Alizon, Joseph},
  title     = {Obstacle detection and tracking by millimeter wave radar},
  journal   = {IFAC Proceedings Volumes},
  year      = {2004},
  volume    = {37},
  number    = {8},
  pages     = {322--327},
  publisher = {Elsevier},
}

@InProceedings{garcia2012data,
  author       = {Garcia, Fernando and Cerri, Pietro and Broggi, Alberto and de la Escalera, Arturo and Armingol, Jos{\'e} Mar{\'\i}a},
  title        = {Data fusion for overtaking vehicle detection based on radar and optical flow},
  booktitle    = {Intelligent Vehicles Symposium (IV), 2012 IEEE},
  year         = {2012},
  pages        = {494--499},
  organization = {IEEE},
}

@InProceedings{gohring2011radar,
  author       = {G{\"o}hring, Daniel and Wang, Miao and Schn{\"u}rmacher, Michael and Ganjineh, Tinosch},
  title        = {Radar/lidar sensor fusion for car-following on highways},
  booktitle    = {Automation, Robotics and Applications (ICARA), 2011 5th International Conference on},
  year         = {2011},
  pages        = {407--412},
  organization = {IEEE},
}

@InProceedings{song2007lateral,
  author       = {Song, Kai-Tai and Chen, Hung-Yi},
  title        = {Lateral driving assistance using optical flow and scene analysis},
  booktitle    = {Intelligent Vehicles Symposium, 2007 IEEE},
  year         = {2007},
  pages        = {624--629},
  organization = {IEEE},
}

@InProceedings{blanc2007larasidecam,
  author       = {Blanc, Nicolas and Steux, Bruno and Hinz, Thomas},
  title        = {LaRASideCam: A fast and robust vision-based blindspot detection system},
  booktitle    = {Intelligent Vehicles Symposium, 2007 IEEE},
  year         = {2007},
  pages        = {480--485},
  organization = {IEEE},
}

@InProceedings{chang2008real,
  author       = {Chang, Wen-Chung and Cho, Chih-Wei},
  title        = {Real-time side vehicle tracking using parts-based boosting},
  booktitle    = {Systems, Man and Cybernetics, 2008. SMC 2008. IEEE International Conference on},
  year         = {2008},
  pages        = {3370--3375},
  organization = {IEEE},
}

@Article{gandhi2006vehicle,
  author    = {Gandhi, Tarak and Trivedi, Mohan M},
  title     = {Vehicle surround capture: Survey of techniques and a novel omni-video-based approach for dynamic panoramic surround maps},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  year      = {2006},
  volume    = {7},
  number    = {3},
  pages     = {293--308},
  publisher = {IEEE},
}

@InProceedings{liu2007rear,
  author       = {Liu, Wei and Wen, XueZhi and Duan, Bobo and Yuan, Huai and Wang, Nan},
  title        = {Rear vehicle detection and tracking for lane change assist},
  booktitle    = {Intelligent Vehicles Symposium, 2007 IEEE},
  year         = {2007},
  pages        = {252--257},
  organization = {IEEE},
}

@Article{sivaraman2013looking,
  author    = {Sivaraman, Sayanan and Trivedi, Mohan Manubhai},
  title     = {Looking at vehicles on the road: A survey of vision-based vehicle detection, tracking, and behavior analysis},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  year      = {2013},
  volume    = {14},
  number    = {4},
  pages     = {1773--1795},
  publisher = {IEEE},
}

@Article{mukhtar2015vehicle,
  author  = {Mukhtar, Amir and Xia, Likun and Tang, Tong Boon},
  title   = {Vehicle Detection Techniques for Collision Avoidance Systems: A Review.},
  journal = {IEEE Trans. Intelligent Transportation Systems},
  year    = {2015},
  volume  = {16},
  number  = {5},
  pages   = {2318--2338},
}

@InProceedings{bernini2014real,
  author       = {Bernini, Nicola and Bertozzi, Massimo and Castangia, Luca and Patander, Marco and Sabbatelli, Mario},
  title        = {Real-time obstacle detection using stereo vision for autonomous ground vehicles: A survey},
  booktitle    = {Intelligent Transportation Systems (ITSC), 2014 IEEE 17th International Conference on},
  year         = {2014},
  pages        = {873--878},
  organization = {IEEE},
}

@Article{krotosky2007color,
  author    = {Krotosky, Stephen J and Trivedi, Mohan Manubhai},
  title     = {On color-, infrared-, and multimodal-stereo approaches to pedestrian detection},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  year      = {2007},
  volume    = {8},
  number    = {4},
  pages     = {619--629},
  publisher = {IEEE},
}

@Article{olmeda2013pedestrian,
  author    = {Olmeda, Daniel and Premebida, Cristiano and Nunes, Urbano and Armingol, Jose Maria and de la Escalera, Arturo},
  title     = {Pedestrian detection in far infrared images},
  journal   = {Integrated Computer-Aided Engineering},
  year      = {2013},
  volume    = {20},
  number    = {4},
  pages     = {347--360},
  publisher = {IOS Press},
}

@Article{li2016vehicle,
  author  = {Li, Bo and Zhang, Tianlei and Xia, Tian},
  title   = {Vehicle detection from 3d lidar using fully convolutional network},
  journal = {arXiv preprint arXiv:1608.07916},
  year    = {2016},
}

@Article{mizumachi2014robust,
  author    = {Mizumachi, Mitsunori and Kaminuma, Atsunobu and Ono, Nobutaka and Ando, Shigeru},
  title     = {Robust sensing of approaching vehicles relying on acoustic cues},
  journal   = {Sensors},
  year      = {2014},
  volume    = {14},
  number    = {6},
  pages     = {9546--9561},
  publisher = {Multidisciplinary Digital Publishing Institute},
}

@Article{premebida2007lidar,
  author  = {Premebida, Cristiano and Monteiro, Gon{\c{c}}alo and Nunes, Urbano and Peixoto, Paulo},
  title   = {A lidar and vision-based approach for pedestrian and vehicle detection and tracking},
  journal = {rn},
  year    = {2007},
  volume  = {10},
  pages   = {2},
}

@Article{alessandretti2007vehicle,
  author    = {Alessandretti, Giancarlo and Broggi, Alberto and Cerri, Pietro},
  title     = {Vehicle and guard rail detection using radar and vision data fusion},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  year      = {2007},
  volume    = {8},
  number    = {1},
  pages     = {95--105},
  publisher = {IEEE},
}

###Article{	  scaramuzza2008,
  author	= {Scaramuzza, D. and Siegwart, R.},
  doi		= {10.1109/TRO.2008.2004490},
  issn		= {1552-3098},
  journal	= {IEEE Transactions on Robotics},
  month		= {oct},
  number	= {5},
  pages		= {1015--1026},
  title		= {{Appearance-Guided Monocular Omnidirectional Visual
		  Odometry for Outdoor Ground Vehicles}},
  url		= {http://ieeexplore.ieee.org/document/4625958/},
  volume	= {24},
  year		= {2008}
}

###Article{	  scaramuzza2008,
  title		= {Appearance-Guided Monocular Omnidirectional Visual
		  Odometry for Outdoor Ground Vehicles},
  author	= {Scaramuzza, D. and Siegwart, R.},
  journal	= {IEEE Transactions on Robotics},
  year		= {2008},
  month		= {oct},
  number	= {5},
  pages		= {1015--1026},
  volume	= {24},
  doi		= {10.1109/TRO.2008.2004490},
  issn		= {1552-3098},
  url		= {http://ieeexplore.ieee.org/document/4625958/}
}

@Misc{scheer2018,
  author    = {Scheer, Steven},
  title     = {{Exclusive: Intel's Mobileye gets self-driving tech deal for 8 million cars}},
  year      = {2018},
  booktitle = {Reuters},
  url       = {https://www.reuters.com/article/us-israel-tech-intel-mobileye-exclusive/exclusive-intels-mobileye-gets-self-driving-tech-deal-for-8-million-cars-idUSKCN1II0K7},
  urldate   = {2018-10-06},
}

###Article{	  phillips2017,
  abstract	= {It is anticipatedthat theMars ScienceLaboratory rover,
		  namedCuriosity,will traverse 10–20 kmon the surface of
		  Mars during its primary mission. In preparation for this
		  traverse, Earth-based tests were performed using Mars
		  weight vehicles. These vehicles were driven over Mars
		  analog bedrock, cohesive soil, and cohesionless sand at
		  various slopes. Vehicle slip was characterized on each of
		  these terrains versus slope for direct upslope driving.
		  Results show that slopes up to 22 degrees are traversable
		  on smooth bedrock and that slopes up to 28 degrees are
		  traversable on some cohesive soils. In cohesionless sand,
		  results show a sharp transition between moderate slip on 10
		  degree slopes and vehicle embedding at 17 degrees. For
		  cohesionless sand, data are also presented showing the
		  relationship between vehicle slip and wheel sinkage. Side
		  by side testing of the Mars Exploration Rover test vehicle
		  and the Mars Science Laboratory test vehicle show how
		  increased wheel diameter leads to better slope climbing
		  ability in sand for vehicles with nearly identical ground
		  pressure. Lastly, preliminary data from Curiosity's initial
		  driving on Mars are presented and compared to the
		  Earth-based testing, showing good agreement for the driving
		  done during the first 250 Martian days.},
  archiveprefix	= {arXiv},
  arxivid	= {10.1.1.91.5767},
  author	= {Phillips, Tyson Govan and Guenther, Nicky and McAree,
		  Peter Ross},
  doi		= {10.1002/rob.21701},
  eprint	= {10.1.1.91.5767},
  file		= {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Phillips, Guenther, McAree
		  - 2017 - When the Dust Settles The Four Behaviors of LiDAR
		  in the Presence of Fine Airborne Particulates.pdf:pdf},
  isbn		= {9783902661623},
  issn		= {15564967},
  journal	= {Journal of Field Robotics},
  mendeley-groups={Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
  month		= {aug},
  number	= {5},
  pages		= {985--1009},
  pmid		= {22164016},
  publisher	= {Wiley-Blackwell},
  title		= {{When the Dust Settles: The Four Behaviors of LiDAR in the
		  Presence of Fine Airborne Particulates}},
  url		= {http://doi.wiley.com/10.1002/rob.21701},
  volume	= {34},
  year		= {2017}
}

@InProceedings{pinchon2018,
  author    = {Pinchon, Nicolas and Cassignol, Olivier and Nicolas, Adrien and Bernardin, Fr{\'{e}}d{\'{e}}ric and Leduc, Patrick and Tarel, Jean-Philippe and Br{\'{e}}mond, Roland and Bercier, Emmanuel and Brunet, Johann},
  title     = {{All-Weather Vision for Automotive Safety: Which Spectral Band?}},
  booktitle = {Advanced Microsystems for Automotive Applications 2018},
  year      = {2018},
  pages     = {3--15},
  month     = {sep},
  publisher = {Springer, Cham},
  doi       = {10.1007/978-3-319-99762-9_1},
  url       = {http://link.springer.com/10.1007/978-3-319-99762-9{\_}1},
}

@Article{palazzi2018,
  author          = {Palazzi, Andrea and Abati, Davide and Calderara, Simone and Solera, Francesco and Cucchiara, Rita},
  title           = {{Predicting the Driver's Focus of Attention: the DR(eye)VE Project}},
  journal         = {(preprint) IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year            = {2018},
  month           = {may},
  issn            = {01628828},
  abstract        = {In this work we aim to predict the driver{\&}{\#}x0027;s
		  focus of attention. The goal is to estimate what a person
		  would pay attention to while driving, and which part of the
		  scene around the vehicle is more critical for the task. To
		  this end we propose a new computer vision model based on a
		  multi-branch deep architecture that integrates three
		  sources of information: raw video, motion and scene
		  semantics. We also introduce DR(eye)VE, the largest dataset
		  of driving scenes for which eye-tracking annotations are
		  available. This dataset features more than 500,000
		  registered frames, matching ego-centric views (from glasses
		  worn by drivers) and car-centric views (from roof-mounted
		  camera), further enriched by other sensors measurements.
		  Results highlight that several attention patterns are
		  shared across drivers and can be reproduced to some extent.
		  The indication of which elements in the scene are likely to
		  capture the driver{\&}{\#}x0027;s attention may benefit
		  several applications in the context of human-vehicle
		  interaction and driver attention analysis.},
  archiveprefix   = {arXiv},
  arxivid         = {1705.03854},
  doi             = {10.1109/TPAMI.2018.2845370},
  eprint          = {1705.03854},
  file            = {::},
  keywords        = {Cameras,Computational modeling,Predictive models,Semantics,Task analysis,Vehicles,Visualization,driver{\&}{\#}x0027,focus of attention,gaze prediction,s attention},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  url             = {http://arxiv.org/abs/1705.03854},
}

@Misc{mobileye2018,
  author    = {Mobileye},
  title     = {{Our Technology - Mobileye}},
  year      = {2018},
  booktitle = {Mobileye webpage},
  url       = {https://www.mobileye.com/our-technology/},
  urldate   = {2018-10-06},
}

@Article{garcia2018,
  author          = {Garcia, Missael and Davis, Tyler and Blair, Steven and Cui, Nan and Gruev, Viktor},
  title           = {{Bioinspired polarization imager with high dynamic range}},
  journal         = {Optica},
  year            = {2018},
  volume          = {5},
  number          = {10},
  pages           = {1240},
  month           = {oct},
  issn            = {2334-2536},
  abstract        = {Polarization is one of the three fundamental properties of
		  light, along with color and intensity, yet most vertebrate
		  species, including humans, are blind with respect to this
		  light modality. In contrast, many invertebrates, including
		  insects, spiders, cephalopods, and stomatopods, have
		  evolved to detect polarization information with
		  high-dynamic-range photosensitive cells and utilize this
		  information in visually guided behavior. In this paper, we
		  present a high-dynamic-range polarization imaging sensor
		  inspired by the visual system of the mantis shrimp. Our
		  bioinspired imager achieves 140 dB dynamic range and 61 dB
		  maximum signal-to-noise ratio across 384 × 288 pixels
		  equipped with logarithmic photodiodes. Contrary to
		  state-of-the-art active pixel sensors, where photodiodes in
		  individual pixels operate in reverse bias mode and yield up
		  to ∼60 dB dynamic range, our pixel has a logarithmic
		  response by operating individual photodiodes in forward
		  bias mode. This novel pixel circuitry is monolithically
		  integrated with pixelated polarization filters composed of
		  250-nm-tall × 75-nm-wide aluminum nanowires to enable
		  snapshot polarization imaging at 30 frames per second. This
		  sensor can enable many automotive and remote sensing
		  applications, where high-dynamic-range imaging augmented
		  with polarization information can provide critical
		  information during hazy or rainy conditions.},
  doi             = {10.1364/OPTICA.5.001240},
  file            = {::},
  keywords        = {(1105405) Polarimetric imaging,(1305440) Polarization-selective devices,(2804788) Optical sensing and sensors,OCIS codes: (2605430) Polarization},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  publisher       = {Optical Society of America},
  url             = {https://www.osapublishing.org/abstract.cfm?URI=optica-5-10-1240
		  https://doi.org/10.1364/OPTICA.5.001240},
}

@Article{engel2018,
  author        = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  title         = {{Direct Sparse Odometry}},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year          = {2018},
  volume        = {40},
  number        = {3},
  pages         = {611--625},
  month         = {jul},
  issn          = {01628828},
  abstract      = {We propose a novel direct sparse visual odometry
		  formulation. It combines a fully direct probabilistic model
		  (minimizing a photometric error) with consistent, joint
		  optimization of all model parameters, including geometry --
		  represented as inverse depth in a reference frame -- and
		  camera motion. This is achieved in real time by omitting
		  the smoothness prior used in other direct methods and
		  instead sampling pixels evenly throughout the images. Since
		  our method does not depend on keypoint detectors or
		  descriptors, it can naturally sample pixels from across all
		  image regions that have intensity gradient, including edges
		  or smooth intensity variations on mostly white walls. The
		  proposed model integrates a full photometric calibration,
		  accounting for exposure time, lens vignetting, and
		  non-linear response functions. We thoroughly evaluate our
		  method on three different datasets comprising several hours
		  of video. The experiments show that the presented approach
		  significantly outperforms state-of-the-art direct and
		  indirect methods in a variety of real-world settings, both
		  in terms of tracking accuracy and robustness.},
  archiveprefix = {arXiv},
  arxivid       = {1607.02565},
  doi           = {10.1109/TPAMI.2017.2658577},
  eprint        = {1607.02565},
  file          = {::},
  isbn          = {0162-8828 VO - PP},
  keywords      = {3D reconstruction,SLAM,Visual odometry,structure from motion},
  pmid          = {28060704},
  url           = {http://arxiv.org/abs/1607.02565},
}

@Misc{edelstein2018,
  author    = {Edelstein, Stephen},
  title     = {{Intel/Mobileye Self-Driving Cars Begin Testing in Jerusalem - The Drive}},
  year      = {2018},
  booktitle = {The Drive},
  url       = {http://www.thedrive.com/tech/20919/intel-mobileye-self-driving-cars-begin-testing-in-jerusalem},
  urldate   = {2018-10-06},
}

###Article{	  aqel2016,
  abstract	= {Accurate localization of a vehicle is a fundamental
		  challenge and one of the most important tasks of mobile
		  robots. For autonomous navigation, motion tracking, and
		  obstacle detection and avoidance, a robot must maintain
		  knowledge of its position over time. Vision-based odometry
		  is a robust technique utilized for this purpose. It allows
		  a vehicle to localize itself robustly by using only a
		  stream of images captured by a camera attached to the
		  vehicle. This paper presents a review of state-of-the-art
		  visual odometry (VO) and its types, approaches,
		  applications, and challenges. VO is compared with the most
		  common localization sensors and techniques, such as
		  inertial navigation systems, global positioning systems,
		  and laser sensors. Several areas for future research are
		  also highlighted.},
  author	= {Aqel, Mohammad O A and Marhaban, Mohammad H and Saripan, M
		  Iqbal and Ismail, Napsiah Bt},
  doi		= {10.1186/s40064-016-3573-7},
  file		= {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Aqel et al. - 2016 -
		  Review of visual odometry types, approaches, challenges,
		  and applications.pdf:pdf},
  issn		= {2193-1801},
  journal	= {SpringerPlus},
  keywords	= {Global positioning system,Image stream,Inertial navigation
		  system,Localization sensors,Visual odometry},
  mendeley-groups={Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
  number	= {1},
  pages		= {1897},
  pmid		= {27843754},
  publisher	= {Springer},
  title		= {{Review of visual odometry: types, approaches, challenges,
		  and applications.}},
  url		= {http://www.ncbi.nlm.nih.gov/pubmed/27843754
		  http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5084145},
  volume	= {5},
  year		= {2016}
}

@Misc{auvsi2018,
  author    = {AUVSI},
  title     = {{Ambarella testing its fully autonomous EVA vehicle on the roads of Silicon Valley |}},
  year      = {2018},
  booktitle = {Association for Unmanned Vehicle Systems International},
  url       = {https://www.auvsi.org/industry-news/ambarella-testing-its-fully-autonomous-eva-vehicle-roads-silicon-valley},
  urldate   = {2018-10-06},
}

@Misc{ambarella2018,
  author    = {Ambarella},
  title     = {{Ambarella Introduces CV2 4K Computer Vision SoC with CVflow™ Architecture and Stereovision}},
  year      = {2018},
  booktitle = {Ambarella webpage},
  url       = {https://www.ambarella.com/news/122/74/Ambarella-Introduces-CV2-4K-Computer-Vision-SoC-with-CVflow-Architecture-and-Stereovision},
  urldate   = {2018-10-06},
}

@Misc{shalev-shwartz2017,
  author        = {Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  title         = {{On a Formal Model of Safe and Scalable Self-driving Cars}},
  month         = {aug},
  year          = {2017},
  abstract      = {In recent years, car makers and tech companies have been
		  racing towards self driving cars. It seems that the main
		  parameter in this race is who will have the first car on
		  the road. The goal of this paper is to add to the equation
		  two additional crucial parameters. The first is
		  standardization of safety assurance --- what are the
		  minimal requirements that every self-driving car must
		  satisfy, and how can we verify these requirements. The
		  second parameter is scalability --- engineering solutions
		  that lead to unleashed costs will not scale to millions of
		  cars, which will push interest in this field into a niche
		  academic corner, and drive the entire field into a "winter
		  of autonomous driving". In the first part of the paper we
		  propose a white-box, interpretable, mathematical model for
		  safety assurance, which we call Responsibility-Sensitive
		  Safety (RSS). In the second part we describe a design of a
		  system that adheres to our safety assurance requirements
		  and is scalable to millions of cars.},
  archiveprefix = {arXiv},
  arxivid       = {1708.06374},
  doi           = {1708.06374v2},
  eprint        = {1708.06374},
  file          = {::},
  url           = {http://arxiv.org/abs/1708.06374},
}

@Misc{bosch2018,
  author          = {B{\"{o}}sch, Patrick M. and Becker, Felix and Becker, Henrik and Axhausen, Kay W.},
  title           = {{Cost-based analysis of autonomous mobility services}},
  month           = {may},
  year            = {2017},
  abstract        = {Fast advances in autonomous driving technology trigger the
		  question of suitable operational models for future
		  autonomous vehicles. A key determinant of such operational
		  models' viability is the competitiveness of their cost
		  structures. Using a comprehensive analysis of the
		  respective cost structures, this research shows that public
		  transportation (in its current form) will only remain
		  economically competitive where demand can be bundled to
		  larger units. In particular, this applies to dense urban
		  areas, where public transportation can be offered at lower
		  prices than autonomous taxis (even if pooled) and private
		  cars. Wherever substantial bundling is not possible, shared
		  and pooled vehicles serve travel demand more efficiently.
		  Yet, in contrast to current wisdom, shared fleets may not
		  be the most efficient alternative. Higher costs and more
		  effort for vehicle cleaning could change the equation.
		  Moreover, the results suggest that a substantial share of
		  vehicles may remain in private possession and use due to
		  their low variable costs. Even more than today, high fixed
		  costs of private vehicles will continue to be accepted,
		  given the various benefits of a private mobility robot.},
  booktitle       = {Transport Policy},
  doi             = {10.1016/j.tranpol.2017.09.005},
  file            = {::},
  isbn            = {9783033046412},
  issn            = {1879310X},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  pages           = {76--91},
  publisher       = {Pergamon},
  url             = {https://www.sciencedirect.com/science/article/pii/S0967070X17300811},
  volume          = {64},
}

###Article{	  wang2008,
  abstract	= {In the visible and near IR, absorption is negligible so
		  that the atmospheric extinction can be derived by
		  atmospheric scattering which is mainly contributed by fog
		  droplet, rain droplet, another types of droplet and small
		  articles. The forward-scattering visibility meter (FVM)
		  works by illuminating with near IR light a small sample
		  volume of about 100 mL of air and measuring the intensity
		  scattered in the angular range of 30° to 36° degrees. The
		  scattered intensity is proportional to the extinction
		  coefficient regardless of the article size distribution and
		  after wavelength calibration. The ratio of scattered signal
		  to extinction coefficient of fog and haze can be achieved
		  by comparative test of FVM outputs and manual observations.
		  Nevertheless, as a result of the application of the
		  measurement during rain with the ratio of fog and haze, an
		  unacceptable error is raised. To obtain an accuracy
		  extinction measurement during rain, an appropriated ratio
		  of scattered signal to extinction coefficient of rain would
		  be found. The calculation for different size distributions
		  of fog and rain with Mie theory has been made in this
		  paper. And a comparison of extinction measurements made
		  with two FVMs and manual observations during fog and rain
		  has been made. The result shows that during rain the FVM
		  extinction coefficient is from 20{\%} to 60{\%} greater
		  than that of manual observations. This result can be used
		  to define correction factors so that the FVM using
		  forward-scattering near IR spectroscopy not only can be
		  used to estimate extinction during fog and haze as well as
		  during rain.},
  author	= {Wang, M. and Liu, W.-Q. and Lu, Y.-H. and Zhao, X.-S. and
		  Song, B.-C. and Zhang, Y.-J. and Wang, Y.-P. and Lian,
		  C.-H. and Chen, Jun and Cheng, Yin and Liu, J.-G. and Wei,
		  Q.-N.},
  issn		= {10000593},
  journal	= {Guang Pu Xue Yu Guang Pu Fen Xi/Spectroscopy and Spectral
		  Analysis},
  keywords	= {Atmospheric extinction,Extinction
		  coefficient,Forward-scattering,Near IR,Phase function,Size
		  distribution,Visibility},
  mendeley-groups={Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
  month		= {aug},
  number	= {8},
  pages		= {1776--80},
  pmid		= {18975801},
  title		= {{Study on the measurement of the atmospheric extinction of
		  fog and rain by forward-scattering near infrared
		  spectroscopy}},
  url		= {http://www.ncbi.nlm.nih.gov/pubmed/18975801},
  volume	= {28},
  year		= {2008}
}

@Misc{waymoteam2017,
  author    = {Waymo team},
  title     = {{Introducing Waymo's suite of custom-built, self-driving hardware}},
  year      = {2017},
  booktitle = {medium.com},
  url       = {https://medium.com/waymo/introducing-waymos-suite-of-custom-built-self-driving-hardware-c47d1714563},
  urldate   = {2018-10-06},
}

###InProceedings{ zhou2014lidar,
  title		= {LIDAR and vision-based real-time traffic sign detection
		  and recognition algorithm for intelligent vehicle},
  author	= {Zhou, Lipu and Deng, Zhidong},
  booktitle	= {Intelligent Transportation Systems (ITSC), 2014 IEEE 17th
		  International Conference on},
  year		= {2014},
  organization	= {IEEE},
  pages		= {578--583}
}

@InCollection{ziebinski2016,
  author    = {Ziebinski, Adam and Cupek, Rafal and Erdogan, Hueseyin and Waechter, Sonja},
  title     = {{A Survey of ADAS Technologies for the Future Perspective of Sensor Fusion}},
  publisher = {Springer, Cham},
  year      = {2016},
  pages     = {135--146},
  month     = {sep},
  doi       = {10.1007/978-3-319-45246-3_13},
  file      = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Ziebinski et al. - 2016 -
		  A Survey of ADAS Technologies for the Future Perspective of
		  Sensor Fusion.pdf:pdf},
  url       = {http://link.springer.com/10.1007/978-3-319-45246-3{\_}13},
}

###PhDThesis{	  zhang2016,
  title		= {Rapid Inspection of Pavement Markings Using Mobile Laser
		  Scanning Point Clouds},
  author	= {Zhang, Haocheng},
  school	= {University of Waterloo},
  year		= {2016},
  month		= {mar},
  file		= {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Zhang - 2016 - Rapid
		  Inspection of Pavement Markings Using Mobile Laser Scanning
		  Point Clouds.pdf:pdf},
  url		= {https://uwspace.uwaterloo.ca/handle/10012/10343}
}

@Article{zhang2016a,
  author          = {Zhang, Libo and Yang, Lin and Luo, Tiejian},
  title           = {{Unified saliency detection model using color and texture features}},
  journal         = {PLoS ONE},
  year            = {2016},
  volume          = {11},
  number          = {2},
  pages           = {e0149328},
  issn            = {19326203},
  abstract        = {Saliency detection attracted attention of many researchers
		  and had become a very active area of research. Recently,
		  many saliency detection models have been proposed and
		  achieved excellent performance in various fields. However,
		  most of these models only consider low-level features. This
		  paper proposes a novel saliency detection model using both
		  color and texture features and incorporating higher-level
		  priors. The SLIC superpixel algorithm is applied to form an
		  over-segmentation of the image. Color saliency map and
		  texture saliency map are calculated based on the region
		  contrast method and adaptive weight. Higher-level priors
		  including location prior and color prior are incorporated
		  into the model to achieve a better performance and full
		  resolution saliency map is obtained by using the
		  up-sampling method. Experimental results on three datasets
		  demonstrate that the proposed saliency detection model
		  outperforms the state-of-the-art models.},
  doi             = {10.1371/journal.pone.0149328},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  pmid            = {26889826},
  publisher       = {Public Library of Science},
  url             = {http://www.ncbi.nlm.nih.gov/pubmed/26889826
		  http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4758633},
}

###InProceedings{ schreiber2013laneloc,
  title		= {Laneloc: Lane marking based localization using highly
		  accurate maps},
  author	= {Schreiber, Markus and Kn{\"o}ppel, Carsten and Franke,
		  Uwe},
  booktitle	= {Intelligent Vehicles Symposium (IV), 2013 IEEE},
  year		= {2013},
  organization	= {IEEE},
  pages		= {449--454}
}

@TechReport{shalev-shwartz2016,
  author        = {Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  title         = {{Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving}},
  year          = {2016},
  month         = {oct},
  abstract      = {Autonomous driving is a multi-agent setting where the host
		  vehicle must apply sophisticated negotiation skills with
		  other road users when overtaking, giving way, merging,
		  taking left and right turns and while pushing ahead in
		  unstructured urban roadways. Since there are many possible
		  scenarios, manually tackling all possible cases will likely
		  yield a too simplistic policy. Moreover, one must balance
		  between unexpected behavior of other drivers/pedestrians
		  and at the same time not to be too defensive so that normal
		  traffic flow is maintained. In this paper we apply deep
		  reinforcement learning to the problem of forming long term
		  driving strategies. We note that there are two major
		  challenges that make autonomous driving different from
		  other robotic tasks. First, is the necessity for ensuring
		  functional safety-something that machine learning has
		  difficulty with given that performance is optimized at the
		  level of an expectation over many instances. Second, the
		  Markov Decision Process model often used in robotics is
		  problematic in our case because of unpredictable behavior
		  of other agents in this multi-agent scenario. We make three
		  contributions in our work. First, we show how policy
		  gradient iterations can be used, and the variance of the
		  gradient estimation using stochastic gradient ascent can be
		  minimized, without Markovian assumptions. Second, we
		  decompose the problem into a composition of a Policy for
		  Desires (which is to be learned) and trajectory planning
		  with hard constraints (which is not learned). The goal of
		  Desires is to enable comfort of driving, while hard
		  constraints guarantees the safety of driving. Third, we
		  introduce a hierarchical temporal abstraction we call an
		  "Option Graph" with a gating mechanism that significantly
		  reduces the effective horizon and thereby reducing the
		  variance of the gradient estimation even further. The
		  Option Graph plays a similar role to "structured
		  prediction" in supervised learning, thereby reducing sample
		  complexity, while also playing a similar role to LSTM
		  gating mechanisms used in supervised deep networks.},
  archiveprefix = {arXiv},
  arxivid       = {1610.03295v1},
  booktitle     = {Arxiv},
  eprint        = {1610.03295v1},
  file          = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Shalev-shwartz, Shammah,
		  Shashua - 2016 - Safe, Multi-Agent, Reinforcement Learning
		  for Autonomous Driving(2).pdf:pdf},
  url           = {https://cdn.mobileye.com/wp-content/uploads/2016/10/Safe-Multi-Agent-Reinforcement-Learning-for-Autonomous-Driving.pdf
		  http://arxiv.org/abs/1610.03295},
}

@InCollection{perez2016,
  author    = {P{\'{e}}rez, Joshu{\'{e}} and Gonzalez, David and Milan{\'{e}}s, Vicente},
  title     = {{Vehicle Control in ADAS Applications: State of the Art}},
  booktitle = {Intelligent Transport Systems: Technologies and Applications},
  publisher = {John Wiley {\&} Sons, Ltd},
  year      = {2016},
  pages     = {206--219},
  address   = {Chichester, UK},
  month     = {oct},
  isbn      = {9781118894774},
  abstract  = {... driver models previously validated are compared with
		  current driver behaviour [4]. The ... dated$\backslash$nand
		  demonstrated important intermediate steps towards highly
		  automated driving for passenger ...$\backslash$nissues
		  remained open, eg managing between manual and fully
		  autonomous driving. ... $\backslash$n},
  doi       = {10.1002/9781118894774.ch11},
  keywords  = {Advanced driver assistance system,Autonomous driving,Intelligent transportation systems,Lateral control,Longitudinal control,Vehicle control},
  url       = {http://doi.wiley.com/10.1002/9781118894774.ch11},
}

@InProceedings{poulton2016,
  author          = {Poulton, Christopher V and Cole, David B and Yaacobi, Ami and Watts, Michael R},
  title           = {{Frequency-modulated Continuous-wave LIDAR Module in Silicon Photonics}},
  booktitle       = {Optical Fiber Communication Conference},
  year            = {2016},
  number          = {c},
  pages           = {4--6},
  address         = {Anaheim},
  abstract        = {Frequency-modulated continuous-wave LIDAR is demonstrated
		  with a silicon photonic device consisting of transmitting
		  and receiving waveguides and photodetectors. A 20 mm
		  resolution and 2 m range is shown. Simultaneous distance
		  and velocity measurements are achieved.},
  doi             = {10.1364/OFC.2016.W4E.3},
  isbn            = {9781943580071},
  keywords        = {3,Integra,Remote sensing and sensors: 280.3640 Lidar,pdf},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  url             = {https://ieeexplore.ieee.org/document/7537823},
}

@InProceedings{duthon2016,
  author          = {Duthon, Pierre and Colomb, Mich{\`{e}}le and Kuntzmann, Laboratoire Jean},
  title           = {{Visual saliency on the road: model and database dependent detection}},
  booktitle       = {Actes du 20{\`{e}}me congr{\`{e}}s national sur la Reconnaissance des Formes et l'Intelligence Artificielle},
  year            = {2016},
  month           = {jun},
  abstract        = {In the road context, objects of interest (salient or not)
		  must be efficiently detected under any condition to ensure
		  safety, for both driver assistance systems and autonomous
		  vehicles. Nine representative state-of-the-art saliency
		  models are evaluated on driving databases (human perception
		  vs. robotics). Although not sufficient for robust
		  detection, bottom-up saliency provides important
		  information, especially when controlling for the classical
		  biases.},
  file            = {::},
  keywords        = {road context,saliency,target detection,visual attention},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  url             = {https://hal.archives-ouvertes.fr/hal-01359997},
}

###Article{	  chun2008,
  abstract	= {This paper focuses on the rolling shutter distortion of
		  CMOS image sensor coming from its unique readout mechanism
		  as the main cause for image degradation when there are
		  fast-moving objects. This paper proposes a post image
		  processing scheme based on motion vector detection to
		  suppress the rolling shutter distortion. Motion vector
		  detection is performed based on an optical flow method at a
		  reasonable computational complexity. A practical
		  implementation scheme is also described.},
  author	= {Chun, Jung Bum and Jung, Hunjoon and Kyung, Chong Min},
  doi		= {10.1109/TCE.2008.4711190},
  issn		= {00983063},
  journal	= {IEEE Transactions on Consumer Electronics},
  keywords	= {CMOS image sensor,Post-processing
		  technique,Rolling-shutter distortion},
  mendeley-groups={Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
  month		= {nov},
  number	= {4},
  pages		= {1479--1487},
  title		= {{Suppressing rolling-shutter distortion of CMOS image
		  sensors by motion vector detection}},
  url		= {http://ieeexplore.ieee.org/document/4711190/},
  volume	= {54},
  year		= {2008}
}

@Article{cornick2016,
  author          = {Cornick, Matthew and Koechling, Jeffrey and Stanley, Byron and Zhang, Beijia},
  title           = {{Localizing Ground Penetrating RADAR: A Step Toward Robust Autonomous Ground Vehicle Localization}},
  journal         = {Journal of Field Robotics},
  year            = {2016},
  volume          = {33},
  number          = {1},
  pages           = {82--102},
  month           = {jan},
  issn            = {15564967},
  abstract        = {It is anticipatedthat theMars ScienceLaboratory rover,
		  namedCuriosity,will traverse 10–20 kmon the surface of
		  Mars during its primary mission. In preparation for this
		  traverse, Earth-based tests were performed using Mars
		  weight vehicles. These vehicles were driven over Mars
		  analog bedrock, cohesive soil, and cohesionless sand at
		  various slopes. Vehicle slip was characterized on each of
		  these terrains versus slope for direct upslope driving.
		  Results show that slopes up to 22 degrees are traversable
		  on smooth bedrock and that slopes up to 28 degrees are
		  traversable on some cohesive soils. In cohesionless sand,
		  results show a sharp transition between moderate slip on 10
		  degree slopes and vehicle embedding at 17 degrees. For
		  cohesionless sand, data are also presented showing the
		  relationship between vehicle slip and wheel sinkage. Side
		  by side testing of the Mars Exploration Rover test vehicle
		  and the Mars Science Laboratory test vehicle show how
		  increased wheel diameter leads to better slope climbing
		  ability in sand for vehicles with nearly identical ground
		  pressure. Lastly, preliminary data from Curiosity's initial
		  driving on Mars are presented and compared to the
		  Earth-based testing, showing good agreement for the driving
		  done during the first 250 Martian days.},
  archiveprefix   = {arXiv},
  arxivid         = {10.1.1.91.5767},
  doi             = {10.1002/rob.21605},
  eprint          = {10.1.1.91.5767},
  file            = {::},
  isbn            = {9783902661623},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  pmid            = {22164016},
  publisher       = {Wiley-Blackwell},
  url             = {http://doi.wiley.com/10.1002/rob.21605},
}

@Misc{chapell2016,
  author          = {Chapell, Lindsay},
  title           = {{The Big Bang of autonomous driving}},
  year            = {2016},
  abstract        = {DARPA Challenge brought lab tests out into the real world,
		  and started a movement},
  booktitle       = {Automotive News},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  url             = {http://www.autonews.com/article/20161219/OEM06/312199908/the-big-bang-of-autonomous-driving},
  urldate         = {2018-10-08},
}

@Article{ziegler2014,
  author   = {Ziegler, Julius and Bender, Philipp and Schreiber, Markus and Lategahn, Henning and Strauss, Tobias and Stiller, Christoph and Dang, Thao and Franke, Uwe and Appenrodt, Nils and Keller, Christoph G. and Kaus, Eberhard and Herrtwich, Ralf G. and Rabe, Clemens and Pfeiffer, David and Lindner, Frank and Stein, Fridtjof and Erbs, Friedrich and Enzweiler, Markus and Knoppel, Carsten and Hipp, Jochen and Haueis, Martin and Trepte, Maximilian and Brenk, Carsten and Tamke, Andreas and Ghanaat, Mohammad and Braun, Markus and Joos, Armin and Fritz, Hans and Mock, Horst and Hein, Martin and Zeeb, Eberhard},
  title    = {{Making bertha drive-an autonomous journey on a historic route}},
  journal  = {IEEE Intelligent Transportation Systems Magazine},
  year     = {2014},
  volume   = {6},
  number   = {2},
  pages    = {8--20},
  issn     = {19391390},
  abstract = {125 years after Bertha Benz completed the first overland
		  journey in automotive history, the Mercedes Benz S-Class S
		  500 INTELLIGENT DRIVE followed the same route from Mannheim
		  to Pforzheim, Germany, in fully autonomous manner. The
		  autonomous vehicle was equipped with close-to-production
		  sensor hardware and relied solely on vision and radar
		  sensors in combination with accurate digital maps to obtain
		  a comprehensive understanding of complex traffic
		  situations. The historic Bertha Benz Memorial Route is
		  particularly challenging for autonomous driving. The course
		  taken by the autonomous vehicle had a length of 103 km and
		  covered rural roads, 23 small villages and major cities
		  (e.g. downtown Mannheim and Heidelberg). The route posed a
		  large variety of difficult traffic scenarios including
		  intersections with and without traffic lights, roundabouts,
		  and narrow passages with oncoming traffic. This paper gives
		  an overview of the autonomous vehicle and presents details
		  on vision and radar-based perception, digital road maps and
		  video-based self-localization, as well as motion planning
		  in complex urban scenarios.},
  doi      = {10.1109/MITS.2014.2306552},
  isbn     = {1939-1390},
  url      = {http://ieeexplore.ieee.org/document/6803933/},
}

@TechReport{frost&sullivan2014,
  author = {{Frost {\&} Sullivan}},
  title  = {{2011 European Consumers' Desirability and Willingness to Pay for Advanced Driver Assistance and Driving Dynamics Technologies Market Research}},
  year   = {2014},
  url    = {https://store.frost.com/2011-european-consumers-desirability-and-willingness-to-pay-for-advanced-driver-assistance-and-driving-dynamics-technologies.html?},
}

###Misc{	  eldada2017,
  address	= {San Francisco},
  author	= {Eldada, Louay},
  booktitle	= {Automated Vehicles Symposium},
  file		= {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Eldada - 2017 - LiDAR and
		  the Autonomous Vehicle Revolution for Truck and Ride
		  Sharing Fleets.pdf:pdf},
  mendeley-groups={Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
  title		= {{LiDAR and the Autonomous Vehicle Revolution for Truck and
		  Ride Sharing Fleets}},
  url		= {https://higherlogicdownload.s3.amazonaws.com/AUVSI/14c12c18-fde1-4c1d-8548-035ad166c766/UploadedImages/2017/PDFs/Proceedings/ESS/Wednesday
		  1330-1400{\_}Louay Eldada.pdf},
  year		= {2017}
}

@InProceedings{engel2014,
  author    = {Engel, Jakob and Sch{\"{o}}ps, Thomas and Cremers, Daniel},
  title     = {{LSD-SLAM: Large-Scale Direct monocular SLAM}},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year      = {2014},
  volume    = {8690 LNCS},
  number    = {PART 2},
  pages     = {834--849},
  publisher = {Springer, Cham},
  abstract  = {We propose a direct (feature-less) monocular SLAM
		  algorithm which, in contrast to current state-of-the-art
		  regarding direct meth- ods, allows to build large-scale,
		  consistent maps of the environment. Along with highly
		  accurate pose estimation based on direct image alignment,
		  the 3D environment is reconstructed in real-time as
		  pose-graph of keyframes with associated semi-dense depth
		  maps. These are obtained by filtering over a large number
		  of pixelwise small-baseline stereo comparisons. The
		  explicitly scale-drift aware formulation allows the
		  approach to operate on challenging sequences including
		  large variations in scene scale. Major enablers are two key
		  novelties: (1) a novel direct tracking method which
		  operates on sim(3), thereby explicitly detecting
		  scale-drift, and (2) an elegant probabilistic solution to
		  include the effect of noisy depth values into tracking. The
		  resulting direct monocular SLAM system runs in real-time on
		  a CPU.},
  doi       = {10.1007/978-3-319-10605-2_54},
  isbn      = {9783319106045},
  issn      = {16113349},
  pmid      = {638263},
  url       = {http://link.springer.com/10.1007/978-3-319-10605-2{\_}54},
}

###InProceedings{ bak2012,
  author	= {Bak, Adrien and Gruyer, Dominique and Bouchafa, Samia and
		  Aubert, Didier},
  booktitle	= {2012 15th International IEEE Conference on Intelligent
		  Transportation Systems},
  doi		= {10.1109/ITSC.2012.6338771},
  isbn		= {978-1-4673-3063-3},
  month		= {sep},
  pages		= {1365--1370},
  publisher	= {IEEE},
  title		= {{Multi-sensor localization - Visual Odometry as a low cost
		  proprioceptive sensor}},
  url		= {http://ieeexplore.ieee.org/document/6338771/},
  year		= {2012}
}

###InProceedings{ bak2012,
  title		= {Multi-sensor localization - Visual Odometry as a low cost
		  proprioceptive sensor},
  author	= {Bak, Adrien and Gruyer, Dominique and Bouchafa, Samia and
		  Aubert, Didier},
  booktitle	= {2012 15th International IEEE Conference on Intelligent
		  Transportation Systems},
  year		= {2012},
  month		= {sep},
  pages		= {1365--1370},
  publisher	= {IEEE},
  doi		= {10.1109/ITSC.2012.6338771},
  isbn		= {978-1-4673-3063-3},
  url		= {http://ieeexplore.ieee.org/document/6338771/}
}

@InProceedings{bender2014,
  author    = {Bender, Philipp and Ziegler, Julius and Stiller, Christoph},
  title     = {{Lanelets: Efficient map representation for autonomous driving}},
  booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
  year      = {2014},
  pages     = {420--425},
  month     = {jun},
  publisher = {IEEE},
  abstract  = {In this paper we propose a highly detailed map for the
		  field of autonomous driving. We introduce the notion of
		  lanelets to represent the drivable environment under both
		  geometrical and topological aspects. Lanelets are atomic,
		  inter- connected drivable road segments which may carry
		  additional data to describe the static environment. We
		  describe the map specification, an example creation process
		  as well as the access library libLanelet which is available
		  for download. Based on the map, we briefly describe our
		  behavioural layer (which we call behaviour generation)
		  which is heavily exploiting the proposed map structure.
		  Both contributions have been used throughout the autonomous
		  journey of the MERCEDES BENZ S 500 INTELLIGENT DRIVE
		  following the Bertha Benz Memorial Route in summer 2013.},
  doi       = {10.1109/IVS.2014.6856487},
  file      = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Bender, Ziegler, Stiller -
		  2014 - Lanelets Efficient map representation for autonomous
		  driving.pdf:pdf},
  isbn      = {9781479936380},
  url       = {http://ieeexplore.ieee.org/document/6856487/},
}

@Article{broggi2013,
  author  = {Broggi, Alberto and Cattani, Stefano and Medici, Paolo and Zani, Paolo},
  title   = {{Applications of computer vision to vehicles: An extreme test}},
  journal = {Studies in Computational Intelligence},
  year    = {2013},
  volume  = {411},
  pages   = {215--250},
  issn    = {1860949X},
  doi     = {10.1007/978-3-642-28661-2_9},
  file    = {::},
  isbn    = {9783642286605},
  url     = {https://link.springer.com/content/pdf/10.1007/978-3-642-28661-2{\_}9.pdf},
}

@Article{anderson2013,
  author   = {Anderson, Sterling J. and Karumanchi, Sisir B. and Iagnemma, Karl and Walker, James M.},
  title    = {{The intelligent copilot: A constraint-based approach to shared-adaptive control of ground vehicles}},
  journal  = {IEEE Intelligent Transportation Systems Magazine},
  year     = {2013},
  volume   = {5},
  number   = {2},
  pages    = {45--54},
  issn     = {1939-1390},
  abstract = {This work presents a new approach to semi-autonomous
		  vehicle hazard avoidance and stability control, based on
		  the design and selective enforcement of constraints. This
		  differs from traditional approaches that rely on the
		  planning and tracking of paths and facilitates
		  "minimally-invasive" control for human-machine systems.
		  Instead of forcing a human operator to follow an
		  automation-determined path, the constraint-based approach
		  identifies safe homotopies, and allows the operator to
		  navigate freely within them, introducing control action
		  only as necessary to ensure that the vehicle does not
		  violate safety constraints. This method evaluates candidate
		  homotopies based on "restrictiveness," rather than
		  traditional measures of path goodness, and designs and
		  enforces requisite constraints on the human's control
		  commands to ensure that the vehicle never leaves the
		  controllable subset of a desired homotopy. This paper
		  demonstrates the approach in simulation and characterizes
		  its effect on human teleoperation of unmanned ground
		  vehicles via a 20-user, 600-trial study on an outdoor
		  obstacle course. Aggregated across all drivers and
		  experiments, the constraintbased control system required an
		  average of 43{\%} of the available control authority to
		  reduce collision frequency by 78{\%} relative to
		  traditional teleoperation, increase average speed by
		  26{\%}, and moderate operator steering commands by 34{\%}.},
  doi      = {10.1109/MITS.2013.2247796},
  isbn     = {1939-1390},
  url      = {http://ieeexplore.ieee.org/document/6507273/},
}

@InProceedings{funke2012,
  author    = {Funke, Joseph and Theodosis, Paul and Hindiyeh, Rami and Stanek, Ganymed and Kritatakirana, Krisada and Gerdes, Chris and Langer, Dirk and Hernandez, Marcial and M{\"{u}}ller-Bessler, Bernhard and Huhnke, Burkhard},
  title     = {{Up to the limits: Autonomous Audi TTS}},
  booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
  year      = {2012},
  pages     = {541--547},
  month     = {jun},
  publisher = {IEEE},
  abstract  = {This paper presents a novel approach to au- tonomous
		  driving at the vehicle's handling limits. Such a system
		  requires a high speed, consistent control signal as well as
		  numerous safety features capable of monitoring and stopping
		  the vehicle. When operating, the system's high level
		  controller utilizes a highly accurate differential GPS and
		  known friction values to drive a precomputed path at the
		  friction limits of the vehicle. The system was tested in a
		  variety of road conditions, including the challenging Pikes
		  Peak Hill climb. Results from this work can be extended to
		  improve driving safety and accident avoidance in
		  vehicles.},
  doi       = {10.1109/IVS.2012.6232212},
  isbn      = {9781467321198},
  issn      = {1931-0587},
  url       = {http://ieeexplore.ieee.org/document/6232212/},
}

@Article{broggi2012,
  author    = {Broggi, A. and Medici, P. and Zani, P. and Coati, A. and Panciroli, M.},
  title     = {{Autonomous vehicles control in the VisLab Intercontinental Autonomous Challenge}},
  journal   = {Annual Reviews in Control},
  year      = {2012},
  volume    = {36},
  number    = {1},
  pages     = {161--171},
  month     = {apr},
  issn      = {13675788},
  abstract  = {Autonomous driving is one of the most interesting fields
		  of research, with a number of important applications, like
		  agricultural, military and, most significantly, safety.
		  This paper addresses the problem of designing a general
		  purpose path planner and its associated low level control
		  for autonomous vehicles operating in unknown environments.
		  Different kinds of inputs, like the results of obstacle
		  detection, ditch localization, lane detection, and global
		  path planning information are merged together using
		  potential fields to build a representation of the
		  environment in real-time; kinematically feasible
		  trajectories, based on vehicle dynamics, are generated on a
		  cost map. This approach demonstrated both flexibility and
		  reliability for vehicle driving in very different
		  environments, including extreme road conditions. This
		  controller was extensively tested during VIAC, the VisLab
		  Intercontinental Autonomous Challenge, a 13,000 km long
		  test for intelligent vehicle applications. The results,
		  collected during the development stage and the experiment
		  itself, are presented in the final part of this article.
		  {\textcopyright} 2012 Published by Elsevier Ltd. All rights
		  reserved.},
  doi       = {10.1016/j.arcontrol.2012.03.012},
  file      = {::},
  isbn      = {1367-5788},
  keywords  = {Autonomous navigation,Path planning},
  publisher = {Pergamon},
  url       = {https://www.sciencedirect.com/science/article/pii/S1367578812000132},
}

@Article{broggi2012a,
  author   = {Broggi, Alberto and Cerri, Pietro and Felisa, Mirko and Laghi, Maria Chiara and Mazzei, Luca and Porta, Pier Paolo},
  title    = {{The VisLab Intercontinental Autonomous Challenge: an extensive test for a platoon of intelligent vehicles}},
  journal  = {International Journal of Vehicle Autonomous Systems},
  year     = {2012},
  volume   = {10},
  number   = {3},
  pages    = {147},
  issn     = {1471-0226},
  abstract = {This paper presents the VisLab Intercontinental Autonomous
		  Challenge (VIAC), an autonomous vehicles test carried out
		  from Parma to Shanghai between July and October 2010 by the
		  VisLab team. The vehicle equipment is explained introducing
		  the sensing systems which were tested during the journey.
		  Trip details and the fi rst statistics are presented as
		  well.},
  doi      = {10.1504/IJVAS.2012.051250},
  url      = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.591.8385
		  http://www.inderscience.com/link.php?id=51250},
}

###InProceedings{ lee2017avm,
  title		= {AVM/LiDAR sensor based lane marking detection method for
		  automated driving on complex urban roads},
  author	= {Lee, Hyunsung and Kim, Seonwook and Park, Sungyoul and
		  Jeong, Yonghwan and Lee, Hojoon and Yi, Kyongsu},
  booktitle	= {Intelligent Vehicles Symposium (IV), 2017 IEEE},
  year		= {2017},
  organization	= {IEEE},
  pages		= {1434--1439}
}

@InProceedings{levinson2011,
  author        = {Levinson, Jesse and Askeland, Jake and Becker, Jan and Dolson, Jennifer and Held, David and Kammel, Soeren and Kolter, J. Zico and Langer, Dirk and Pink, Oliver and Pratt, Vaughan and Sokolsky, Michael and Stanek, Ganymed and Stavens, David and Teichman, Alex and Werling, Moritz and Thrun, Sebastian},
  title         = {{Towards fully autonomous driving: Systems and algorithms}},
  booktitle     = {IEEE Intelligent Vehicles Symposium, Proceedings},
  year          = {2011},
  pages         = {163--168},
  month         = {jun},
  publisher     = {IEEE},
  abstract      = {In order to achieve autonomous operation of a vehicle in
		  urban situations with unpredictable traffic, several
		  realtime systems must interoperate, including environment
		  perception, localization, planning, and control. In
		  addition, a robust vehicle platform with appropriate
		  sensors, computational hardware, networking, and software
		  infrastructure is essential.},
  archiveprefix = {arXiv},
  arxivid       = {1702.06827},
  doi           = {10.1109/IVS.2011.5940562},
  eprint        = {1702.06827},
  isbn          = {9781457708909},
  issn          = {1931-0587},
  pmid          = {11317986},
  url           = {http://ieeexplore.ieee.org/document/5940562/},
}

@PhdThesis{levinson2011a,
  author   = {Levinson, Jesse},
  title    = {{Automatic Laser Calibration, Mapping, and Localization for Autonomous Vehicles}},
  year     = {2011},
  abstract = {This dissertation presents several related algorithms that
		  enable$\backslash$nimportant capabilities for self-driving
		  vehicles. Using a rotating$\backslash$nmulti-beam laser
		  rangefinder to sense the world, our vehicle
		  scans$\backslash$nmillions of 3D points every second.
		  Calibrating these sensors plays$\backslash$na crucial role
		  in accurate perception, but manual calibration
		  is$\backslash$nunreasonably tedious, and generally
		  inaccurate. As an alternative,$\backslash$nwe present an
		  unsupervised algorithm for automatically
		  calibrating$\backslash$nboth the intrinsics and extrinsics
		  of the laser unit from only seconds$\backslash$nof driving
		  in an arbitrary and unknown environment. We show
		  that$\backslash$nthe results are not only vastly easier to
		  obtain than traditional$\backslash$ncalibration techniques,
		  they are also more accurate. A second
		  key$\backslash$nchallenge in autonomous navigation is
		  reliable localization in the$\backslash$nface of
		  uncertainty. Using our calibrated sensors, we obtain
		  high$\backslash$nresolution infrared reflectivity readings
		  of the world. From these,$\backslash$nwe build large-scale
		  self-consistent probabilistic laser maps
		  of$\backslash$nurban scenes, and show that we can reliably
		  localize a vehicle against$\backslash$nthese maps to within
		  centimeters, even in dynamic environments,
		  by$\backslash$nfusing noisy GPS and IMU readings with the
		  laser in realtime. We$\backslash$nalso present a
		  localization algorithm that was used in the
		  DARPA$\backslash$nUrban Challenge, which operated without a
		  prerecorded laser map,$\backslash$nand allowed our vehicle
		  to complete the entire six-hour course
		  without$\backslash$na single localization failure. Finally,
		  we present a collection of$\backslash$nalgorithms for the
		  mapping and detection of traffic lights in
		  realtime.$\backslash$nThese methods use a combination of
		  computer-vision techniques and$\backslash$nprobabilistic
		  approaches to incorporating uncertainty in order
		  to$\backslash$nallow our vehicle to reliably ascertain the
		  state of traffic-light-controlled$\backslash$nintersections.},
  pages    = {153},
  url      = {https://searchworks.stanford.edu/view/9275866
		  http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:AUTOMATIC+LASER+CALIBRATION+,+MAPPING+,+AND+LOCALIZATION+FOR+AUTONOMOUS+VEHICLES+A+DISSERTATION+SUBMITTED+TO+THE+DEPARTMENT+OF+COMPUTER+SCIENCE+AND},
}

@InProceedings{broggi2011,
  author    = {Broggi, Alberto and Buzzoni, Michele and Felisa, Mirko and Zani, Paolo},
  title     = {{Stereo obstacle detection in challenging environments: The VIAC experience}},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  year      = {2011},
  pages     = {1599--1604},
  month     = {sep},
  publisher = {IEEE},
  abstract  = {Obstacle detection by means of stereo-vision is a
		  fundamental task in computer vision, which has spurred a
		  lot of research over the years, especially in the field of
		  vehicular robotics. The information provided by this class
		  of algorithms is used both in driving assistance systems
		  and in autonomous vehicles, so the quality of the results
		  and the processing times become critical, as detection
		  failures or delays can have serious consequences. The
		  obstacle detection system presented in this paper has been
		  extensively tested during VIAC, the VisLab Intercontinental
		  Autonomous Challenge [1], [2], which has offered a unique
		  chance to face a number of different scenarios along the
		  roads of two continents, in a variety of conditions; data
		  collected during the expedition has also become a reference
		  benchmark for further algorithm improvements.},
  doi       = {10.1109/IROS.2011.6048211},
  isbn      = {9781612844541},
  issn      = {2153-0858},
  url       = {http://ieeexplore.ieee.org/document/6094535/},
}

@InProceedings{bertozzi2011,
  author    = {Bertozzi, Massimo and Broggi, Alberto and Cardarelli, Elena and Fedriga, Rean Isabella and Mazzei, Luca and Porta, Pier Paolo},
  title     = {{VIAC expedition toward autonomous mobility}},
  booktitle = {IEEE Robotics and Automation Magazine},
  year      = {2011},
  volume    = {18},
  number    = {3},
  pages     = {120--124},
  month     = {sep},
  doi       = {10.1109/MRA.2011.942490},
  issn      = {10709932},
  url       = {http://ieeexplore.ieee.org/document/6016589/},
}

###InProceedings{ frejlichowski2015application,
  title		= {Application of the Polar--Fourier Greyscale Descriptor to
		  the Automatic Traffic Sign Recognition},
  author	= {Frejlichowski, Dariusz},
  booktitle	= {International Conference Image Analysis and Recognition},
  year		= {2015},
  organization	= {Springer},
  pages		= {506--513}
}

@TechReport{frost&sullivan2010,
  author = {{Frost {\&} Sullivan}},
  title  = {{2009 European Consumers Desirability and Willingness to Pay for Advanced Safety and Driver Assistance Systems Market Research}},
  year   = {2010},
  url    = {https://store.frost.com/2009-european-consumers-desirability-and-willingness-to-pay-for-advanced-safety-and-driver-assistance-systems.html},
}

@Article{montemerlo2008,
  author    = {Montemerlo, Michael and Becker, Jan and Bhat, Suhrid and Dahlkamp, Hendrik and Dolgov, Dmitri and Ettinger, Scott and Haehnel, Dirk and Hilden, Tim and Hoffmann, Gabe and Huhnke, Burkhard and Johnston, Doug and Klumpp, Stefan and Langer, Dirk and Levandowski, Anthony and Levinson, Jesse and Marcil, Julien and Orenstein, David and Paefgen, Johannes and Penny, Isaac and Petrovskaya, Anna and Pflueger, Mike and Stanek, Ganymed and Stavens, David and Vogt, Antone and Thrun, Sebastian},
  title     = {{Junior: the Stanford entry in the Urban Challenge}},
  journal   = {Journal of Field Robotics},
  year      = {2008},
  volume    = {25},
  number    = {9},
  pages     = {569--597},
  month     = {sep},
  issn      = {15564959},
  abstract  = {This article presents the architecture of Junior, a
		  robotic vehicle capable of navigating ur-ban environments
		  autonomously. In doing so, the vehicle is able to select
		  its own routes, perceive and interact with other traffic,
		  and execute various urban driving skills including lane
		  changes, U-turns, parking, and merging into moving traffic.
		  The vehicle successfully finished and won second place in
		  the DARPA Urban Challenge, a robot competition or-ganized
		  by the U.},
  doi       = {10.1002/rob.20258},
  file      = {::},
  publisher = {Wiley-Blackwell},
  url       = {http://doi.wiley.com/10.1002/rob.20258
		  https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20258{\%}0Ahttp://dx.doi.org/10.1002/rob.20258},
}

###Article{	  timofte2014multi,
  title		= {Multi-view traffic sign detection, recognition, and 3d
		  localisation},
  author	= {Timofte, Radu and Zimmermann, Karel and Van Gool, Luc},
  journal	= {Machine vision and applications},
  year		= {2014},
  number	= {3},
  pages		= {633--647},
  volume	= {25},
  publisher	= {Springer}
}

@TechReport{urmson2007,
  author = {Urmson, Chris},
  title  = {{DARPA Urban Challenge Final Report for Tartan Racing}},
  year   = {2007},
  file   = {::},
  url    = {https://pdfs.semanticscholar.org/3116/38e299acef3cbd3423649b77ef73c2a94fc1.pdf},
}

###Article{	  ozgunalp2017multiple,
  title		= {Multiple lane detection algorithm based on novel dense
		  vanishing point estimation},
  author	= {Ozgunalp, Umar and Fan, Rui and Ai, Xiao and Dahnoun,
		  Naim},
  journal	= {IEEE Transactions on Intelligent Transportation Systems},
  year		= {2017},
  number	= {3},
  pages		= {621--632},
  volume	= {18},
  publisher	= {IEEE}
}

@Book{ozguner2007,
  title           = {{Autonomous Ground Vehicles}},
  publisher       = {Artech House},
  year            = {2007},
  author          = {Ozguner, Umit and Acarman, Tankut and Redmill, Keith},
  isbn            = {978-1608071920},
  abstract        = {In the near future, we will witness vehicles with the
		  ability to provide drivers with several advanced safety and
		  performance assistance features. Autonomous technology in
		  ground vehicles will afford us capabilities like
		  intersection collision warning, lane change warning, backup
		  parking, parallel parking aids, and bus precision parking.
		  Providing you with a practical understanding of this
		  technology area, this innovative resource focuses on basic
		  autonomous control and feedback for stopping and steering
		  ground vehicles.Covering sensors, estimation, and sensor
		  fusion to percept the vehicle motion and surrounding
		  objects, this unique book explains the key aspects that
		  makes autonomous vehicle behavior possible. Moreover, you
		  find detailed examples of fusion and Kalman filtering. From
		  maps, path planning, and obstacle avoidance scenarios...to
		  cooperative mobility among autonomous vehicles,
		  vehicle-to-vehicle communication, and
		  vehicle-to-infrastructure communication, this
		  forward-looking book presents the most critical topics in
		  the field today.},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  pages           = {292},
  url             = {http://us.artechhouse.com/Autonomous-Ground-Vehicles-P1476.aspx},
}

@Article{thrun2006,
  author   = {Thrun, Sebastian and Montemerlo, Mike and Dahlkamp, Hendrik and Et, Al.},
  title    = {{The robot that won the DARPA Grand Challenge.}},
  journal  = {Journal of Field Robotics},
  year     = {2006},
  volume   = {23-9},
  number   = {9},
  pages    = {661--692},
  issn     = {1556-4967},
  abstract = {This article describes the robot Stanley, which won the
		  2005 DARPA Grand Challenge. Stanley was developed for
		  high-speed desert driving without manual intervention. The
		  robot's software system relied predominately on
		  state-of-the-art artificial intelligence technologies, such
		  as machine learning and probabilistic reasoning. This paper
		  describes the major components of this architecture, and
		  discusses the results of the Grand Challenge race.},
  doi      = {10.1002/rob.20147},
  file     = {::},
  url      = {www.interscience.wiley.com
		  http://doi.wiley.com/10.1002/rob.20147},
}

@InProceedings{tatschke2006,
  author    = {Tatschke, Thomas and Park, S. B. and Amditis, Angelos and Polychronopoulos, Aristomenis and Scheunert, Ullrich and Aycard, Olivier},
  title     = {{ProFusion2 - Towards a modular, robust and reliable fusion architecture for automotive environment perception}},
  booktitle = {Advanced Microsystems for Automotive Applications 2006},
  year      = {2006},
  pages     = {451--469},
  address   = {Berlin/Heidelberg},
  publisher = {Springer-Verlag},
  abstract  = {This publication focuses on a modular architecture for
		  sensor data fusion regarding to research work of common
		  interest related to sensors and sensor data fusion. This
		  architecture will be based on an extended environment model
		  and representation, consisting of a set of common data
		  structures for sensor, object and situation refinement data
		  and algorithms as well as the corresponding models. The aim
		  of such research is to contribute to a measurable
		  enhancement of the output performance provided by
		  multi-sensor systems in terms of actual availability,
		  reliability, accuracy and precision of the perception
		  results. In this connection, investigations towards fusion
		  concepts and paradigms, such as
		  {\^{a}}€˜redundant{\^{a}}€™ and
		  {\^{a}}€˜complementary{\^{a}}€™, as well as
		  {\^{a}}€˜early{\^{a}}€™ and track-based sensor data
		  fusion approaches, are conducted, in order to significantly
		  enhance the overall performance of the perception system.},
  doi       = {10.1007/3-540-33410-6_32},
  isbn      = {3540334092},
  keywords  = {Early fusion,Environment modelling,Environment perception,Fusion feedback,Fusion framework,Grid-based fusion,Multi level fusion,ProFusion2,Sensor data fusion,Track-based fusion},
  url       = {http://link.springer.com/10.1007/3-540-33410-6{\_}32
		  http://lig-membres.imag.fr/aycard/html/Publications/2006/Tatschke06.pdf},
}

@InProceedings{polychronopoulos2006,
  author    = {Polychronopoulos, A. and Amditis, A. and Scheunert, U. and Tatschke, T.},
  title     = {{Revisiting JDL model for automotive safety applications: The PF2 functional model}},
  booktitle = {2006 9th International Conference on Information Fusion, FUSION},
  year      = {2006},
  pages     = {1--7},
  month     = {jul},
  publisher = {IEEE},
  abstract  = {The question raised in this paper, for the first time, is
		  how the JDL model can be applied in multi-sensor automotive
		  safety systems, since new sensors are integrated on-board,
		  while new functions support the driver, intervene and
		  control the vehicle. The paper proposes a hybrid
		  hierarchical structure and develops a suitable functional
		  model, namely the ProFusion2 (PF2) model; PF2 serves the
		  broad automotive sensor data fusion community as a
		  conceptual framework of common understanding and it
		  provides recommendations and guidelines for implementation
		  of fusion systems. Reference implementations are given as
		  complete examples from the major automotive research
		  initiative in Europe (PReVENT project).},
  doi       = {10.1109/ICIF.2006.301681},
  file      = {:E$\backslash$:/publicaciones/2018{\_}review{\_}sensores{\_}percepcion{\_}ADAS/doc/polychronopoulos2006.pdf:pdf},
  isbn      = {1424409535},
  issn      = {1-4244-0953-5},
  keywords  = {Environment model,JDL model,Object refinement,PF2 functional model,Situation refinement},
  url       = {http://ieeexplore.ieee.org/document/4085967/},
}

###Article{	  tan2016weakly,
  title		= {Weakly supervised metric learning for traffic sign
		  recognition in a LIDAR-equipped vehicle},
  author	= {Tan, Min and Wang, Baoyuan and Wu, Zhaohui and Wang,
		  Jingdong and Pan, Gang},
  journal	= {IEEE Transactions on Intelligent Transportation Systems},
  year		= {2016},
  number	= {5},
  pages		= {1415--1427},
  volume	= {17},
  publisher	= {IEEE}
}

@Misc{tartanracing2005,
  author          = {{Tartan Racing}},
  title           = {{Boss at a glance}},
  year            = {2005},
  file            = {::},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  url             = {http://www.tartanracing.org/press/boss-glance.pdf},
}

###InProceedings{ nie2012camera,
  title		= {Camera and lidar fusion for road intersection detection},
  author	= {Nie, Yiming and Chen, Qingyang and Chen, Tongtong and Sun,
		  Zhenping and Dai, Bin},
  booktitle	= {Electrical \& Electronics Engineering (EEESYM), 2012 IEEE
		  Symposium on},
  year		= {2012},
  organization	= {IEEE},
  pages		= {273--276}
}

@Article{nordin2004,
  author          = {Nordin, Daniel},
  title           = {{Optical Frequency Modulated Continuous Wave (FMCW) Range and Velocity Measurements}},
  journal         = {Thesis},
  year            = {2004},
  pages           = {110},
  file            = {::},
  institution     = {Lulea University of Technology, Sweden},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  url             = {https://www.diva-portal.org/smash/get/diva2:999065/FULLTEXT01.pdf
		  http://epubl.luth.se/1402-1544/2004/43/LTU-DT-0443-SE.pdf},
}

###Article{	  liu2015,
  abstract	= {With the continuing growth of highway construction and
		  vehicle use expansion all over the world, highway vehicle
		  traffic rule violation (TRV) detection has become more and
		  more important so as to avoid traffic accidents and
		  injuries in intelligent transportation systems (ITS) and
		  vehicular ad hoc networks (VANETs). Since very few works
		  have contributed to solve the TRV detection problem by
		  moving vehicle measurements and surveillance devices, this
		  paper develops a novel parallel ultrasonic sensor system
		  that can be used to identify the TRV behavior of a host
		  vehicle in real-time. Then a two-dimensional state method
		  is proposed, utilizing the spacial state and time
		  sequential states from the data of two parallel ultrasonic
		  sensors to detect and count the highway vehicle violations.
		  Finally, the theoretical TRV identification probability is
		  analyzed, and actual experiments are conducted on different
		  highway segments with various driving speeds, which
		  indicates that the identification accuracy of the proposed
		  method can reach about 90.97{\%}.},
  author	= {Liu, Jun and Han, Jiuqiang and Lv, Hongqiang and Li,
		  Bing},
  doi		= {10.3390/s150409000},
  file		= {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - An
		  Ultrasonic Sensor System Based on a Two-Dimensional State
		  Method for Highway Vehicle Violation Detection
		  Applicat.pdf:pdf},
  issn		= {14248220},
  journal	= {Sensors (Switzerland)},
  keywords	= {Highway vehicle traffic rule violation
		  detection,Intelligent transportation
		  systems,Two-dimensional state method,Ultrasonic sensor
		  system},
  month		= {apr},
  number	= {4},
  pages		= {9000--9021},
  pmid		= {1603305},
  publisher	= {Multidisciplinary Digital Publishing Institute},
  title		= {{An ultrasonic sensor system based on a two-dimensional
		  state method for highway vehicle violation detection
		  applications}},
  url		= {http://www.mdpi.com/1424-8220/15/4/9000/},
  volume	= {15},
  year		= {2015}
}

###Article{	  liu2015,
  title		= {An ultrasonic sensor system based on a two-dimensional
		  state method for highway vehicle violation detection
		  applications},
  author	= {Liu, Jun and Han, Jiuqiang and Lv, Hongqiang and Li,
		  Bing},
  journal	= {Sensors (Switzerland)},
  year		= {2015},
  month		= {apr},
  number	= {4},
  pages		= {9000--9021},
  volume	= {15},
  abstract	= {With the continuing growth of highway construction and
		  vehicle use expansion all over the world, highway vehicle
		  traffic rule violation (TRV) detection has become more and
		  more important so as to avoid traffic accidents and
		  injuries in intelligent transportation systems (ITS) and
		  vehicular ad hoc networks (VANETs). Since very few works
		  have contributed to solve the TRV detection problem by
		  moving vehicle measurements and surveillance devices, this
		  paper develops a novel parallel ultrasonic sensor system
		  that can be used to identify the TRV behavior of a host
		  vehicle in real-time. Then a two-dimensional state method
		  is proposed, utilizing the spacial state and time
		  sequential states from the data of two parallel ultrasonic
		  sensors to detect and count the highway vehicle violations.
		  Finally, the theoretical TRV identification probability is
		  analyzed, and actual experiments are conducted on different
		  highway segments with various driving speeds, which
		  indicates that the identification accuracy of the proposed
		  method can reach about 90.97{\%}.},
  doi		= {10.3390/s150409000},
  file		= {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - An
		  Ultrasonic Sensor System Based on a Two-Dimensional State
		  Method for Highway Vehicle Violation Detection
		  Applicat.pdf:pdf},
  issn		= {14248220},
  keywords	= {Highway vehicle traffic rule violation
		  detection,Intelligent transportation
		  systems,Two-dimensional state method,Ultrasonic sensor
		  system},
  pmid		= {1603305},
  publisher	= {Multidisciplinary Digital Publishing Institute},
  url		= {http://www.mdpi.com/1424-8220/15/4/9000/}
}

@Article{llinas2000,
  author   = {Llinas, James and Bowman, Christopher and Rogova, Galina and Steinberg, Alan},
  title    = {{Revisiting the JDL data fusion model II}},
  journal  = {Space and Naval Warfare Systems Command},
  year     = {2004},
  volume   = {1},
  number   = {7},
  pages    = {1--14},
  abstract = {This paper suggests refinements and extensions of the JDL
		  Data Fusion Model, the standard process model used for a
		  multiplicity of community purposes. However, this Model has
		  not been reviewed in accordance with (a) the dynamics of
		  world events and (b) the changes, discoveries, and new
		  methods in both the data fusion research and development
		  community and related IT technologies. This paper suggests
		  ways to revise and extend this important model. Proposals
		  are made regarding (a) improvements in the understanding of
		  internal processing within a fusion node and (b) extending
		  the model to include (1) remarks on issues related to
		  quality control, reliability, and consistency in DF
		  processing, (2) assertions about the need for co-processing
		  of abductive/ inductive and deductive inferencing
		  processes, (3) remarks about the need for and exploitation
		  of an onto logicallybased approach to DF process design,
		  and (4) extensions to account for the case of Distributed
		  Data Fusion (DDF).},
  isbn     = {917056115X},
  keywords = {data fusion,fusion,information fusion,jdl model},
  url      = {http://oai.dtic.mil/oai/oai?verb=getRecord{\&}metadataPrefix=html{\&}identifier=ADA525721},
}

@PhdThesis{montemerlo2003a,
  author   = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
  title    = {{FastSLAM: A factored solution to the simultaneous localization and mapping problem}},
  school   = {Carnegie Mellon University},
  year     = {2003},
  abstract = {The ability to simultaneously localize a robot and
		  accurately map its surroundings is considered by many to be
		  a key prerequisite of truly autonomous robots. However, few
		  approaches to this problem scale up to handle the very
		  large number of landmarks present in real environments.
		  Kalman filter-based algorithms, for example, require time
		  quadratic in the number of landmarks to incorporate each
		  sensor observation. This paper presents FastSLAM, an
		  algorithm that recursively estimates the full posterior
		  distribution over robot pose and landmark locations, yet
		  scales logarithmically with the number of landmarks in the
		  map. This algorithm is based on a factorization of the
		  posterior into a product of conditional landmark
		  distributions and a distribution over robot paths. The
		  algorithm has been run successfully on as many as 50,000
		  landmarks, environments far beyond the reach of previous
		  approaches. Experimental results demonstrate the advantages
		  and limitations of the FastSLAM algorithm on both simulated
		  and real-world data.},
  annote   = {NULL},
  doi      = {10.1.1.16.2153},
  file     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Montemerlo et al. - 2003 -
		  FastSLAM A factored solution to the simultaneous
		  localization and mapping problem.pdf:pdf},
  isbn     = {0262511290},
  url      = {https://www.ri.cmu.edu/pub{\_}files/pub4/montemerlo{\_}michael{\_}2003{\_}1/montemerlo{\_}michael{\_}2003{\_}1.pdf
		  http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:FastSLAM+:+A+Factored+Solution+to+the+Simultaneous+Localization+and+Mapping+Problem{\#}0},
}

@Article{gregor2002,
  author   = {Gregor, Rudolf and L{\"{u}}tzeler, M. and Pellkofer, M. and Siedersberger, K. H. and Dickmanns, Ernst Dieter},
  title    = {{EMS-Vision: A Perceptual System for Autonomous Vehicles}},
  journal  = {IEEE Transactions on Intelligent Transportation Systems},
  year     = {2002},
  volume   = {3},
  number   = {1},
  pages    = {48--59},
  month    = {mar},
  issn     = {15249050},
  abstract = {The paper gives a survey on the new Expectation-based
		  Multifocal$\backslash$nSaccadic vision (EMS-vision) system
		  for autonomous vehicle guidance$\backslash$ndeveloped at
		  the Universitat der Bundeswehr Munchen (UBM). EMS-Vision
		  is$\backslash$nthe third generation dynamic vision system
		  following the 4-D approach.$\backslash$nIts core element is
		  a new camera arrangement, mounted on a
		  high$\backslash$nbandwidth pan-tilt head for active gaze
		  control. Central knowledge$\backslash$nrepresentation and a
		  hierarchical system architecture allow
		  efficient$\backslash$nactivation and control of behavioral
		  capabilities for perception and$\backslash$naction. The
		  system has been implemented on commercial
		  off-the-shelf$\backslash$n(COTS) hardware components in
		  both UBM test vehicles VaMoRs and VaMP$\backslash$nResults
		  from autonomous turnoff maneuvers, performed on army
		  proving$\backslash$ngrounds, are discussed},
  doi      = {10.1109/6979.994795},
  isbn     = {0-7803-6363-9},
  keywords = {Active vision system,Autonomous vehicles,Control of perception and action,Dynamic machine vision,Knowledge representation,System architecture},
  url      = {http://ieeexplore.ieee.org/document/994795/},
}

###InProceedings{ maqueda2018,
  abstract	= {Event cameras are bio-inspired vision sensors that
		  naturally capture the dynamics of a scene, filtering out
		  redundant information. This paper presents a deep neural
		  network approach that unlocks the potential of event
		  cameras on a challenging motion-estimation task: prediction
		  of a vehicle's steering angle. To make the best out of this
		  sensor-algorithm combination, we adapt state-of-the-art
		  convolutional architectures to the output of event sensors
		  and extensively evaluate the performance of our approach on
		  a publicly available large scale event-camera dataset
		  ({\~{}}1000 km). We present qualitative and quantitative
		  explanations of why event cameras allow robust steering
		  prediction even in cases where traditional cameras fail,
		  e.g. challenging illumination conditions and fast motion.
		  Finally, we demonstrate the advantages of leveraging
		  transfer learning from traditional to event-based vision,
		  and show that our approach outperforms state-of-the-art
		  algorithms based on standard cameras.},
  address	= {Salt Lake City},
  archiveprefix	= {arXiv},
  arxivid	= {1804.01310},
  author	= {Maqueda, Ana I and Loquercio, Antonio and Gallego,
		  Guillermo and Garcia, Narciso and Scaramuzza, Davide},
  booktitle	= {IEEE Conference on Computer Vision and Pattern Recognition
		  (CVPR)},
  doi		= {10.1109/CVPR.2018.00568},
  eprint	= {1804.01310},
  file		= {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/Maqueda et al. - 2018 -
		  Event-based Vision meets Deep Learning on Steering
		  Prediction for Self-driving Cars.pdf:pdf},
  keywords	= {Computer Vision,Machine Learning},
  mendeley-groups={Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
  title		= {{Event-based Vision meets Deep Learning on Steering
		  Prediction for Self-driving Cars}},
  url		= {http://rpg.ifi.uzh.ch/docs/CVPR18{\_}Maqueda.pdf
		  http://arxiv.org/abs/1804.01310},
  year		= {2018}
}

@Article{massot1999,
  author          = {Massot, M.-H.a and Allouche, J.-F.b and B{\'{e}}n{\'{e}}jam, E.c and Parent, M.d},
  title           = {{Praxitele: Preliminary results from the Saint-Quentin station-car experiment}},
  journal         = {Transportation Research Record},
  year            = {1999},
  volume          = {1666},
  number          = {1666},
  pages           = {125--132},
  month           = {jan},
  issn            = {03611981},
  abstract        = {The Praxitele system is the first large-scale,
		  operational, public individual-transportation system - or
		  station-car system - using self-service electric vehicles.
		  It was developed in France by a consortium of industrial
		  companies and research institutes, formed in 1993. Its
		  operation started in the city of Saint-Quentin-en-Yvelines,
		  a high-tech center near Paris, at the end of 1997 with 50
		  electric vehicles from Renault. At midterm in the
		  experiment, close to 500 participants were using the
		  system. This is the first report on the experiment, which
		  continued until the end of 1998. Preliminary conclusions
		  show that users have expressed a high level of satisfaction
		  and a desire to expand the system. However, no conclusion
		  can be drawn on yet the economics of such a system, which
		  remains expensive and underutilized.},
  doi             = {10.3141/1666-15},
  keywords        = {Economic and social effects,Electric vehicles,Mass transportation,Self service electric vehicles,Station-car syste},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  publisher       = {Transportation Research Board of the National Academies},
  url             = {http://trrjournalonline.trb.org/doi/10.3141/1666-15
		  http://www.scopus.com/inward/record.url?eid=2-s2.0-0033285652{\&}partnerID=40{\&}md5=2c0ce78034a4e8db551c9b336d44bd1e},
}

@Book{broggi1999,
  title     = {{Automatic Vehicle Guidance : the Experience of the ARGO Autonomous Vehicle}},
  publisher = {World Scientific},
  year      = {1999},
  author    = {Broggi, Alberto. and Bertozzi, Massimo and Fascioli, Alessandra and Conte, Gianni},
  isbn      = {9810237200},
  booktitle = {World Scientific Co., Singapore},
  pages     = {242},
  url       = {http://ftp.utcluj.ro/pub/docs/imaging/Autonomous{\_}driving/Articole
		  sortate/Lazar
		  Mircea/AutoDriving2/parma/www.ce.unipr.it/people/broggi/publications/argo.pdf},
}

@InProceedings{broggi1998,
  author    = {Broggi, Alberto and Bertozzi, Massimo and Fascioli, Alessandra and Conte, Gianni},
  title     = {{the Experience of the ARGO Autonomous Vehicle}},
  booktitle = {Enhanced and Synthetic Vision},
  year      = {1998},
  editor    = {Verly, Jacques G.},
  volume    = {3364},
  pages     = {218--229},
  month     = {jul},
  publisher = {International Society for Optics and Photonics},
  abstract  = {This paper presents and discusses the first results
		  obtained by the GOLD (Generic Obstacle and Lane Detection)
		  system as an automatic driver of ARGO. ARGO is a Lancia
		  Thema passenger car equipped with a vision-based system
		  that allows to extract road and environmental information
		  from the acquired scene. By means of stereo vision,
		  obstacles on the road are detected and localized, while the
		  processing of a single monocular image allows to extract
		  the road geometry in front of the vehicle. The generality
		  of the underlying approach allows to detect generic
		  obstacles (without constraints on shape, color, or
		  symmetry) and to detect lane markings even in dark and in
		  strong shadow conditions. The hardware system consists of a
		  PC Pentium 200 Mhz with MMX technology and a frame-grabber
		  board able to acquire 3 b/w images simultaneously; the
		  result of the processing (position of obstacles and
		  geometry of the road) is used to drive an actuator on the
		  steering wheel, while debug information are presented to
		  the user on an on-board monitor and a led-based control
		  panel.},
  doi       = {10.1117/12.317473},
  isbn      = {9810237200},
  issn      = {0277786X},
  keywords  = {algorithms and the architectures,argo is the experimental,autonomous vehicle,autonomous vehicle developed at,computer vision,conducted over the last,dell,few years on the,for vision based automatic,informazione of the,it integrates the main,italy,lane detection,obstacle detection,results of the research,road vehicles guidance,the dipartimento di ingegneria,university of parma},
  url       = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=947967
		  http://www.worldscientific.com/doi/pdf/10.1142/9789812814937{\_}bmatter},
}

@Misc{thorpe1997,
  author    = {Thorpe, Chuck and Jochem, Todd and Pomerleau, Dean},
  title     = {{1997 Automated highway free agent demonstration}},
  year      = {1997},
  abstract  = {In August of 1997, The US National Automated Highway
		  System Consortium (NAHSC) presented a proof of technical
		  feasibility demonstration of automated driving. The 97 Demo
		  took place on car-pool lanes on I-15, in San Diego,
		  California. Members of the Consortium demonstrated many
		  different functions: Vision-based road following, Following
		  magnetic nails, Following a radar reflective strip,
		  Radar-based headway maintenance, Ladar-based headway
		  maintenance, Evolutionary systems, Close vehicle following
		  (platooning), Cooperative maneuvering, Obstacle detection
		  and avoidance, Mixed automated and manual driving, Mixed
		  automated cars and buses, and Semi-automated maintenance.
		  CMU led the effort to build one of the seven demonstration
		  scenarios, the Free Agent Demonstration (FAD). The FAD
		  involved two fully automated cars, one partially automated
		  car, and two fully automated city buses. The scenario
		  demonstrates lane entry, speed and headway control, lane
		  following, lane changing, obstacle detection, and
		  cooperative maneuvers. This paper describes the free agent
		  demonstration itself, the technology that made the
		  demonstration possible, and the future work to analyze the
		  feasibility of turning the demonstration system into a
		  practical prototype.},
  booktitle = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
  doi       = {10.1109/ITSC.1997.660524},
  isbn      = {0-7803-4269-0},
  pages     = {496--501},
  publisher = {IEEE},
  url       = {http://ieeexplore.ieee.org/document/660524/
		  http://www.scopus.com/inward/record.url?eid=2-s2.0-0031366442{\&}partnerID=tZOtx3y1},
}

@InProceedings{parent1993,
  author          = {Parent, M. and Daviet, P.},
  title           = {{Automatic Driving For Small Public Urban Vehicles}},
  booktitle       = {Proceedings of the Intelligent Vehicles '93 Symposium},
  year            = {1993},
  pages           = {402--407},
  publisher       = {IEEE},
  doi             = {10.1109/IVS.1993.697360},
  isbn            = {0-7803-1370-4},
  mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
  url             = {http://ieeexplore.ieee.org/document/697360/},
}

###Article{	  depontemuller2017,
  abstract	= {Future driver assistance systems will rely on accurate,
		  reliable and continuous knowledge on the position of other
		  road participants, including pedestrians, bicycles and
		  other vehicles. The usual approach to tackle this
		  requirement is to use on-board ranging sensors inside the
		  vehicle. Radar, laser scanners or vision-based systems are
		  able to detect objects in their line-of-sight. In contrast
		  to these non-cooperative ranging sensors, cooperative
		  approaches follow a strategy in which other road
		  participants actively support the estimation of the
		  relative position. The limitations of on-board ranging
		  sensors regarding their detection range and angle of view
		  and the facility of blockage can be approached by using a
		  cooperative approach based on vehicle-to-vehicle
		  communication. The fusion of both, cooperative and
		  non-cooperative strategies, seems to offer the largest
		  benefits regarding accuracy, availability and robustness.
		  This survey offers the reader a comprehensive review on
		  different techniques for vehicle relative positioning. The
		  reader will learn the important performance indicators when
		  it comes to relative positioning of vehicles, the different
		  technologies that are both commercially available and
		  currently under research, their expected performance and
		  their intrinsic limitations. Moreover, the latest research
		  in the area of vision-based systems for vehicle detection,
		  as well as the latest work on GNSS-based vehicle
		  localization and vehicular communication for relative
		  positioning of vehicles, are reviewed. The survey also
		  includes the research work on the fusion of cooperative and
		  non-cooperative approaches to increase the reliability and
		  the availability.},
  author	= {{de Ponte M{\"{u}}ller}, Fabian and Fabian},
  doi		= {10.3390/s17020271},
  file		= {:D$\backslash$:/Users/109123/AppData/Local/Mendeley
		  Ltd./Mendeley Desktop/Downloaded/de Ponte M{\"{u}}ller,
		  Fabian - 2017 - Survey on Ranging Sensors and Cooperative
		  Techniques for Relative Positioning of Vehicles.pdf:pdf},
  issn		= {1424-8220},
  journal	= {Sensors},
  keywords	= {GNSS,cooperative,laser scanner,localization,relative
		  positioning,to,vehicle,vehicle sensors},
  mendeley-groups={Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
  month		= {jan},
  number	= {2},
  pages		= {271},
  publisher	= {Multidisciplinary Digital Publishing Institute},
  title		= {{Survey on Ranging Sensors and Cooperative Techniques for
		  Relative Positioning of Vehicles}},
  url		= {http://www.mdpi.com/1424-8220/17/2/271},
  volume	= {17},
  year		= {2017}
}

@Article{dickmanns1987,
  author    = {Dickmanns, E.D. and Zapp, A.},
  title     = {{Autonomous High Speed Road Vehicle Guidance by Computer Vision 1}},
  journal   = {IFAC Proceedings Volumes},
  year      = {1987},
  volume    = {20},
  number    = {5},
  pages     = {221--226},
  month     = {jul},
  issn      = {14746670},
  abstract  = {A visual feedback control system has been developed which
		  is able to guide road vehicles on well structured roads at
		  high speeds. The road boundary markings are tracked by a
		  multiprocessor image processing system using contour
		  correlation and curvature models together with the laws of
		  perspective projection. Feature position data are the input
		  into Kalman filters to estimate both the vehicle state
		  vector relative to the driving lane and road curvature
		  parameters. Velocity is measured conventionally.
		  Longitudinal control by throttle and braking is geared to
		  lateral acceleration due to road curvature; lateral control
		  has an anticipatory feed forward and a compensatory
		  feedback component. The control system has been tested with
		  a CCD TV-camera and image sequence processing hardware in a
		  real time simulation loop and with our experimental
		  vehicle, a 5 ton-van equipped with sensors, onboard
		  computers and actuators for autonomous driving},
  doi       = {10.1016/S1474-6670(17)55320-3},
  file      = {::},
  publisher = {Elsevier},
  url       = {https://www.sciencedirect.com/science/article/pii/S1474667017553203
		  http://linkinghub.elsevier.com/retrieve/pii/S1474667017553203},
}

@inproceedings{Brookner2016,
author = {Brookner, Eli},
booktitle = {IEEE International Symposium on Phased Array Systems and Technology (PAST)},
doi = {10.1109/ARRAY.2016.7832577},
isbn = {978-1-5090-1447-7},
keywords = {aesa,antenna,cloaking,conformal,ebg,esa,fractals,invisibility,magnetic dielectric,magnetic ground plane,metamaterial,phased arrays,radar,stealth,waim},
mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
month = {oct},
pages = {1--9},
publisher = {IEEE},
title = {{Metamaterial Advances for Radar and Communications}},
url = {http://ieeexplore.ieee.org/document/7832577/},
year = {2016}
}
@article{Sleasman2017,
abstract = {We investigate the use of a dynamic metasurface as the transmitting antenna for a synthetic aperture radar (SAR) imaging system. The dynamic metasurface consists of a one-dimensional microstrip waveguide with complementary electric resonator (cELC) elements patterned into the upper conductor. Integrated into each of the cELCs are two diodes that can be used to shift each cELC resonance out of band with an applied voltage. The aperture is designed to operate at K band frequencies (17.5 to 20.3 GHz), with a bandwidth of 2.8 GHz. We experimentally demonstrate imaging with a fabricated metasurface aperture using existing SAR modalities, showing image quality comparable to traditional antennas. The agility of this aperture allows it to operate in spotlight and stripmap SAR modes, as well as in a third modality inspired by computational imaging strategies. We describe its operation in detail, demonstrate high-quality imaging in both 2D and 3D, and examine various trade-offs governing the integration of dynamic metasurfaces in future SAR imaging platforms.},
archivePrefix = {arXiv},
arxivId = {1703.00072},
author = {Sleasman, Timothy and Boyarsky, Michael and Pulido-Mancera, Laura and Fromenteze, Thomas and Imani, Mohammadreza F. and Reynolds, Matthew S. and Smith, David R.},
doi = {10.1109/TAP.2017.2758797},
eprint = {1703.00072},
file = {::},
isbn = {0018-926X},
issn = {0018926X},
journal = {IEEE Transactions on Antennas and Propagation},
keywords = {Beam steering,microwave imaging,radar imaging,reconfigurable antennas,synthetic aperture radar (SAR),waveguide antennas},
mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
month = {feb},
number = {12},
pages = {6864--6877},
title = {{Experimental Synthetic Aperture Radar with Dynamic Metasurfaces}},
url = {http://arxiv.org/abs/1703.00072 http://dx.doi.org/10.1109/TAP.2017.2758797},
volume = {65},
year = {2017}
}
@article{Thomas2013,
abstract = {This research applies a recently developed model of accident causation, developed to investigate industrial accidents, to a specially gathered sample of 997 crashes investigated in-depth in 6 countries. Based on the work of Hollnagel the model considers a collision to be a consequence of a breakdown in the interaction between road users, vehicles and the organisation of the traffic environment. 54{\%} of road users experienced interpretation errors while 44{\%} made observation errors and 37{\%} planning errors. In contrast to other studies only 11{\%} of drivers were identified as distracted and 8{\%} inattentive. There was remarkably little variation in these errors between the main road user types. The application of the model to future in-depth crash studies offers the opportunity to identify new measures to improve safety and to mitigate the social impact of collisions. Examples given include the potential value of co-driver advisory technologies to reduce observation errors and predictive technologies to avoid conflicting interactions between road users.},
author = {Thomas, Pete and Morris, Andrew and Talbot, Rachel and Fagerlind, Helen},
file = {::},
issn = {1943-2461},
journal = {Annals of advances in automotive medicine. Association for the Advancement of Automotive Medicine. Scientific Conference},
mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
pages = {13--22},
pmid = {24406942},
title = {{Identifying the causes of road crashes in Europe.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24406942{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3861814},
volume = {57},
year = {2013}
}
@misc{NTSB2017,
author = {NTSB},
booktitle = {National Transportation Safety Board Office of Public Affairs},
mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
pages = {702},
title = {{NTSB Opens Docket on Tesla Crash}},
url = {https://www.ntsb.gov/news/press-releases/Pages/PR20170619.aspx},
urldate = {2018-10-16},
year = {2017}
}
@techreport{NTSB2018,
abstract = {About 9:58 p.m., on Sunday, March 18, 2018, an Uber Technologies, Inc. test vehicle, based on a modified 2017 Volvo XC90 and operating with a self-driving system in computer control mode, struck a pedestrian on northbound Mill Avenue, in Tempe, Maricopa County, Arizona. The Uber test vehicle was occupied by one vehicle operator, a 44-year-old female. No passengers were in the vehicle.},
author = {NTSB},
file = {::},
institution = {National Transportation Safety Board},
mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
number = {May},
pages = {4},
title = {{PRELIMINARY REPORT - HIGHWAY - HWY18MH010}},
url = {https://www.ntsb.gov/investigations/AccidentReports/Reports/HWY18MH010-prelim.pdf},
year = {2018}
}
@article{Pendleton2017,
abstract = {Autonomous vehicles are expected to play a key role in the future of urban transportation systems, as they offer potential for additional safety, increased productivity, greater accessibility, better road efﬁciency, and positive impact on the environment. Research in autonomous systems has seen dramatic advances in recent years, due to the increases in available computing power and reduced cost in sensing and computing technologies, resulting in maturing technological readiness level of fully autonomous vehicles. The objective of this paper is to provide a general overview of the recent developments in the realm of autonomous vehicle software systems. Fundamental components of autonomous vehicle software are reviewed, and recent developments in each area are discussed.},
author = {Pendleton, Scott and Andersen, Hans and Du, Xinxin and Shen, Xiaotong and Meghjani, Malika and Eng, You and Rus, Daniela and Ang, Marcelo},
doi = {10.3390/machines5010006},
file = {::},
isbn = {01411926 (ISSN)},
issn = {2075-1702},
journal = {Machines},
keywords = {automotive control,autonomous vehicles,localization,multi,perception,planning,vehicle cooperation},
mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
month = {feb},
number = {1},
pages = {6},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Perception, Planning, Control, and Coordination for Autonomous Vehicles}},
url = {http://www.mdpi.com/2075-1702/5/1/6},
volume = {5},
year = {2017}
}
@phdthesis{Yenkanchi2016,
author = {Yenkanchi, Shashibushan},
file = {::},
mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
pages = {68},
school = {University of Windsor},
title = {{Multi Sensor Data Fusion for Autonomous Vehicles}},
url = {https://scholar.uwindsor.ca/etd/5680},
year = {2016}
}
@article{Ziebinski2016a,
abstract = {A performance analysis of a modified self-regulated self-excited$\backslash$nsingle-phase induction generator is presented. This generator consists$\backslash$nof a three-phase squirrel-cage induction machine Y-connected with three$\backslash$ncapacitors, and one of these capacitors is connected with a single-phase$\backslash$nload. Using a circuit model, the generator's steady-state performance is$\backslash$ntheoretically predicted, and confirmed through experiment},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ziebinski, Adam and Cupek, Rafal and Erdogan, Hueseyin and Waechter, Sonja},
doi = {10.1007/978-3-319-45246-3_13},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {978-3-642-40494-8},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {ADAS,Camera sensor,Lidar sensor,Radar sensor,Sensor fusion},
mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
pages = {135--146},
pmid = {25246403},
publisher = {Springer, Cham},
title = {{A survey of ADAS technologies for the future perspective of sensor fusion}},
url = {http://link.springer.com/10.1007/978-3-319-45246-3{\_}13},
volume = {9876 LNCS},
year = {2016}
}
@techreport{Waymo2017,
abstract = {Waymo's mission is to bring self-driving technology to the world, making it safe and easy for people and things to move around. We believe our technology can improve mobility by giving people the freedom to get around, and save thousands of lives now lost to traffic crashes. o u r m i s s i o n 2 3 Waymo Safety Report On The Road to Fully Self-Driving Self-driving vehicles hold the promise to improve road safety and offer new mobility options to millions of people. Whether they're saving lives or helping people run errands, commute to work, or drop kids off at school, fully self-driving vehicles hold enormous potential to transform people's lives for the better. Safety is at the core of Waymo's mission — it's why we were founded over eight years ago as the Google self-driving car project. Every year, 1.2 million lives are lost to traffic crashes around the world, and in the U.S. the number of tragedies is growing. A common element of these crashes is that 94{\%} involve human error. Driving is not as safe or as easy as it should be, while distracted driving is on the rise. We believe our technology could save thousands of lives now lost to traffic crashes every year. Our commitment to safety is reflected in everything we do, from our company culture to how we design and test our technology. In this, our first Safety Report on Waymo's fully self-driving technology, we detail Waymo's work on — and our commitment to — safety. This overview of our safety program underscores the important lessons learned through the 3.5 million miles Waymo's vehicles have self-driven on public roads and through our billions of miles of simulated driving.},
author = {Waymo},
file = {:D$\backslash$:/Users/109123/Desktop/waymo-safety-report-2017-10.pdf:pdf},
institution = {Waymo},
mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
pages = {43},
title = {{On the Road to Fully Self-Driving}},
url = {https://storage.googleapis.com/sdc-prod/v1/safety-report/waymo-safety-report-2017-10.pdf},
year = {2017}
}
@techreport{NHTSA2016,
abstract = {Technology in transportation is not new. In fact, the airplane, the automobile, the train and the horse-drawn carriage all introduced new opportunities and new complications to the safe movement of people and goods.$\backslash$r$\backslash$n$\backslash$r$\backslash$nAs the digital era increasingly reaches deeper into transportation, our task at the U.S. Department of Transportation is not only to keep pace, but to ensure public safety while establishing a strong foundation such that the rules of the road can be known, understood, and responded to by industry and the public. The self-driving car raises more possibilities and more questions than perhaps any other transportation innovation under present discussion. That is as it should be. Possessing the potential to uproot personal mobility as we know it, to make it safer and even more ubiquitous than conventional automobiles and perhaps even more efficient, self-driving cars have become the archetype of our future transportation. Still, important concerns emerge. Will they fully replace the human driver? What ethical judgments will they be called upon to make? What socioeconomic impacts flow from such a dramatic change? Will they disrupt the nature of privacy and security?$\backslash$r$\backslash$n$\backslash$r$\backslash$nMany of these larger questions will require longer and more thorough dialogue with government, industry, academia and, most importantly, the public.},
author = {NHTSA},
file = {:D$\backslash$:/Users/109123/Desktop/Federal automated vehicles policy.pdf:pdf},
institution = {NHTSA},
mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
number = {September},
pages = {1--116},
title = {{Federal Automated Vehicles Policy}},
url = {https://www.transportation.gov/AV{\%}0Apapers3://publication/uuid/2E47DA05-79E3-4DA5-BE3B-B172900149A1},
year = {2016}
}
@article{Nowakowski2015,
abstract = {Higher levels of road vehicle automation pose a regulatory challenge in the U.S. At the national level, the National Highway Traffic Safety Administration (NHTSA) has been responsible for ensuring vehicle safety through the mandatory Federal Motor Vehicle Safety Standards (FMVSS) and the voluntary New Car Assessment Program (NCAP). Although NHTSA typically regulates the technology aspects of vehicle safety, state agencies, such as a Department of Motor Vehicles (DMV), are responsible for the regulations governing training, evaluation, and licensing of drivers, and the registration of vehicles. Automation that allows drivers to disengage from monitoring and control tasks introduces safety concerns related to the vehicle technology (typically regulated by NHTSA) and to the Automated Vehicle's (AV) driving behavior and compliance with vehicle codes (typically regulated by states). This paper details a variety of issues that need to be addressed in support of state regulations for manufacturers' testing of AVs on public roads and the general public's operation of automated driving systems. The key challenges for these regulations are how to ensure public safety without discouraging technical innovations and how to define meaningful requirements in the absence of existing technical standards for automated driving systems. The topics covered in this paper include regulatory models of certification, driver training and licensing requirements, and a discussion of the distinction between behavioral competency (how well the automation handles the environment) and functional safety (how well the vehicle handles internal faults and failures). This information is reported here so that other jurisdictions and institutions that will need to grapple with the same issues will be able to benefit from what we have learned in the process of developing the first comprehensive set of state regulations governing road vehicle automation.},
author = {Nowakowski, Christopher and Shladover, Steven E and Chan, Ching-Yao and Tan, Han-Shue},
doi = {10.3141/2489-16},
file = {::},
isbn = {5106653673},
issn = {0361-1981},
journal = {Transportation Research Record: Journal of the Transportation Research Board},
mendeley-groups = {2018{\_}review{\_}sensors{\_}perception{\_}ADAS},
pages = {137--144},
title = {{Development of California Regulations to Govern Testing and Operation of Automated Driving Systems}},
url = {https://pdfs.semanticscholar.org/1ce1/ea57551a98a36f44bf9c45607da4462bfadc.pdf http://trrjournalonline.trb.org/doi/10.3141/2489-16},
volume = {2489},
year = {2015}
}

@InProceedings{behere2015functional,
  author       = {Behere, Sagar and Torngren, Martin},
  title        = {A functional architecture for autonomous driving},
  booktitle    = {Automotive Software Architecture (WASA), 2015 First International Workshop on},
  year         = {2015},
  pages        = {3--10},
  organization = {IEEE},
}

@Comment{jabref-meta: databaseType:bibtex;}
