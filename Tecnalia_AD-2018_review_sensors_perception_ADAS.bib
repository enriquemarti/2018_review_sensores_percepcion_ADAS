Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@techreport{Shalev-shwartz2016,
abstract = {Autonomous driving is a multi-agent setting where the host vehicle must apply sophisticated negotiation skills with other road users when overtaking, giving way, merging, taking left and right turns and while pushing ahead in unstructured urban roadways. Since there are many possible scenarios, manually tackling all possible cases will likely yield a too simplistic policy. Moreover, one must balance between unexpected behavior of other drivers/pedestrians and at the same time not to be too defensive so that normal traffic flow is maintained. In this paper we apply deep reinforcement learning to the problem of forming long term driving strategies. We note that there are two major challenges that make autonomous driving different from other robotic tasks. First, is the necessity for ensuring functional safety-something that machine learning has difficulty with given that performance is optimized at the level of an expectation over many instances. Second, the Markov Decision Process model often used in robotics is problematic in our case because of unpredictable behavior of other agents in this multi-agent scenario. We make three contributions in our work. First, we show how policy gradient iterations can be used, and the variance of the gradient estimation using stochastic gradient ascent can be minimized, without Markovian assumptions. Second, we decompose the problem into a composition of a Policy for Desires (which is to be learned) and trajectory planning with hard constraints (which is not learned). The goal of Desires is to enable comfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce a hierarchical temporal abstraction we call an "Option Graph" with a gating mechanism that significantly reduces the effective horizon and thereby reducing the variance of the gradient estimation even further. The Option Graph plays a similar role to "structured prediction" in supervised learning, thereby reducing sample complexity, while also playing a similar role to LSTM gating mechanisms used in supervised deep networks.},
archivePrefix = {arXiv},
arxivId = {1610.03295v1},
author = {Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
booktitle = {Arxiv},
eprint = {1610.03295v1},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shalev-shwartz, Shammah, Shashua - 2016 - Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving(2).pdf:pdf},
month = {oct},
title = {{Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving}},
url = {https://cdn.mobileye.com/wp-content/uploads/2016/10/Safe-Multi-Agent-Reinforcement-Learning-for-Autonomous-Driving.pdf http://arxiv.org/abs/1610.03295},
year = {2016}
}
@article{Phillips2017,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Phillips, Tyson Govan and Guenther, Nicky and McAree, Peter Ross},
doi = {10.1002/rob.21701},
eprint = {10.1.1.91.5767},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Guenther, McAree - 2017 - When the Dust Settles The Four Behaviors of LiDAR in the Presence of Fine Airborne Particulates.pdf:pdf},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
month = {aug},
number = {5},
pages = {985--1009},
pmid = {22164016},
publisher = {Wiley-Blackwell},
title = {{When the Dust Settles: The Four Behaviors of LiDAR in the Presence of Fine Airborne Particulates}},
url = {http://doi.wiley.com/10.1002/rob.21701},
volume = {34},
year = {2017}
}
@article{Wang2008,
abstract = {In the visible and near IR, absorption is negligible so that the atmospheric extinction can be derived by atmospheric scattering which is mainly contributed by fog droplet, rain droplet, another types of droplet and small articles. The forward-scattering visibility meter (FVM) works by illuminating with near IR light a small sample volume of about 100 mL of air and measuring the intensity scattered in the angular range of 30° to 36° degrees. The scattered intensity is proportional to the extinction coefficient regardless of the article size distribution and after wavelength calibration. The ratio of scattered signal to extinction coefficient of fog and haze can be achieved by comparative test of FVM outputs and manual observations. Nevertheless, as a result of the application of the measurement during rain with the ratio of fog and haze, an unacceptable error is raised. To obtain an accuracy extinction measurement during rain, an appropriated ratio of scattered signal to extinction coefficient of rain would be found. The calculation for different size distributions of fog and rain with Mie theory has been made in this paper. And a comparison of extinction measurements made with two FVMs and manual observations during fog and rain has been made. The result shows that during rain the FVM extinction coefficient is from 20{\%} to 60{\%} greater than that of manual observations. This result can be used to define correction factors so that the FVM using forward-scattering near IR spectroscopy not only can be used to estimate extinction during fog and haze as well as during rain.},
author = {Wang, M. and Liu, W.-Q. and Lu, Y.-H. and Zhao, X.-S. and Song, B.-C. and Zhang, Y.-J. and Wang, Y.-P. and Lian, C.-H. and Chen, Jun and Cheng, Yin and Liu, J.-G. and Wei, Q.-N.},
issn = {10000593},
journal = {Guang Pu Xue Yu Guang Pu Fen Xi/Spectroscopy and Spectral Analysis},
keywords = {Atmospheric extinction,Extinction coefficient,Forward-scattering,Near IR,Phase function,Size distribution,Visibility},
month = {aug},
number = {8},
pages = {1776--80},
pmid = {18975801},
title = {{Study on the measurement of the atmospheric extinction of fog and rain by forward-scattering near infrared spectroscopy}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18975801},
volume = {28},
year = {2008}
}
@phdthesis{Levinson2011a,
abstract = {This dissertation presents several related algorithms that enable$\backslash$nimportant capabilities for self-driving vehicles. Using a rotating$\backslash$nmulti-beam laser rangefinder to sense the world, our vehicle scans$\backslash$nmillions of 3D points every second. Calibrating these sensors plays$\backslash$na crucial role in accurate perception, but manual calibration is$\backslash$nunreasonably tedious, and generally inaccurate. As an alternative,$\backslash$nwe present an unsupervised algorithm for automatically calibrating$\backslash$nboth the intrinsics and extrinsics of the laser unit from only seconds$\backslash$nof driving in an arbitrary and unknown environment. We show that$\backslash$nthe results are not only vastly easier to obtain than traditional$\backslash$ncalibration techniques, they are also more accurate. A second key$\backslash$nchallenge in autonomous navigation is reliable localization in the$\backslash$nface of uncertainty. Using our calibrated sensors, we obtain high$\backslash$nresolution infrared reflectivity readings of the world. From these,$\backslash$nwe build large-scale self-consistent probabilistic laser maps of$\backslash$nurban scenes, and show that we can reliably localize a vehicle against$\backslash$nthese maps to within centimeters, even in dynamic environments, by$\backslash$nfusing noisy GPS and IMU readings with the laser in realtime. We$\backslash$nalso present a localization algorithm that was used in the DARPA$\backslash$nUrban Challenge, which operated without a prerecorded laser map,$\backslash$nand allowed our vehicle to complete the entire six-hour course without$\backslash$na single localization failure. Finally, we present a collection of$\backslash$nalgorithms for the mapping and detection of traffic lights in realtime.$\backslash$nThese methods use a combination of computer-vision techniques and$\backslash$nprobabilistic approaches to incorporating uncertainty in order to$\backslash$nallow our vehicle to reliably ascertain the state of traffic-light-controlled$\backslash$nintersections.},
author = {Levinson, Jesse},
pages = {153},
title = {{Automatic Laser Calibration, Mapping, and Localization for Autonomous Vehicles}},
url = {https://searchworks.stanford.edu/view/9275866 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:AUTOMATIC+LASER+CALIBRATION+,+MAPPING+,+AND+LOCALIZATION+FOR+AUTONOMOUS+VEHICLES+A+DISSERTATION+SUBMITTED+TO+THE+DEPARTMENT+OF+COMPUTER+SCIENCE+AND+THE+COMMITTEE+ON+GRADUATE+STUDIES+OF+STANFORD},
year = {2011}
}
@misc{Eldada2017,
address = {San Francisco},
author = {Eldada, Louay},
booktitle = {Automated Vehicles Symposium},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eldada - 2017 - LiDAR and the Autonomous Vehicle Revolution for Truck and Ride Sharing Fleets.pdf:pdf},
title = {{LiDAR and the Autonomous Vehicle Revolution for Truck and Ride Sharing Fleets}},
url = {https://higherlogicdownload.s3.amazonaws.com/AUVSI/14c12c18-fde1-4c1d-8548-035ad166c766/UploadedImages/2017/PDFs/Proceedings/ESS/Wednesday 1330-1400{\_}Louay Eldada.pdf},
year = {2017}
}
@article{Strobel2013,
author = {Strobel, Markus and D{\"{o}}ttling, Dietmar},
doi = {10.1515/aot-2012-0081},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Strobel, D{\"{o}}ttling - 2013 - High dynamic range CMOS (HDRC) imagers for safety systems.pdf:pdf},
issn = {2192-8584},
journal = {Advanced Optical Technologies},
keywords = {CMOS image sensor,OCIS codes: 110.4850,SafetyEYE,global shutter,high dynamic range CMOS (HDRC),safe camera system},
month = {jan},
number = {2},
pages = {147--157},
publisher = {THOSS Media {\&} De Gruyter},
title = {{High dynamic range CMOS (HDRC) imagers for safety systems}},
url = {https://www.degruyter.com/view/j/aot.2013.2.issue-2/aot-2012-0081/aot-2012-0081.xml},
volume = {2},
year = {2013}
}
@inproceedings{Dagan2004,
abstract = {The large number of rear end collisions due to driver inattention has been identified as a major automotive safety issue. Even a short advance warning can significantly reduce the number and severity of the collisions. This paper describes a vision based forward collision warning (FCW) system for highway safety. The algorithm described in this paper computes time to contact (TTC) and possible collision course directly from the size and position of the vehicles in the image - which are the natural measurements for a vision based system - without having to compute a 3D representation of the scene. The use of a single low cost image sensor results in an affordable system which is simple to install. The system has been implemented on real-time hardware and has been test driven on highways. Collision avoidance tests have also been performed on test tracks.},
author = {Dagan, E. and Mano, O. and Stein, G.P. and Shashua, A.},
booktitle = {IEEE Intelligent Vehicles Symposium, 2004},
doi = {10.1109/IVS.2004.1336352},
isbn = {0-7803-8310-9},
pages = {37--42},
publisher = {IEEE},
title = {{Forward collision warning with a single camera}},
url = {http://ieeexplore.ieee.org/document/1336352/},
year = {2004}
}
@article{Nguyen2013,
author = {Nguyen, Van Cuong and Heo, Moon Beom and Jee, Gyu-In},
doi = {10.11003/JKGS.2013.2.1.081},
issn = {2288-8187},
journal = {Journal of Positioning, Navigation, and Timing},
keywords = {1-point method,Ackermann's principle,Bundle Adjustment,kpubs,kpubs.org,rotation estimation},
month = {apr},
number = {1},
pages = {81--89},
publisher = {The Korean GNSS Society},
title = {{1-Point Ransac Based Robust Visual Odometry}},
url = {http://koreascience.or.kr/journal/view.jsp?kj=HOHSB0{\&}py=2013{\&}vnc=v2n1{\&}sp=81},
volume = {2},
year = {2013}
}
@phdthesis{Zhang2016,
author = {Zhang, Haocheng},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - 2016 - Rapid Inspection of Pavement Markings Using Mobile Laser Scanning Point Clouds.pdf:pdf},
month = {mar},
school = {University of Waterloo},
title = {{Rapid Inspection of Pavement Markings Using Mobile Laser Scanning Point Clouds}},
url = {https://uwspace.uwaterloo.ca/handle/10012/10343},
year = {2016}
}
@inproceedings{Bertozzi2011,
author = {Bertozzi, Massimo and Broggi, Alberto and Cardarelli, Elena and Fedriga, Rean Isabella and Mazzei, Luca and Porta, Pier Paolo},
booktitle = {IEEE Robotics and Automation Magazine},
doi = {10.1109/MRA.2011.942490},
issn = {10709932},
month = {sep},
number = {3},
pages = {120--124},
title = {{VIAC expedition toward autonomous mobility}},
url = {http://ieeexplore.ieee.org/document/6016589/},
volume = {18},
year = {2011}
}
@article{Kohler2013,
author = {K{\"{o}}hler, Mike and Hasch, J{\"{u}}rgen and Bl{\"{o}}cher, Hans Ludwig and Schmidt, Lorenz Peter},
doi = {10.1017/S175907871200075X},
issn = {17590787},
journal = {International Journal of Microwave and Wireless Technologies},
keywords = {Automotive radar},
month = {feb},
number = {1},
pages = {49--54},
title = {{Feasibility of automotive radar at frequencies beyond 100 GHz}},
url = {http://www.journals.cambridge.org/abstract{\_}S175907871200075X},
volume = {5},
year = {2013}
}
@inproceedings{Ieng2003,
address = {Nagoya, Japan},
author = {Ieng, S-S and Gruyer, D},
booktitle = {Proceedings of the 18th International Technical Conference on the Enhanced Safety of Vehicles},
month = {may},
publisher = {National Highway Traffic Safety Administration},
title = {{Merging lateral cameras information with proprioceptive sensors in vehicle location gives centimetric precision}},
url = {https://trid.trb.org/view.aspx?id=750799},
year = {2003}
}
@article{OMalley2008,
abstract = {Abstract In automotive design, the issue of safety remains a growing priority. Recently the focus has extended beyond the occupants of the vehicle and has turned towards other Vulnerable Road Users (VRU). Simple night vision systems have already become an ...},
author = {O'Malley, R. and Glavin, Martin and Jones, E.},
doi = {10.1049/cp:20080657},
isbn = {978-0-86341-931-7},
journal = {Signals and Systems Conference, 208.(ISSC 2008). IET Irish},
keywords = {- pedestrian detection,active safety,agery,driver assist,infrared,obstacle detection,thermal im-},
pages = {168--173},
pmid = {4780948},
publisher = {IEE},
title = {{A review of automotive infrared pedestrian detection techniques}},
url = {http://digital-library.theiet.org/content/conferences/10.1049/cp{\_}20080657 papers2://publication/uuid/FFD556E9-8AF3-48F2-9C9B-3B57247056B7{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4780948{\%}5Cnpapers2://publication/uuid/FC2526CD-D332-4959-B833},
year = {2008}
}
@inproceedings{Censi2014,
abstract = {— The agility of a robotic system is ultimately limited by the speed of its processing pipeline. The use of a Dynamic Vision Sensors (DVS), a sensor producing asynchronous events as luminance changes are perceived by its pixels, makes it pos-sible to have a sensing pipeline of a theoretical latency of a few microseconds. However, several challenges must be overcome: a DVS does not provide the grayscale value but only changes in the luminance; and because the output is composed by a sequence of events, traditional frame-based visual odometry methods are not applicable. This paper presents the first visual odometry system based on a DVS plus a normal CMOS camera to provide the absolute brightness values. The two sources of data are automatically spatiotemporally calibrated from logs taken during normal operation. We design a visual odometry method that uses the DVS events to estimate the relative displacement since the previous CMOS frame by processing each event individually. Experiments show that the rotation can be estimated with surprising accuracy, while the translation can be estimated only very noisily, because it produces few events due to very small apparent motion.},
address = {Hong Kong},
author = {Censi, Andrea and Scaramuzza, Davide},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2014.6906931},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Censi, Scaramuzza - 2014 - Low-Latency Event-Based Visual Odometry.pdf:pdf},
pages = {703--710},
title = {{Low-Latency Event-Based Visual Odometry}},
url = {http://rpg.ifi.uzh.ch/docs/ICRA14{\_}Censi.pdf},
year = {2014}
}
@article{Broggi2012,
abstract = {Autonomous driving is one of the most interesting fields of research, with a number of important applications, like agricultural, military and, most significantly, safety. This paper addresses the problem of designing a general purpose path planner and its associated low level control for autonomous vehicles operating in unknown environments. Different kinds of inputs, like the results of obstacle detection, ditch localization, lane detection, and global path planning information are merged together using potential fields to build a representation of the environment in real-time; kinematically feasible trajectories, based on vehicle dynamics, are generated on a cost map. This approach demonstrated both flexibility and reliability for vehicle driving in very different environments, including extreme road conditions. This controller was extensively tested during VIAC, the VisLab Intercontinental Autonomous Challenge, a 13,000 km long test for intelligent vehicle applications. The results, collected during the development stage and the experiment itself, are presented in the final part of this article. {\textcopyright} 2012 Published by Elsevier Ltd. All rights reserved.},
author = {Broggi, A. and Medici, P. and Zani, P. and Coati, A. and Panciroli, M.},
doi = {10.1016/j.arcontrol.2012.03.012},
file = {::},
isbn = {1367-5788},
issn = {13675788},
journal = {Annual Reviews in Control},
keywords = {Autonomous navigation,Path planning},
month = {apr},
number = {1},
pages = {161--171},
publisher = {Pergamon},
title = {{Autonomous vehicles control in the VisLab Intercontinental Autonomous Challenge}},
url = {https://www.sciencedirect.com/science/article/pii/S1367578812000132},
volume = {36},
year = {2012}
}
@article{Liu2015,
abstract = {With the continuing growth of highway construction and vehicle use expansion all over the world, highway vehicle traffic rule violation (TRV) detection has become more and more important so as to avoid traffic accidents and injuries in intelligent transportation systems (ITS) and vehicular ad hoc networks (VANETs). Since very few works have contributed to solve the TRV detection problem by moving vehicle measurements and surveillance devices, this paper develops a novel parallel ultrasonic sensor system that can be used to identify the TRV behavior of a host vehicle in real-time. Then a two-dimensional state method is proposed, utilizing the spacial state and time sequential states from the data of two parallel ultrasonic sensors to detect and count the highway vehicle violations. Finally, the theoretical TRV identification probability is analyzed, and actual experiments are conducted on different highway segments with various driving speeds, which indicates that the identification accuracy of the proposed method can reach about 90.97{\%}.},
author = {Liu, Jun and Han, Jiuqiang and Lv, Hongqiang and Li, Bing},
doi = {10.3390/s150409000},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - An Ultrasonic Sensor System Based on a Two-Dimensional State Method for Highway Vehicle Violation Detection Applicat.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Highway vehicle traffic rule violation detection,Intelligent transportation systems,Two-dimensional state method,Ultrasonic sensor system},
month = {apr},
number = {4},
pages = {9000--9021},
pmid = {1603305},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{An ultrasonic sensor system based on a two-dimensional state method for highway vehicle violation detection applications}},
url = {http://www.mdpi.com/1424-8220/15/4/9000/},
volume = {15},
year = {2015}
}
@inproceedings{Tatschke2006,
abstract = {This publication focuses on a modular architecture for sensor data fusion regarding to research work of common interest related to sensors and sensor data fusion. This architecture will be based on an extended environment model and representation, consisting of a set of common data structures for sensor, object and situation refinement data and algorithms as well as the corresponding models. The aim of such research is to contribute to a measurable enhancement of the output performance provided by multi-sensor systems in terms of actual availability, reliability, accuracy and precision of the perception results. In this connection, investigations towards fusion concepts and paradigms, such as {\^{a}}€˜redundant{\^{a}}€™ and {\^{a}}€˜complementary{\^{a}}€™, as well as {\^{a}}€˜early{\^{a}}€™ and track-based sensor data fusion approaches, are conducted, in order to significantly enhance the overall performance of the perception system.},
address = {Berlin/Heidelberg},
author = {Tatschke, Thomas and Park, S. B. and Amditis, Angelos and Polychronopoulos, Aristomenis and Scheunert, Ullrich and Aycard, Olivier},
booktitle = {Advanced Microsystems for Automotive Applications 2006},
doi = {10.1007/3-540-33410-6_32},
isbn = {3540334092},
keywords = {Early fusion,Environment modelling,Environment perception,Fusion feedback,Fusion framework,Grid-based fusion,Multi level fusion,ProFusion2,Sensor data fusion,Track-based fusion},
pages = {451--469},
publisher = {Springer-Verlag},
title = {{ProFusion2 - Towards a modular, robust and reliable fusion architecture for automotive environment perception}},
url = {http://link.springer.com/10.1007/3-540-33410-6{\_}32 http://lig-membres.imag.fr/aycard/html/Publications/2006/Tatschke06.pdf},
year = {2006}
}
@misc{Shalev-Shwartz2017,
abstract = {In recent years, car makers and tech companies have been racing towards self driving cars. It seems that the main parameter in this race is who will have the first car on the road. The goal of this paper is to add to the equation two additional crucial parameters. The first is standardization of safety assurance --- what are the minimal requirements that every self-driving car must satisfy, and how can we verify these requirements. The second parameter is scalability --- engineering solutions that lead to unleashed costs will not scale to millions of cars, which will push interest in this field into a niche academic corner, and drive the entire field into a "winter of autonomous driving". In the first part of the paper we propose a white-box, interpretable, mathematical model for safety assurance, which we call Responsibility-Sensitive Safety (RSS). In the second part we describe a design of a system that adheres to our safety assurance requirements and is scalable to millions of cars.},
archivePrefix = {arXiv},
arxivId = {1708.06374},
author = {Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
doi = {1708.06374v2},
eprint = {1708.06374},
file = {::},
month = {aug},
title = {{On a Formal Model of Safe and Scalable Self-driving Cars}},
url = {http://arxiv.org/abs/1708.06374},
year = {2017}
}
@techreport{LeddarTech2016,
abstract = {Remote sensing consists of acquiring information about a specific object in the vicinity of a sensor without making physical contact with the object. Countless applications such as automotive driver assistance systems and autonomous driving, drone and robot collision avoidance and navigation, traffic management and level sensing exist thanks to this technique. Multiple technology options are available for remote sensing; we can divide them into three broad applications: Presence or proximity detection, where the absence or presence of an object in a general area is the only information that is required (e.g., for security applications). This is the simplest form of remote sensing; Speed measurement, where the exact position of an object does not need to be known but where its accurate speed is required (e.g., for law enforcement applications); and Detection and ranging, where the position of an object relative to the sensor needs to be precisely and accurately determined. This paper will concentrate on technologies capable of providing a detection and ranging functionality, as it is the most complex of the three applications. From the position information, presence and speed can be retrieved so technologies capable of detection and ranging can be universally applied to all remote sensing applications.},
address = {Quebec},
author = {Olivier, Pierre},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Olivier - 2016 - Leddar Optical Time-of-Flight sensing Technology A new approach to detection and ranging.pdf:pdf},
institution = {LeddarTech},
pages = {13},
title = {{LEDDAR optical Time-of-Flight sensing technology: A new approach to detection and ranging}},
url = {https://leddartech.com/app/uploads/dlm{\_}uploads/2016/02/Leddar-Optical-Time-of-Flight-Sensing-Technology-1.pdf https://d1wx5us9wukuh0.cloudfront.net/app/uploads/dlm{\_}uploads/2016/02/Leddar-Optical-Time-of-Flight-Sensing-Technology-1.pdf},
year = {2016}
}
@article{CastilloAguilar2015,
abstract = {The appearance of active safety systems, such as Anti-lock Braking System, Traction Control System, Stability Control System, etc., represents a major evolution in road safety. In the automotive sector, the term vehicle active safety systems refers to those whose goal is to help avoid a crash or to reduce the risk of having an accident. These systems safeguard us, being in continuous evolution and incorporating new capabilities continuously. In order for these systems and vehicles to work adequately, they need to know some fundamental information: the road condition on which the vehicle is circulating. This early road detection is intended to allow vehicle control systems to act faster and more suitably, thus obtaining a substantial advantage. In this work, we try to detect the road condition the vehicle is being driven on, using the standard sensors installed in commercial vehicles. Vehicle models were programmed in on-board systems to perform real-time estimations of the forces of contact between the wheel and road and the speed of the vehicle. Subsequently, a fuzzy logic block is used to obtain an index representing the road condition. Finally, an artificial neural network was used to provide the optimal slip for each surface. Simulations and experiments verified the proposed method.},
author = {{Castillo Aguilar}, Juan Jes{\'{u}}s and {Cabrera Carrillo}, Juan Antonio and {Guerra Fern{\'{a}}ndez}, Antonio Jes{\'{u}}s and {Carabias Acosta}, Enrique},
doi = {10.3390/s151229908},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Castillo Aguilar et al. - 2015 - Robust Road Condition Detection System Using In-Vehicle Standard Sensors.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Friction estimation,Normal driving,Optimal slip estimation,Standard vehicle sensor},
month = {dec},
number = {12},
pages = {32056--32078},
pmid = {26703605},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Robust road condition detection system using in-vehicle standard sensors}},
url = {http://www.mdpi.com/1424-8220/15/12/29908},
volume = {15},
year = {2015}
}
@article{Koch2011,
abstract = {Pavement condition assessment is essential when developing road network maintenance programs. In practice, the data collection process is to a large extent automated. However, pavement distress detection (cracks, potholes, etc.) is mostly performed manually, which is labor-intensive and time-consuming. Existing methods either rely on complete 3D surface reconstruction, which comes along with high equipment and computation costs, or make use of acceleration data, which can only provide preliminary and rough condition surveys. In this paper we present a method for automated pothole detection in asphalt pavement images. In the proposed method an image is first segmented into defect and non-defect regions using histogram shape-based thresholding. Based on the geometric properties of a defect region the potential pothole shape is approximated utilizing morphological thinning and elliptic regression. Subsequently, the texture inside a potential defect shape is extracted and compared with the texture of the surrounding non-defect pavement in order to determine if the region of interest represents an actual pothole. This methodology has been implemented in a MATLAB prototype, trained and tested on 120 pavement images. The results show that this method can detect potholes in asphalt pavement images with reasonable accuracy. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {Koch, Christian and Brilakis, Ioannis},
doi = {10.1016/j.aei.2011.01.002},
isbn = {1474-0346},
issn = {14740346},
journal = {Advanced Engineering Informatics},
keywords = {Image processing,Pavement assessment,Pothole detection,Visual sensing},
month = {aug},
number = {3},
pages = {507--515},
publisher = {Elsevier},
title = {{Pothole detection in asphalt pavement images}},
url = {http://www.sciencedirect.com/science/article/pii/S1474034611000036},
volume = {25},
year = {2011}
}
@article{Scaramuzza2008,
author = {Scaramuzza, D. and Siegwart, R.},
doi = {10.1109/TRO.2008.2004490},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
month = {oct},
number = {5},
pages = {1015--1026},
title = {{Appearance-Guided Monocular Omnidirectional Visual Odometry for Outdoor Ground Vehicles}},
url = {http://ieeexplore.ieee.org/document/4625958/},
volume = {24},
year = {2008}
}
@article{Aqel2016,
abstract = {Accurate localization of a vehicle is a fundamental challenge and one of the most important tasks of mobile robots. For autonomous navigation, motion tracking, and obstacle detection and avoidance, a robot must maintain knowledge of its position over time. Vision-based odometry is a robust technique utilized for this purpose. It allows a vehicle to localize itself robustly by using only a stream of images captured by a camera attached to the vehicle. This paper presents a review of state-of-the-art visual odometry (VO) and its types, approaches, applications, and challenges. VO is compared with the most common localization sensors and techniques, such as inertial navigation systems, global positioning systems, and laser sensors. Several areas for future research are also highlighted.},
author = {Aqel, Mohammad O A and Marhaban, Mohammad H and Saripan, M Iqbal and Ismail, Napsiah Bt},
doi = {10.1186/s40064-016-3573-7},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aqel et al. - 2016 - Review of visual odometry types, approaches, challenges, and applications.pdf:pdf},
issn = {2193-1801},
journal = {SpringerPlus},
keywords = {Global positioning system,Image stream,Inertial navigation system,Localization sensors,Visual odometry},
number = {1},
pages = {1897},
pmid = {27843754},
publisher = {Springer},
title = {{Review of visual odometry: types, approaches, challenges, and applications.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27843754 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5084145},
volume = {5},
year = {2016}
}
@inproceedings{Bak2012,
author = {Bak, Adrien and Gruyer, Dominique and Bouchafa, Samia and Aubert, Didier},
booktitle = {2012 15th International IEEE Conference on Intelligent Transportation Systems},
doi = {10.1109/ITSC.2012.6338771},
isbn = {978-1-4673-3063-3},
month = {sep},
pages = {1365--1370},
publisher = {IEEE},
title = {{Multi-sensor localization - Visual Odometry as a low cost proprioceptive sensor}},
url = {http://ieeexplore.ieee.org/document/6338771/},
year = {2012}
}
@article{Ziegler2014,
abstract = {125 years after Bertha Benz completed the first overland journey in automotive history, the Mercedes Benz S-Class S 500 INTELLIGENT DRIVE followed the same route from Mannheim to Pforzheim, Germany, in fully autonomous manner. The autonomous vehicle was equipped with close-to-production sensor hardware and relied solely on vision and radar sensors in combination with accurate digital maps to obtain a comprehensive understanding of complex traffic situations. The historic Bertha Benz Memorial Route is particularly challenging for autonomous driving. The course taken by the autonomous vehicle had a length of 103 km and covered rural roads, 23 small villages and major cities (e.g. downtown Mannheim and Heidelberg). The route posed a large variety of difficult traffic scenarios including intersections with and without traffic lights, roundabouts, and narrow passages with oncoming traffic. This paper gives an overview of the autonomous vehicle and presents details on vision and radar-based perception, digital road maps and video-based self-localization, as well as motion planning in complex urban scenarios.},
author = {Ziegler, Julius and Bender, Philipp and Schreiber, Markus and Lategahn, Henning and Strauss, Tobias and Stiller, Christoph and Dang, Thao and Franke, Uwe and Appenrodt, Nils and Keller, Christoph G. and Kaus, Eberhard and Herrtwich, Ralf G. and Rabe, Clemens and Pfeiffer, David and Lindner, Frank and Stein, Fridtjof and Erbs, Friedrich and Enzweiler, Markus and Knoppel, Carsten and Hipp, Jochen and Haueis, Martin and Trepte, Maximilian and Brenk, Carsten and Tamke, Andreas and Ghanaat, Mohammad and Braun, Markus and Joos, Armin and Fritz, Hans and Mock, Horst and Hein, Martin and Zeeb, Eberhard},
doi = {10.1109/MITS.2014.2306552},
isbn = {1939-1390},
issn = {19391390},
journal = {IEEE Intelligent Transportation Systems Magazine},
number = {2},
pages = {8--20},
title = {{Making bertha drive-an autonomous journey on a historic route}},
url = {http://ieeexplore.ieee.org/document/6803933/},
volume = {6},
year = {2014}
}
@inproceedings{Funke2012,
abstract = {This paper presents a novel approach to au- tonomous driving at the vehicle's handling limits. Such a system requires a high speed, consistent control signal as well as numerous safety features capable of monitoring and stopping the vehicle. When operating, the system's high level controller utilizes a highly accurate differential GPS and known friction values to drive a precomputed path at the friction limits of the vehicle. The system was tested in a variety of road conditions, including the challenging Pikes Peak Hill climb. Results from this work can be extended to improve driving safety and accident avoidance in vehicles.},
author = {Funke, Joseph and Theodosis, Paul and Hindiyeh, Rami and Stanek, Ganymed and Kritatakirana, Krisada and Gerdes, Chris and Langer, Dirk and Hernandez, Marcial and M{\"{u}}ller-Bessler, Bernhard and Huhnke, Burkhard},
booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
doi = {10.1109/IVS.2012.6232212},
isbn = {9781467321198},
issn = {1931-0587},
month = {jun},
pages = {541--547},
publisher = {IEEE},
title = {{Up to the limits: Autonomous Audi TTS}},
url = {http://ieeexplore.ieee.org/document/6232212/},
year = {2012}
}
@article{Besbes2015,
abstract = {One of the main challenges in intelligent vehicles concerns pedestrian detection for driving assistance. Recent experiments have showed that state-of-the-art descriptors provide better performances on the far-infrared (FIR) spectrum than on the visible one, even in daytime conditions, for pedestrian classification. In this paper, we propose a pedestrian detector with on-board FIR camera. Our main contribution is the exploitation of the specific characteristics of FIR images to design a fast, scale-invariant and robust pedestrian detector. Our system consists of three modules, each based on speeded-up robust feature (SURF) matching. The first module allows generating regions-of-interest (ROI), since in FIR images of the pedestrian shapes may vary in large scales, but heads appear usually as light regions. ROI are detected with a high recall rate with the hierarchical codebook of SURF features located in head regions. The second module consists of pedestrian full-body classification by using SVM. This module allows one to enhance the precision with low computational cost. In the third module, we combine the mean shift algorithm with inter-frame scale-invariant SURF feature tracking to enhance the robustness of our system. The experimental evaluation shows that our system outperforms, in the FIR domain, the state-of-the-art Haar-like Adaboost-cascade, histogram of oriented gradients (HOG)/linear SVM (linSVM) and MultiFtrpedestrian detectors, trained on the FIR images.},
author = {Besbes, Bassem and Rogozan, Alexandrina and Rus, Adela Maria and Bensrhair, Abdelaziz and Broggi, Alberto},
doi = {10.3390/s150408570},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Besbes et al. - 2015 - Pedestrian Detection in Far-Infrared Daytime Images Using a Hierarchical Codebook of SURF.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Far-infrared images,Hierarchical codebook,Pedestrian classification and trackings,Pedestrian detection,SURF,SVM,Scale-invariant feature matching},
month = {apr},
number = {4},
pages = {8570--8594},
pmid = {25871724},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Pedestrian detection in far-infrared daytime images using a hierarchical codebook of SURF}},
url = {http://www.mdpi.com/1424-8220/15/4/8570/},
volume = {15},
year = {2015}
}
@inproceedings{Janda2013,
abstract = {An approach for detecting the road boundary on different types of roads without any preliminary knowledge is presented. We fuse information obtained from an algorithm which detects road markings and road edges in images acquired by a video camera as well as data from a radar sensor. Each road marking, each road edge and each road barrier is tracked individually. Hence we can even capture exits or laybys. We use an edge image for road marking detection and texture information for road edge detection. Additional data provided by a radar sensor is used to measure targets referring to static barriers along the road side such as guardrails. The output of each processing unit is fused into a Kalman filter framework, where the confidence of each subsystem influences the innovation of the overall system. The underlying geometric road model comprises parameters for multiple lanes, the flanking road edge as well as the vehicle's relative pose. The work is part of the project Interactive.},
author = {Janda, Florian and Pangerl, Sebastian and Lang, Eva and Fuchs, Erich},
booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
doi = {10.1109/IVS.2013.6629625},
isbn = {9781467327558},
issn = {1931-0587},
month = {jun},
pages = {1173--1178},
publisher = {IEEE},
title = {{Road boundary detection for run-off road prevention based on the fusion of video and radar}},
url = {http://ieeexplore.ieee.org/document/6629625/},
year = {2013}
}
@inproceedings{Stein2003,
abstract = {This paper describes a vision-based adaptive cruise control (ACC) system which uses a single camera as input. In particular, we discuss how to compute the range and range-rate from a single camera and discuss how the imaging geometry affects the range and range rate accuracy. We determine the bound on the accuracy given a particular configuration. These bounds in turn determine what steps must be made to achieve good performance. The system has been implemented on a test vehicle and driven on various highways over thousands of miles.},
author = {Stein, Gideon P. and Mano, Ofer and Shashua, Amnon},
booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
doi = {10.1109/IVS.2003.1212895},
isbn = {0780378482},
pages = {120--125},
publisher = {IEEE},
title = {{Vision-based ACC with a single camera: Bounds on range and range rate accuracy}},
url = {http://ieeexplore.ieee.org/document/1212895/},
year = {2003}
}
@article{Broggi2013,
author = {Broggi, Alberto and Cattani, Stefano and Medici, Paolo and Zani, Paolo},
doi = {10.1007/978-3-642-28661-2_9},
file = {::},
isbn = {9783642286605},
issn = {1860949X},
journal = {Studies in Computational Intelligence},
pages = {215--250},
title = {{Applications of computer vision to vehicles: An extreme test}},
url = {https://link.springer.com/content/pdf/10.1007/978-3-642-28661-2{\_}9.pdf},
volume = {411},
year = {2013}
}
@article{Dickmanns1987,
abstract = {A visual feedback control system has been developed which is able to guide road vehicles on well structured roads at high speeds. The road boundary markings are tracked by a multiprocessor image processing system using contour correlation and curvature models together with the laws of perspective projection. Feature position data are the input into Kalman filters to estimate both the vehicle state vector relative to the driving lane and road curvature parameters. Velocity is measured conventionally. Longitudinal control by throttle and braking is geared to lateral acceleration due to road curvature; lateral control has an anticipatory feed forward and a compensatory feedback component. The control system has been tested with a CCD TV-camera and image sequence processing hardware in a real time simulation loop and with our experimental vehicle, a 5 ton-van equipped with sensors, onboard computers and actuators for autonomous driving},
author = {Dickmanns, E.D. and Zapp, A.},
doi = {10.1016/S1474-6670(17)55320-3},
file = {::},
issn = {14746670},
journal = {IFAC Proceedings Volumes},
month = {jul},
number = {5},
pages = {221--226},
publisher = {Elsevier},
title = {{Autonomous High Speed Road Vehicle Guidance by Computer Vision 1}},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017553203 http://linkinghub.elsevier.com/retrieve/pii/S1474667017553203},
volume = {20},
year = {1987}
}
@article{Lundquist2011,
abstract = {We provide a sensor fusion framework for solving the problem of joint ego-motion and road geometry estimation. More specifically we employ a sensor fusion framework to make systematic use of the measurements from a forward looking radar and camera, steering wheel angle sensor, wheel speed sensors and inertial sensors to compute good estimates of the road geometry and the motion of the ego vehicle on this road. In order to solve this problem we derive dynamical models for the ego vehicle, the road and the leading vehicles. The main difference to existing approaches is that we make use of a new dynamic model for the road. An extended Kalman filter is used to fuse data and to filter measurements from the camera in order to improve the road geometry estimate. The proposed solution has been tested and compared to existing algorithms for this problem, using measurements from authentic traffic environments on public roads in Sweden. The results clearly indicate that the proposed method provides better estimates. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Lundquist, Christian and Sch{\"{o}}n, Thomas B.},
doi = {10.1016/j.inffus.2010.06.007},
isbn = {1566-2535},
issn = {15662535},
journal = {Information Fusion},
keywords = {Bicycle model,Extended Kalman filter,Road geometry estimation,Sensor fusion,Single track model},
month = {oct},
number = {4},
pages = {253--263},
title = {{Joint ego-motion and road geometry estimation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1566253510000709},
volume = {12},
year = {2011}
}
@inproceedings{Bender2014,
abstract = {In this paper we propose a highly detailed map for the field of autonomous driving. We introduce the notion of lanelets to represent the drivable environment under both geometrical and topological aspects. Lanelets are atomic, inter- connected drivable road segments which may carry additional data to describe the static environment. We describe the map specification, an example creation process as well as the access library libLanelet which is available for download. Based on the map, we briefly describe our behavioural layer (which we call behaviour generation) which is heavily exploiting the proposed map structure. Both contributions have been used throughout the autonomous journey of the MERCEDES BENZ S 500 INTELLIGENT DRIVE following the Bertha Benz Memorial Route in summer 2013.},
author = {Bender, Philipp and Ziegler, Julius and Stiller, Christoph},
booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
doi = {10.1109/IVS.2014.6856487},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bender, Ziegler, Stiller - 2014 - Lanelets Efficient map representation for autonomous driving.pdf:pdf},
isbn = {9781479936380},
month = {jun},
pages = {420--425},
publisher = {IEEE},
title = {{Lanelets: Efficient map representation for autonomous driving}},
url = {http://ieeexplore.ieee.org/document/6856487/},
year = {2014}
}
@article{Gregor2002,
abstract = {The paper gives a survey on the new Expectation-based Multifocal$\backslash$nSaccadic vision (EMS-vision) system for autonomous vehicle guidance$\backslash$ndeveloped at the Universitat der Bundeswehr Munchen (UBM). EMS-Vision is$\backslash$nthe third generation dynamic vision system following the 4-D approach.$\backslash$nIts core element is a new camera arrangement, mounted on a high$\backslash$nbandwidth pan-tilt head for active gaze control. Central knowledge$\backslash$nrepresentation and a hierarchical system architecture allow efficient$\backslash$nactivation and control of behavioral capabilities for perception and$\backslash$naction. The system has been implemented on commercial off-the-shelf$\backslash$n(COTS) hardware components in both UBM test vehicles VaMoRs and VaMP$\backslash$nResults from autonomous turnoff maneuvers, performed on army proving$\backslash$ngrounds, are discussed},
author = {Gregor, Rudolf and L{\"{u}}tzeler, M. and Pellkofer, M. and Siedersberger, K. H. and Dickmanns, Ernst Dieter},
doi = {10.1109/6979.994795},
isbn = {0-7803-6363-9},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Active vision system,Autonomous vehicles,Control of perception and action,Dynamic machine vision,Knowledge representation,System architecture},
month = {mar},
number = {1},
pages = {48--59},
title = {{EMS-Vision: A Perceptual System for Autonomous Vehicles}},
url = {http://ieeexplore.ieee.org/document/994795/},
volume = {3},
year = {2002}
}
@article{Broggi2012a,
abstract = {This paper presents the VisLab Intercontinental Autonomous Challenge (VIAC), an autonomous vehicles test carried out from Parma to Shanghai between July and October 2010 by the VisLab team. The vehicle equipment is explained introducing the sensing systems which were tested during the journey. Trip details and the fi rst statistics are presented as well.},
author = {Broggi, Alberto and Cerri, Pietro and Felisa, Mirko and Laghi, Maria Chiara and Mazzei, Luca and Porta, Pier Paolo},
doi = {10.1504/IJVAS.2012.051250},
issn = {1471-0226},
journal = {International Journal of Vehicle Autonomous Systems},
number = {3},
pages = {147},
title = {{The VisLab Intercontinental Autonomous Challenge: an extensive test for a platoon of intelligent vehicles}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.591.8385 http://www.inderscience.com/link.php?id=51250},
volume = {10},
year = {2012}
}
@inproceedings{Maqueda2018,
abstract = {Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, filtering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle's steering angle. To make the best out of this sensor-algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset ({\~{}}1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras.},
address = {Salt Lake City},
archivePrefix = {arXiv},
arxivId = {1804.01310},
author = {Maqueda, Ana I and Loquercio, Antonio and Gallego, Guillermo and Garcia, Narciso and Scaramuzza, Davide},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2018.00568},
eprint = {1804.01310},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maqueda et al. - 2018 - Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars.pdf:pdf},
keywords = {Computer Vision,Machine Learning},
title = {{Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars}},
url = {http://rpg.ifi.uzh.ch/docs/CVPR18{\_}Maqueda.pdf http://arxiv.org/abs/1804.01310},
year = {2018}
}
@article{Pueo2016,
abstract = {Video analysis can be a qualitative or quantitative process to analyze motion occurring in a single plane using one camera (two-dimensional or 2D) or in more than one plane using two or more cameras simultaneously (three-dimensional or 3D). Quantitative 2D video analysis is performed through a digitizing process that converts body segments or sport implements into digital horizontal and vertical coordinates in the computer. In order for these measurements to be accurate, image capture by means of video cameras must be sharp and motion blur-free, especially in high speed motions. In this paper, a detailed introduction to factors affecting image quality will be presented. Furthermore, selection of the most appropriate camera setting to undertake high speed motion analysis with the best quality possible, both spatially (focus and resolution) and temporally (frame rate, motion blur, shutter options and lighting), will be discussed. Rather than considering commercial criteria, the article will focus on key features to choose the most convenient model both from technical and economical perspectives. Then, a revision of available cameras on the market as of 2015 will be carried out, with selected models grouped into three categories: high-, mid- and low-range, according to their maximum performance in relation to high speed features. Finally, a suggested recording procedure to minimize perspective errors and produce high quality video recordings will be presented. This guideline starts with indications for camera selection prior to purchase or for testing if a given camera would fulfil the minimum features. A good video recording dramatically improves the analysis quality and enables digitizing software to produce accurate measurements},
author = {Pueo, Basilio},
doi = {10.14198/jhse.2016.111.05},
issn = {19885202},
journal = {Journal of Human Sport and Exercise},
keywords = {Biomechanics,Frame rate,Motion blur,Performance,Shutter speed},
month = {dec},
number = {1},
pages = {53--73},
title = {{High speed cameras for motion analysis in sports science}},
url = {http://hdl.handle.net/10045/61909},
volume = {11},
year = {2016}
}
@misc{Thorpe1997,
abstract = {In August of 1997, The US National Automated Highway System Consortium (NAHSC) presented a proof of technical feasibility demonstration of automated driving. The 97 Demo took place on car-pool lanes on I-15, in San Diego, California. Members of the Consortium demonstrated many different functions: Vision-based road following, Following magnetic nails, Following a radar reflective strip, Radar-based headway maintenance, Ladar-based headway maintenance, Evolutionary systems, Close vehicle following (platooning), Cooperative maneuvering, Obstacle detection and avoidance, Mixed automated and manual driving, Mixed automated cars and buses, and Semi-automated maintenance. CMU led the effort to build one of the seven demonstration scenarios, the Free Agent Demonstration (FAD). The FAD involved two fully automated cars, one partially automated car, and two fully automated city buses. The scenario demonstrates lane entry, speed and headway control, lane following, lane changing, obstacle detection, and cooperative maneuvers. This paper describes the free agent demonstration itself, the technology that made the demonstration possible, and the future work to analyze the feasibility of turning the demonstration system into a practical prototype.},
author = {Thorpe, Chuck and Jochem, Todd and Pomerleau, Dean},
booktitle = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
doi = {10.1109/ITSC.1997.660524},
isbn = {0-7803-4269-0},
pages = {496--501},
publisher = {IEEE},
title = {{1997 Automated highway free agent demonstration}},
url = {http://ieeexplore.ieee.org/document/660524/ http://www.scopus.com/inward/record.url?eid=2-s2.0-0031366442{\&}partnerID=tZOtx3y1},
year = {1997}
}
@techreport{Urmson2007,
author = {Urmson, Chris},
file = {::},
title = {{DARPA Urban Challenge Final Report for Tartan Racing}},
url = {https://pdfs.semanticscholar.org/3116/38e299acef3cbd3423649b77ef73c2a94fc1.pdf},
year = {2007}
}
@inproceedings{Levinson2011,
abstract = {In order to achieve autonomous operation of a vehicle in urban situations with unpredictable traffic, several realtime systems must interoperate, including environment perception, localization, planning, and control. In addition, a robust vehicle platform with appropriate sensors, computational hardware, networking, and software infrastructure is essential.},
archivePrefix = {arXiv},
arxivId = {1702.06827},
author = {Levinson, Jesse and Askeland, Jake and Becker, Jan and Dolson, Jennifer and Held, David and Kammel, Soeren and Kolter, J. Zico and Langer, Dirk and Pink, Oliver and Pratt, Vaughan and Sokolsky, Michael and Stanek, Ganymed and Stavens, David and Teichman, Alex and Werling, Moritz and Thrun, Sebastian},
booktitle = {IEEE Intelligent Vehicles Symposium, Proceedings},
doi = {10.1109/IVS.2011.5940562},
eprint = {1702.06827},
isbn = {9781457708909},
issn = {1931-0587},
month = {jun},
pages = {163--168},
pmid = {11317986},
publisher = {IEEE},
title = {{Towards fully autonomous driving: Systems and algorithms}},
url = {http://ieeexplore.ieee.org/document/5940562/},
year = {2011}
}
@article{Gatziolis2008,
abstract = {Light detection and ranging (LIDAR) is an emerging remote-sensing technology with promising potential to assist in mapping, monitoring, and assessment of forest resources. Continuous technological advancement and substantial reductions in data acquisition cost have enabled acquisition of laser data over entire states and regions. These developments have triggered an explosion of interest in LIDAR technology. Despite a growing body of peer-reviewed literature documenting the merits of LIDAR for forest assessment, management, and planning, there seems to be little information describing in detail the acquisition, quality assessment, and processing of laser data for forestry applications. This report addresses this information deficit by providing a foundational knowledge base containing answers to the most frequently asked questions. Keywords: LIDAR, Pacific Northwest, FIA, forest inventory, laser, absolute and relative accuracy, precision, registration, stand penetration, DEM, canopy surface, resolution, data storage, data quality assessment, topography, scanning.},
author = {Gatziolis, Demetrios and Andersen, Hans Erik},
doi = {Gen. Tech. Rep. PNW-GTR-768},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gatziolis, Andersen - 2008 - A guide to LIDAR data acquisition and processing for the forests of the Pacific Northwest.pdf:pdf},
journal = {General Technical Report PNW-GTR-768},
keywords = {DEM,FIA,LIDAR,Pacific Northwest,absolute and relative accuracy,canopy surface,data quality assessment,data storage,forest inventory,laser,precision,registration,resolution,scanning.,stand penetration,topography},
number = {July},
pages = {1--40},
title = {{A Guide to LIDAR Data Acquisition and Processing for the Forests of the Pacific Northwest}},
url = {https://www.fs.usda.gov/treesearch/pubs/30652 http://www.arlis.org/docs/vol1/A/276932054.pdf},
volume = {768},
year = {2008}
}
@article{Velez2017,
author = {Velez, Gorka and Otaegui, Oihana},
doi = {10.1049/iet-its.2016.0026},
issn = {1751-956X},
journal = {IET Intelligent Transport Systems},
month = {apr},
number = {3},
pages = {103--112},
title = {{Embedding vision-based advanced driver assistance systems: a survey}},
url = {http://digital-library.theiet.org/content/journals/10.1049/iet-its.2016.0026},
volume = {11},
year = {2017}
}
@article{DePonteMuller2017,
abstract = {Future driver assistance systems will rely on accurate, reliable and continuous knowledge on the position of other road participants, including pedestrians, bicycles and other vehicles. The usual approach to tackle this requirement is to use on-board ranging sensors inside the vehicle. Radar, laser scanners or vision-based systems are able to detect objects in their line-of-sight. In contrast to these non-cooperative ranging sensors, cooperative approaches follow a strategy in which other road participants actively support the estimation of the relative position. The limitations of on-board ranging sensors regarding their detection range and angle of view and the facility of blockage can be approached by using a cooperative approach based on vehicle-to-vehicle communication. The fusion of both, cooperative and non-cooperative strategies, seems to offer the largest benefits regarding accuracy, availability and robustness. This survey offers the reader a comprehensive review on different techniques for vehicle relative positioning. The reader will learn the important performance indicators when it comes to relative positioning of vehicles, the different technologies that are both commercially available and currently under research, their expected performance and their intrinsic limitations. Moreover, the latest research in the area of vision-based systems for vehicle detection, as well as the latest work on GNSS-based vehicle localization and vehicular communication for relative positioning of vehicles, are reviewed. The survey also includes the research work on the fusion of cooperative and non-cooperative approaches to increase the reliability and the availability.},
author = {{de Ponte M{\"{u}}ller}, Fabian and Fabian},
doi = {10.3390/s17020271},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de Ponte M{\"{u}}ller, Fabian - 2017 - Survey on Ranging Sensors and Cooperative Techniques for Relative Positioning of Vehicles.pdf:pdf},
issn = {1424-8220},
journal = {Sensors},
keywords = {GNSS,cooperative,laser scanner,localization,relative positioning,to,vehicle,vehicle sensors},
month = {jan},
number = {2},
pages = {271},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Survey on Ranging Sensors and Cooperative Techniques for Relative Positioning of Vehicles}},
url = {http://www.mdpi.com/1424-8220/17/2/271},
volume = {17},
year = {2017}
}
@book{Broggi1999,
author = {Broggi, Alberto. and Bertozzi, Massimo and Fascioli, Alessandra and Conte, Gianni},
booktitle = {World Scientific Co., Singapore},
isbn = {9810237200},
pages = {242},
publisher = {World Scientific},
title = {{Automatic Vehicle Guidance : the Experience of the ARGO Autonomous Vehicle}},
url = {http://ftp.utcluj.ro/pub/docs/imaging/Autonomous{\_}driving/Articole sortate/Lazar Mircea/AutoDriving2/parma/www.ce.unipr.it/people/broggi/publications/argo.pdf},
year = {1999}
}
@article{McManamon1996,
abstract = {{Optical phased arrays represent an enabling new technology that makes possible simple affordable, lightweight, optical sensors offering very precise stabilization, random-access pointing programmable multiple simultaneous beams, a dynamic focus/defocus capability, and moderate to excellent optical power handling capability. These new arrays steer or otherwise operate on an already formed beam. A phase profile is imposed on an optical beam as it is either transmitted through or reflected from the phase shifter array. The imposed phase profile steers, focuses, fans out, or corrects phase aberrations on the beam. The array of optical phase shifters is realized through lithographic patterning of an electrical addressing network on the superstrate of a liquid crystal waveplate. Refractive index changes sufficiently large to realize full-wave differential phase shifts can be effected using low ({\textless}10 V) voltages applied to the liquid crystal phase plate electrodes. High efficiency large-angle steering with phased arrays requires phase shifter spacing on the order of a wavelength or less; consequently addressing issues make 1-D optical arrays much more practical than 2-D arrays. Orthogonal oriented 1-D phased arrays are used to deflect a beam in both dimensions. Optical phased arrays with apertures on the order of 4 cm by 4 cm have been fabricated for steering green, red, 1.06 $\mu$m, and 10.6 $\mu$m radiation. System concepts that include a passive acquisition sensor as well as a laser radar are presented{\}}, keywords={\{}aberrations;arrays;liquid crystal devices;lithography;optical radar;optical sensors;phase shifters;phased array radar;refractive index;1.06 micrometre;10 V;10.6 micrometre;1D optical arrays;dynamic focus/defocus capability;electrical addressing network;full-wave differential phase shifts;large-angle steering;laser radar;liquid crystal waveplate;lithographic patterning;optical phased array technology;optical power handling capability;optical sensors;passive acquisition sensor;phase aberrations;phase profile;phase shifter array;programmable multiple simultaneous beams;random-access pointing;refractive index changes;Laser radar;Liquid crystals;Optical arrays;Optical beams;Optical refraction;Optical sensors;Optical variables control;Phase shifters;Phased arrays;Sensor arrays}},
author = {Mcmanamon, Paul F. and Dorschner, Terry A. and Corkum, David L. and Friedman, Larry J. and Hobbs, Douglas S. and Holz, Michael and Liberman, Sergey and Nguyen, Huy Q. and Resler, Daniel P. and Sharp, Richard C. and Watson, Edward A. and Dorschner, T. A. and Friedman, L. J. and Hobbs, D. S. and Holz, M. and Resler, D. P. and Sharp, R. C.},
doi = {10.1109/5.482231},
issn = {00189219},
journal = {Proceedings of the IEEE},
month = {feb},
number = {2},
pages = {268--298},
title = {{Optical phased array technology}},
url = {http://ieeexplore.ieee.org/document/482231/},
volume = {84},
year = {1996}
}
@article{Yebes2015,
abstract = {Driver assistance systems and autonomous robotics rely on the deployment of several sensors for environment perception. Compared to LiDAR systems, the inexpensive vision sensors can capture the 3D scene as perceived by a driver in terms of appearance and depth cues. Indeed, providing 3D image understanding capabilities to vehicles is an essential target in order to infer scene semantics in urban environments. One of the challenges that arises from the navigation task in naturalistic urban scenarios is the detection of road participants (e.g., cyclists, pedestrians and vehicles). In this regard, this paper tackles the detection and orientation estimation of cars, pedestrians and cyclists, employing the challenging and naturalistic KITTI images. This work proposes 3D-aware features computed from stereo color images in order to capture the appearance and depth peculiarities of the objects in road scenes. The successful part-based object detector, known as DPM, is extended to learn richer models from the 2.5D data (color and disparity), while also carrying out a detailed analysis of the training pipeline. A large set of experiments evaluate the proposals, and the best performing approach is ranked on the KITTI website. Indeed, this is the first work that reports results with stereo data for the KITTI object challenge, achieving increased detection ratios for the classes car and cyclist compared to a baseline DPM.},
author = {Yebes, J. Javier and Bergasa, Luis M. and Garc{\'{i}}a-Garrido, Miguel {\'{A}}ngel},
doi = {10.3390/s150409228},
isbn = {14248220},
issn = {14248220},
journal = {Sensors (Basel, Switzerland)},
keywords = {2.5D},
mendeley-tags = {2.5D},
month = {apr},
number = {4},
pages = {9228--9250},
pmid = {102279770},
title = {{Visual Object Recognition with 3D-Aware Features in KITTI Urban Scenes}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25903553 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4431302 http://www.mdpi.com/1424-8220/15/4/9228/},
volume = {15},
year = {2015}
}
@inproceedings{Broggi2011,
abstract = {Obstacle detection by means of stereo-vision is a fundamental task in computer vision, which has spurred a lot of research over the years, especially in the field of vehicular robotics. The information provided by this class of algorithms is used both in driving assistance systems and in autonomous vehicles, so the quality of the results and the processing times become critical, as detection failures or delays can have serious consequences. The obstacle detection system presented in this paper has been extensively tested during VIAC, the VisLab Intercontinental Autonomous Challenge [1], [2], which has offered a unique chance to face a number of different scenarios along the roads of two continents, in a variety of conditions; data collected during the expedition has also become a reference benchmark for further algorithm improvements.},
author = {Broggi, Alberto and Buzzoni, Michele and Felisa, Mirko and Zani, Paolo},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6048211},
isbn = {9781612844541},
issn = {2153-0858},
month = {sep},
pages = {1599--1604},
publisher = {IEEE},
title = {{Stereo obstacle detection in challenging environments: The VIAC experience}},
url = {http://ieeexplore.ieee.org/document/6094535/},
year = {2011}
}
@misc{EULawandPublications2004,
booktitle = {EU Law and Publications},
pages = {2},
title = {{2004/545/EC: Commission Decision of 8 July 2004 on the harmonisation of radio spectrum in the 79 GHz range for the use of automotive short-range radar equipment in the Community}},
url = {https://publications.europa.eu/en/publication-detail/-/publication/9d425670-b54b-4c65-8461-824dbf71facc/language-en},
urldate = {2018-07-19},
year = {2004}
}
@article{Chia-KaiLiang2008,
abstract = {Due to the sequential-readout structure of complementary metal-oxide semiconductor image sensor array, each scanline of the acquired image is exposed at a different time, resulting in the so-called electronic rolling shutter that induces geometric image distortion when the object or the video camera moves during image capture. In this paper, we propose an image processing technique using a planar motion model to address the problem. Unlike previous methods that involve complex 3-D feature correspondences, a simple approach to the analysis of inter- and intraframe distortions is presented. The high-resolution velocity estimates used for restoring the image are obtained by global motion estimation, BEzier curve fitting, and local motion estimation without resort to correspondence identification. Experimental results demonstrate the effectiveness of the algorithm.},
author = {Liang, Chia Kai and Chang, Li Wen and Chen, Homer H.},
doi = {10.1109/TIP.2008.925384},
isbn = {1057-7149 (Print)},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Complementary metal-oxide semiconductor (CMOS) sen,Motion analysis,Rolling shutter},
month = {aug},
number = {8},
pages = {1323--1330},
pmid = {18632342},
title = {{Analysis and compensation of rolling shutter effect}},
url = {http://ieeexplore.ieee.org/document/4549748/},
volume = {17},
year = {2008}
}
@phdthesis{Montemerlo2003a,
abstract = {The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on a factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and real-world data.},
annote = {NULL},
author = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
doi = {10.1.1.16.2153},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Montemerlo et al. - 2003 - FastSLAM A factored solution to the simultaneous localization and mapping problem.pdf:pdf},
isbn = {0262511290},
school = {Carnegie Mellon University},
title = {{FastSLAM: A factored solution to the simultaneous localization and mapping problem}},
url = {https://www.ri.cmu.edu/pub{\_}files/pub4/montemerlo{\_}michael{\_}2003{\_}1/montemerlo{\_}michael{\_}2003{\_}1.pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:FastSLAM+:+A+Factored+Solution+to+the+Simultaneous+Localization+and+Mapping+Problem{\#}0},
year = {2003}
}
@article{Thrun2006,
abstract = {This article describes the robot Stanley, which won the 2005 DARPA Grand Challenge. Stanley was developed for high-speed desert driving without manual intervention. The robot's software system relied predominately on state-of-the-art artificial intelligence technologies, such as machine learning and probabilistic reasoning. This paper describes the major components of this architecture, and discusses the results of the Grand Challenge race.},
author = {Thrun, Sebastian and Montemerlo, Mike and Dahlkamp, Hendrik and Et, Al.},
doi = {10.1002/rob.20147},
file = {::},
issn = {1556-4967},
journal = {Journal of Field Robotics},
number = {9},
pages = {661--692},
title = {{The robot that won the DARPA Grand Challenge.}},
url = {www.interscience.wiley.com http://doi.wiley.com/10.1002/rob.20147},
volume = {23-9},
year = {2006}
}
@inproceedings{Polychronopoulos2006,
abstract = {The question raised in this paper, for the first time, is how the JDL model can be applied in multi-sensor automotive safety systems, since new sensors are integrated on-board, while new functions support the driver, intervene and control the vehicle. The paper proposes a hybrid hierarchical structure and develops a suitable functional model, namely the ProFusion2 (PF2) model; PF2 serves the broad automotive sensor data fusion community as a conceptual framework of common understanding and it provides recommendations and guidelines for implementation of fusion systems. Reference implementations are given as complete examples from the major automotive research initiative in Europe (PReVENT project).},
author = {Polychronopoulos, A. and Amditis, A. and Scheunert, U. and Tatschke, T.},
booktitle = {2006 9th International Conference on Information Fusion, FUSION},
doi = {10.1109/ICIF.2006.301681},
file = {:E$\backslash$:/publicaciones/2018{\_}review{\_}sensores{\_}percepcion{\_}ADAS/doc/polychronopoulos2006.pdf:pdf},
isbn = {1424409535},
issn = {1-4244-0953-5},
keywords = {Environment model,JDL model,Object refinement,PF2 functional model,Situation refinement},
month = {jul},
pages = {1--7},
publisher = {IEEE},
title = {{Revisiting JDL model for automotive safety applications: The PF2 functional model}},
url = {http://ieeexplore.ieee.org/document/4085967/},
year = {2006}
}
@article{Maddalena2005,
abstract = {After penetrating over a decade the consumer and industrial world, digital imaging is slowly but inevitably gaining marketshare in the automotive world. Cameras will become a key sensor in increasing car safety, driving assistance and driving comfort. The image sensors for automotive will be dominated by CMOS sensors as the requirements are different from the consumer market or the industrial or medical markets. Dynamic range, temperature range, cost, speed and many others are key parameters that need to be optimized. For this reason, automotive sensors differ from the other market's sensors and need to use different design and processing techniques in order to achieve the automotive specifications. This paper will show how Melexis has developed two CMOS imagers to target the automotive safety market and automotive CMOS imagers in general.},
address = {Berlin/Heidelberg},
author = {Maddalena, S. and Darmon, A. and Diels, R.},
doi = {10.1007/3-540-27463-4_29},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maddalena, Darmon, Diels - 2005 - Automotive CMOS Image Sensors.pdf:pdf},
journal = {Advanced Microsystems for Automotive {\ldots}},
pages = {401--412},
publisher = {Springer-Verlag},
title = {{Automotive CMOS image sensors}},
url = {http://link.springer.com/10.1007/3-540-27463-4{\_}29 http://www.springerlink.com/index/M354109378G10242.pdf},
year = {2005}
}
@incollection{Ziebinski2016,
author = {Ziebinski, Adam and Cupek, Rafal and Erdogan, Hueseyin and Waechter, Sonja},
doi = {10.1007/978-3-319-45246-3_13},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ziebinski et al. - 2016 - A Survey of ADAS Technologies for the Future Perspective of Sensor Fusion.pdf:pdf},
month = {sep},
pages = {135--146},
publisher = {Springer, Cham},
title = {{A Survey of ADAS Technologies for the Future Perspective of Sensor Fusion}},
url = {http://link.springer.com/10.1007/978-3-319-45246-3{\_}13},
year = {2016}
}
@article{Llinas2000,
abstract = {This paper suggests refinements and extensions of the JDL Data Fusion Model, the standard process model used for a multiplicity of community purposes. However, this Model has not been reviewed in accordance with (a) the dynamics of world events and (b) the changes, discoveries, and new methods in both the data fusion research and development community and related IT technologies. This paper suggests ways to revise and extend this important model. Proposals are made regarding (a) improvements in the understanding of internal processing within a fusion node and (b) extending the model to include (1) remarks on issues related to quality control, reliability, and consistency in DF processing, (2) assertions about the need for co-processing of abductive/ inductive and deductive inferencing processes, (3) remarks about the need for and exploitation of an onto logicallybased approach to DF process design, and (4) extensions to account for the case of Distributed Data Fusion (DDF).},
author = {Llinas, James and Bowman, Christopher and Rogova, Galina and Steinberg, Alan},
isbn = {917056115X},
journal = {Space and Naval Warfare Systems Command},
keywords = {data fusion,fusion,information fusion,jdl model},
number = {7},
pages = {1--14},
title = {{Revisiting the JDL data fusion model II}},
url = {http://oai.dtic.mil/oai/oai?verb=getRecord{\&}metadataPrefix=html{\&}identifier=ADA525721},
volume = {1},
year = {2004}
}
@article{Vidal2017,
abstract = {Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this paper, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly-coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130{\%} over event-only pipelines, and 85{\%} over standard-frames-only visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate - to the best of our knowledge - the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visual-inertial odometry, such as low-light environments and high-dynamic range scenes.},
archivePrefix = {arXiv},
arxivId = {1709.06310},
author = {Vidal, Antoni Rosinol and Rebecq, Henri and Horstschaefer, Timo and Scaramuzza, Davide},
doi = {10.1109/LRA.2018.2793357},
eprint = {1709.06310},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vidal et al. - 2017 - Ultimate SLAM Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenarios.pdf:pdf},
issn = {2377-3766},
journal = {IEEE ROBOTICS AND AUTOMATION LETTERS},
number = {2},
title = {{Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenarios}},
url = {http://rpg.ifi.uzh.ch/docs/RAL18{\_}VidalRebecq.pdf http://arxiv.org/abs/1709.06310{\%}0Ahttp://dx.doi.org/10.1109/LRA.2018.2793357},
volume = {3},
year = {2017}
}
@article{Glennie2010,
abstract = {The static calibration and analysis of the Velodyne HDL-64E S2 scanning LiDAR system is presented and analyzed. The mathematical model for measurements for the HDL-64E S2 scanner is derived and discussed. A planar feature based least squares adjustment approach is presented and utilized in a minimally constrained network in order to derive an optimal solution for the laser's internal calibration parameters. Finally, the results of the adjustment along with a detailed examination of the adjustment residuals are given. A three-fold improvement in the planar misclosure residual RMSE over the standard factory calibration model was achieved by the proposed calibration. Results also suggest that there may still be some unmodelled distortions in the range measurements from the scanner. However, despite this, the overall precision of the adjusted laser scanner data appears to make it a viable choice for high accuracy mobile scanning applications.},
author = {Glennie, Craig and Lichti, Derek D.},
doi = {10.3390/rs2061610},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glennie, Lichti - 2010 - Static Calibration and Analysis of the Velodyne HDL-64E S2 for High Accuracy Mobile Scanning.pdf:pdf},
isbn = {2072-4292},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Accuracy,Error analysis,Lidar,System calibration},
month = {jun},
number = {6},
pages = {1610--1624},
publisher = {Molecular Diversity Preservation International},
title = {{Static calibration and analysis of the velodyne HDL-64E S2 for high accuracy mobile scanning}},
url = {http://www.mdpi.com/2072-4292/2/6/1610},
volume = {2},
year = {2010}
}
@article{Santana2016,
abstract = {Comma.ai's approach to Artificial Intelligence for self-driving cars is based on an agent that learns to clone driver behaviors and plans maneuvers by simulating future events in the road. This paper illustrates one of our research approaches for driving simulation. One where we learn to simulate. Here we investigate variational autoencoders with classical and learned cost functions using generative adversarial networks for embedding road frames. Afterwards, we learn a transition model in the embedded space using action conditioned Recurrent Neural Networks. We show that our approach can keep predicting realistic looking video for several frames despite the transition model being optimized without a cost function in the pixel space.},
archivePrefix = {arXiv},
arxivId = {1608.01230},
author = {Santana, Eder and Hotz, George},
eprint = {1608.01230},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santana, Hotz - 2016 - Learning a Driving Simulator.pdf:pdf},
month = {aug},
title = {{Learning a Driving Simulator}},
url = {http://arxiv.org/abs/1608.01230},
year = {2016}
}
@article{Chun2008,
abstract = {This paper focuses on the rolling shutter distortion of CMOS image sensor coming from its unique readout mechanism as the main cause for image degradation when there are fast-moving objects. This paper proposes a post image processing scheme based on motion vector detection to suppress the rolling shutter distortion. Motion vector detection is performed based on an optical flow method at a reasonable computational complexity. A practical implementation scheme is also described.},
author = {Chun, Jung Bum and Jung, Hunjoon and Kyung, Chong Min},
doi = {10.1109/TCE.2008.4711190},
issn = {00983063},
journal = {IEEE Transactions on Consumer Electronics},
keywords = {CMOS image sensor,Post-processing technique,Rolling-shutter distortion},
month = {nov},
number = {4},
pages = {1479--1487},
title = {{Suppressing rolling-shutter distortion of CMOS image sensors by motion vector detection}},
url = {http://ieeexplore.ieee.org/document/4711190/},
volume = {54},
year = {2008}
}
@article{Kishida2015,
abstract = {High-resolution millimeter-wave radar that operates in the 79 GHz band is expected to achieve a significant increase in the distance resolution of radar systems because of the availability of a wide frequency bandwidth of 4 GHz as compared with 0.5 GHz of the existing 77 GHz-band mil-limeter-wave radar. For this reason, it has the potential to distinguish between a vehicle and a human, which was conventionally difficult, and recognize their movements. Therefore it raises expectations for use as a surrounding monitoring radar in driving safety support and auto-matic driving. As one of FUJITSU TEN's efforts regarding sensing technologies for driving safety support and automatic driving, it has been developing 79 GHz-band high-resolution millimeter-wave radar. This paper presents specifications of radar for application to systems that assist in safe driving and automatic driving and the results of testing a prototype for a wider bandwidth that is required to accomplish the technology's purpose. This radar increases the ability to de-tect a pedestrian in the surroundings of a vehicle, which was difficult to do with the existing 77 GHz-band radar. Furthermore, this paper also describes how the newly developed radar of-fers the possibility of improving the performance of a sensor for automatic driving and systems to assist in safe driving.},
author = {Kishida, Masayuki and Ohguchi, Katsuyuki and Shono, Masayoshi},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kishida, Ohguchi, Shono - 2015 - 79 GHz-Band High-Resolution Millimeter-Wave Radar.pdf:pdf},
journal = {FUJITSU Sci. Tech. J},
number = {4},
pages = {55--59},
title = {{79 GHz-Band High-Resolution Millimeter- Wave Radar}},
url = {https://www.fujitsu.com/global/documents/about/resources/publications/fstj/archives/vol51-4/paper09.pdf},
volume = {51},
year = {2015}
}
@article{Reina2015,
abstract = {Radar overcomes the shortcomings of laser, stereovision, and sonar because it can operate successfully in dusty, foggy, blizzard-blinding, and poorly lit scenarios. This paper presents a novel method for ground and obstacle segmentation based on radar sensing. The algorithm operates directly in the sensor frame, without the need for a separate synchronised navigation source, calibration parameters describing the location of the radar in the vehicle frame, or the geometric restrictions made in the previous main method in the field. Experimental results are presented in various urban scenarios to validate this approach, showing its potential applicability for advanced driving assistance systems and autonomous vehicle operations.},
author = {Reina, Giulio and Johnson, David and Underwood, James},
doi = {10.3390/s150614661},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reina, Johnson, Underwood - 2015 - Radar Sensing for Intelligent Vehicles in Urban Environments.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Navigation systems,Perception in urban environment,Radar sensing,Robotic intelligent vehicles,Unmanned ground vehicles},
month = {jun},
number = {6},
pages = {14661--14678},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Radar sensing for intelligent vehicles in urban environments}},
url = {http://www.mdpi.com/1424-8220/15/6/14661/},
volume = {15},
year = {2015}
}
@misc{Lachner2009,
address = {Tokyo},
author = {Lachner, Rudolf},
publisher = {ITS Forum},
title = {{Development Status of Next Generation Automotive Radar in EU}},
url = {http://www.itsforum.gr.jp/Public/J3Schedule/P22/lachner090226.pdf},
year = {2009}
}
@inproceedings{Broggi1998,
abstract = {This paper presents and discusses the first results obtained by the GOLD (Generic Obstacle and Lane Detection) system as an automatic driver of ARGO. ARGO is a Lancia Thema passenger car equipped with a vision-based system that allows to extract road and environmental information from the acquired scene. By means of stereo vision, obstacles on the road are detected and localized, while the processing of a single monocular image allows to extract the road geometry in front of the vehicle. The generality of the underlying approach allows to detect generic obstacles (without constraints on shape, color, or symmetry) and to detect lane markings even in dark and in strong shadow conditions. The hardware system consists of a PC Pentium 200 Mhz with MMX technology and a frame-grabber board able to acquire 3 b/w images simultaneously; the result of the processing (position of obstacles and geometry of the road) is used to drive an actuator on the steering wheel, while debug information are presented to the user on an on-board monitor and a led-based control panel.},
author = {Broggi, Alberto and Bertozzi, Massimo and Fascioli, Alessandra and Conte, Gianni},
booktitle = {Enhanced and Synthetic Vision},
doi = {10.1117/12.317473},
editor = {Verly, Jacques G.},
isbn = {9810237200},
issn = {0277786X},
keywords = {algorithms and the architectures,argo is the experimental,autonomous vehicle,autonomous vehicle developed at,computer vision,conducted over the last,dell,few years on the,for vision based automatic,informazione of the,it integrates the main,italy,lane detection,obstacle detection,results of the research,road vehicles guidance,the dipartimento di ingegneria,university of parma},
month = {jul},
pages = {218--229},
publisher = {International Society for Optics and Photonics},
title = {{the Experience of the ARGO Autonomous Vehicle}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=947967 http://www.worldscientific.com/doi/pdf/10.1142/9789812814937{\_}bmatter},
volume = {3364},
year = {1998}
}
@inproceedings{Mueggler2014,
abstract = {In the last few years, we have witnessed impres- sive demonstrations of aggressive flights and acrobatics using quadrotors. However, those robots are actually blind. They do not see by themselves, but through the “eyes” of an external motion capture system. Flight maneuvers using onboard sensors are still slow compared to those attainable with motion capture systems. At the current state, the agility of a robot is limited by the latency of its perception pipeline. To obtain more agile robots, we need to use faster sensors. In this paper, we present the first onboard perception system for 6-DOF localization during high-speed maneuvers using a Dynamic Vision Sensor (DVS). Unlike a standard CMOS camera, a DVS does not wastefully send full image frames at a fixed frame rate. Conversely, similar to the human eye, it only transmits pixel-level brightness changes at the time they occur with microsecond resolution, thus, offering the possibility to create a perception pipeline whose latency is negligible compared to the dynamics of the robot. We exploit these characteristics to estimate the pose of a quadrotor with respect to a known pattern during high-speed maneuvers, such as flips, with rotational speeds up to 1,200 ◦/s. Additionally, we provide a versatile method to capture ground-truth data using a DVS},
author = {Mueggler, Elias and Huber, Basil and Scaramuzza, Davide},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2014.6942940},
isbn = {9781479969340},
issn = {21530866},
month = {sep},
pages = {2761--2768},
publisher = {IEEE},
title = {{Event-based, 6-DOF pose tracking for high-speed maneuvers}},
url = {http://ieeexplore.ieee.org/document/6942940/},
year = {2014}
}
@article{Montemerlo2008,
abstract = {This article presents the architecture of Junior, a robotic vehicle capable of navigating ur-ban environments autonomously. In doing so, the vehicle is able to select its own routes, perceive and interact with other traffic, and execute various urban driving skills including lane changes, U-turns, parking, and merging into moving traffic. The vehicle successfully finished and won second place in the DARPA Urban Challenge, a robot competition or-ganized by the U.},
author = {Montemerlo, Michael and Becker, Jan and Bhat, Suhrid and Dahlkamp, Hendrik and Dolgov, Dmitri and Ettinger, Scott and Haehnel, Dirk and Hilden, Tim and Hoffmann, Gabe and Huhnke, Burkhard and Johnston, Doug and Klumpp, Stefan and Langer, Dirk and Levandowski, Anthony and Levinson, Jesse and Marcil, Julien and Orenstein, David and Paefgen, Johannes and Penny, Isaac and Petrovskaya, Anna and Pflueger, Mike and Stanek, Ganymed and Stavens, David and Vogt, Antone and Thrun, Sebastian},
doi = {10.1002/rob.20258},
file = {::},
issn = {15564959},
journal = {Journal of Field Robotics},
month = {sep},
number = {9},
pages = {569--597},
publisher = {Wiley-Blackwell},
title = {{Junior: the Stanford entry in the Urban Challenge}},
url = {http://doi.wiley.com/10.1002/rob.20258 https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20258{\%}0Ahttp://dx.doi.org/10.1002/rob.20258},
volume = {25},
year = {2008}
}
