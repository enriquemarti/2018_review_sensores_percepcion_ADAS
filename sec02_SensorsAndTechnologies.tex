
This work is focused in exteroceptive sensors, leaving proprioceptive sensors
and communications out of the scope of the review.
Exteroception in Automated Driving is related with information in the
surroundings of the vehicle, as opposed to propioception that is related with 
the state of the vehicle itself (speed, accelerations, component integrity). 
%Propriceptive sensors and communications are out of the scope of this review.

Next subsections present the advantages, drawbacks and current challenges for 
the three principal sensor technologies for exteroceptive perception in
Automated Driving: artificial vision, radar and LiDAR. 
Each one is followed with a review of relevant emergent technologies in the
field.

After that, a taxonomy of information domains is presented. It is useful for
several purposes. First it allows to link sensors technologies with perception 
algorithms described in section \ref{sec:03-problemsapplications}, 
since the first provide the raw data needed by the second. 
Second, the categorization is used to structure a subsequent
analysis about the suitability and adequacy of the presented sensing 
technologies for perception in Automated Driving. 
This last part includes also the expected performance under different
environmental and weather conditions.

\subsection{Artificial Vision}
Artificial vision is a popular technology that has been used for decades in 
disciplines as mobile robotics, surveillance or industrial inspection. 
This technology offers interesting features, as the low cost of sensors 
--for most popular types-- and providing range of information types including
spatial (shape, size, distances), dynamic (motion of objects by analyzing their 
displacement between consecutive frames) and semantic (shape analysis).

Cameras available in the market offer a wide range of configurations in
resolution (from less than 0.25 to more than 40 Mpx), frame rate (up to
thousands of frames per second (FPS)), sensor size, and optics parameters.
However, Automated Driving poses some particular challenges to camera sensors
and artificial vision technology:

%\begin{itemize}
    %\item 
    \textbf{Varying light and visibility conditions.} Driving happens at day, 
    at night, indoors, or at dusk or dawn with the sun close to the horizon. 
    Dark spots, shadows, glares, reflections and other effects complicate the
    implementation of reliable artificial visible algorithms.
    Extending the capturing spectrum can solve some of these problems. 
    Far infrared (FIR) cameras (wavelength 900-1400 nm) 
    are effective for pedestrian and animal detection
    \cite{OMalley2008, Besbes2015}, in the dark and through dust and smoke.
    Near Infrared (NIR)(750-900 nm) complements visible spectrum with a better
    contrast in high dynamic range scenes, and better night visibility. 
    In \cite{Pinchon2018} authors compare visible light, NIR and FIR cameras 
    under different light and atmospheric conditions.
    
%    \item 
\textbf{Scenes with a High Dynamic Range (HDR)} contain dark and strongly
    illuminated areas in the same frame, as entering or exiting a tunnel.
    Common sensor technologies have single shot dynamic range of 60-75 dB,
    which cause a loss of information in the extremes (under- or overexposure).
    In 2017 Sony launched a 120 dB automotive sensor and 2k resolution.
    An automotive grade sensor combining HDR capabilities and NIR
    light detection is analyzed in \cite{Maddalena2005} and the work 
    \cite{Strobel2013} presents a sensor with 130/170 dB range (global/rolling 
    shutter configurations).
    % is presented for industrial safety application.
        
%    \item High speed objects: an object moving at high speed can appear blurred
%    if the exposure time of the camera is too long. This effect can be
%    corrected by using sensors 
%    requires short exposure times cameras need higher exposures time as
%    illumination is weaker. Fast moving elements appear blurred, which can 
%    affect later processes as border or feature detection. Also, if the sensor
%    does not capture light in its full surface simultaneously (rolling shutter)
%    distortion effects can appear in those objects. 
%    High sensitivity sensors and global shutter cameras can mitigate these 
%    problems. In \cite{Chia-KaiLiang2008}\cite{Chun2008} rolling shutter 
%    effects are corrected by a motion compensation software.
%\end{itemize}

A more extensive review of camera and sensor problems can be found in  
\cite{Pueo2016}, from the perspective of recording scenes in sports.
%In order to deal with these difficulties, different technologies and solutions 
%have been proposed. 
%
%\begin{itemize}    
%    \item High Dynamic Range imaging (HDR): common sensors in photographic and 
%    industrial cameras offer a dynamic range of 60-75 dB, while
%    mixed illumination environments require at least 120 dB.
%    An automotive grade sensor combining HDR capabilities and Near Infra-Red
%    light detection is analyzed in \cite{Maddalena2005}. In 2017 Sony launched
%    a 120 dB automotive sensor and 2k resolution.
%    In \cite{Strobel2013} a sensor with 130 dB range (global shutter) and up
%    to 170 dB (rolling shutter) is presented for industrial safety application.
%    
%    \item Global shutter sensors and Rolling shutter compensation. In rolling
%    shutter cameras the elements of the sensor capture light at different time 
%    intervals, causing moving objects to appear distorted and creating
%    half-dark-half-light frames in scenes illuminated by flickering lights
%    (LEDs, fluorescents). 
%    This effects can be corrected by a motion compensation software 
%    \cite{Chia-KaiLiang2008}\cite{Chun2008}. Global shutter cameras, on the
%    other hand, have the ability to capture light in all the elements of the 
%    sensor simultaneously. 
%        
%    \item Captured spectrum: Far infrared cameras (wavelength 900-1400 nm, also
%    called thermal cameras) are effective for pedestrian and animal detection
%    \cite{OMalley2008}\cite{Besbes2015} in the dark and through dust and smoke.
%    Near Infrared (750-900 nm) complements visible spectrum with a better
%    contrast in high dynamic range scenes, and better night visibility. 
%    In \cite{Pinchon2018} authors compare visible light, near infrared and far
%    infrared cameras in different light and atmospheric conditions.
%    
%\end{itemize} 

\subsubsection{3D technology}
Traditional camera technology is essentially 2D, but there are some
types of vision sensors that can perceive depth information. This section
describes the three principal types that are already available as commercial
devices, although not always targeting the automotive market.

\textbf{Stereo vision.} Depth is calculated \cite{Hamzah2016} from the 
apparent displacement of visual features in the images captured by two 
carefully calibrated monocular cameras pointing in the same direction and
separated by some distance (known as baseline). 
            
One of the greatest advantages of stereo vision systems is their capability 
to provide dense depth maps, as opposed to sparse sensors (e.g. LiDARs).  
Stereo vision drawbacks include issues with low-textured patterns 
(e.g. solid colors) that difficult establishing correspondences between
frames.
%Also, it has a high computational complexity, and the pair of 
%cameras requires a careful calibration to ensure proper translation of 
%disparities to depths.

Monocular SLAM (Simultaneous Location And Mapping) algorithms share some of the
working principles of stereo system: the motion of a single monocular camera 
creates an artificial baseline between consecutive frames, from which depth and
camera motion are estimated.
Some works as \cite{Engel2014, Engel2018} represent a good alternative to
stereo sensors for location and mapping. 
    
\textbf{Structured light.} A monocular camera coupled with a device that
illuminates the scene with a known pattern of infrared light. 
Irregular surfaces produce an apparent distortion of the light pattern, that is
captured by the camera and translated to a depth map.

Structured light devices overcome some limitations of stereoscopic systems:
they do not depend on textured surfaces and have a lower computational cost. 
However, they require the same high-accuracy calibration \cite{Garbat2013}
and its operative range (usually below 20 meters) is limited by the power of
the emitter and the intensity of ambient light. Reflections can affect its
performance.

%    This technology can be combined with stereo vision to improve robustness
%    and operative conditions, as in the commercial device Intel RealSense 
%D4500.

%    \item \emph{Time-of-flight:} 
\textbf{Time-of-flight.} Is an active sensing technology 
\cite{Hansard2013} based in the same round-trip-time principle 
of LiDAR sensors (see \ref{sec:02-c-lidar}): an emitter composed of infrared 
LEDs floods the scene with modulated light that is captured by the sensor after
being reflected by elements in the environment. 
The round-trip-time can be calculated for each pixel based on the phase shift 
of incoming light, which is then translated to a distance.

Using a non-directed source of light (as opposed to the low divergence laser
emitter in LiDAR) has advantages as the ability to create dense depth maps and
a high refresh rate exceeding 50 Hz. However, its operative range is short for
automotive applications (10-20 meters) and has problems working under intense
ambient light. 
Some research lines as indirect time-of-flight \cite{Villa2017}, pulsed light
time-of-flight or avalanche photodiodes \cite{Panasonic2018} could increase
working range to 50-250 meters.
    
%\end{itemize}


\subsubsection{Emerging vision technologies}

In event-based vision the elements of the sensor (pixels) are triggered 
asynchronously and independently when they detect a change on light intensity 
(an \emph{event}).
%An event can be seen as something similar to the output of a feature detection 
%algorithm for artificial vision applications.
The sensor produce a stream of events that can be grouped in time windows for
getting a frame-like image. 
Independence of sensor elements raises the dynamic range of the sensor to 
120 dB, allowing high speed applications in low light conditions. 
\cite{Mueggler2014} shows tracking at 1000 FPS under regular indoor lightning 
conditions, although the sensor works in sub-microsecond time scales.
Events can be the input to visual odometry \cite{Censi2014} and SLAM
\cite{Vidal2017} applications, relieving the CPU of time consuming operations
on raw images. 
%A steering wheel control for Automated Driving systems 
%based on deep learning has been shown in \cite{Maqueda2018}.

%Light polarization represents an additional source of information which
%is know to be used by animals able to perceive it (some ants, 
%mantis-shrimp). 
There is an active line of research \cite{Garcia2018} around sensors 
capturing light polarization, which perform consistently under adverse
meteorological conditions and provide exotic types of information (e.g.
materials, composition, water in the road).
 

\subsection{Radar}

Radar technology use high frequency electromagnetic waves to measure the
distance to objects based on the \emph{round-trip time} principle, which is the
time it takes the wave to reach the object, bounce on it and travel back to the
sensor. 
 
%There are two principal technologies: pulsed radar and Frequency-Modulated
%Continuous Wave (FMCW). Pulsed radar emits a short burst and measures distance
%using the raw round-trip time.
Most modern automotive radars are based on the Frequency-Modulated Continuous
Wave (FMCW) technology, and use digital beamforming \cite{Hasch2015} to control
the direction of the emitted wave. 
FMCW consists on emitting a signal with a well known and stable frequency that
is modulated with another continuous signal that varies its frequency up and
down (typically using a triangular shape). Distance is
determined using the frequency shift between the emitted and reflected signals. 
Radars also exploit Doppler effect to get a direct observation of the relative
speed of the target with respect to the sensor. 

%Many modern automotive radars use a technique known as digital beamforming
%\cite{Hasch2015} to direct the emitted wave in the desired direction.

%Starting in 2004, EU allocated a permanent 5 GHz wide band around 79 GHz
%\cite{EULawandPublications2004}. Short distance applications as blind spot
%detection, parking assistance, jam assistance and pre-crash measures use the 
%upper part (77-81 GHz), since it offers a better resolution. Long distance 
%applications as ACC use a radar signal around 76-78 GHz. 
%Most modern automotive radars use FMCW technology in multi-frequency chips 
%that 
%can switch between different bands and functions dynamically.

One of the strongest arguments for including radar sensing in automated 
vehicles is its independence of light and weather conditions. 
It works in the dark, and detections are almost equally good with snow, 
rain, fog or dust \cite{Reina2015}. Long range radars can see up to 250 m
in very adverse conditions, where no other sensor works.

Radar sensors present some difficulties and drawbacks:

%\begin{itemize}
    \textbf{Sensible to target reflectivity.} Processing radar data is a tricky
    task, due in part to the heterogeneous reflectivity of the different 
    materials. 
    Metals amplify radar signal, easing detection of vehicles but increasing
    the apparent size of small objects as discarded cans in the road, while 
    other materials (e.g. wood) are virtually transparent.
    This can cause false positives (detect a non existing obstacle) and false
    negatives (not detecting an actual obstacle).
    
    \textbf{Resolution and accuracy.} Radars are very accurate measuring 
    distance
    and speed along the line that connects the sensor with a target. However, 
    horizontal resolution depends on the characteristics of the emitted beam.
    Raw angular resolution in digital beamforming systems falls between 2 to 5
    degrees \cite{Schneider2005}, that can be improved to 0.1-1 degrees using 
    advanced 
    processing techniques \cite{Kissinger2012}. 
    With this angular resolution, it can be difficult to separate (detect as
    independent targets) a pedestrian from a nearby car at 30 m distance. 
    At 100 m distance it can be impossible to separate vehicles in neighbor
    lanes, determine if a vehicle is in our same lane, and even if a detection
    is a vehicle or a bridge over the road.
%\end{itemize}

\subsubsection{Emerging radar technologies}

One of the most active research area is related with high resolution radar
imaging for automobiles. Apart from benefits in target tracking and object
separation, a higher resolution can get richer semantic information and enable
further applications as target classification and environment mapping. 
An example can be found in \cite{Reina2015}, where a 90GHz rotating radar in
the roof of a car is used to map the environment, including vehicles, static
objects and ground.
The paper \cite{Kohler2013} demonstrates the feasibility of radars operating
between 100 and 300 GHz, analyzing atmospheric absorption and reflectivity of
materials usually found in driving scenarios.
%For example, Arbe Robotics \cite{ArbeRobotics2018} is working in a 
%model with 300 m range, a field of view of 100 degrees horizontal, 30 degrees
%vertical, and a resolution of 1 degree azimuth and 2 degrees elevation. 
%This model is expected to generate a full 4D (3D position plus speed) image of
%the scene at 25-50 Hz, thanks to the embedded machine learning and SLAM
%algorithms.

One of the key technologies that can lead to high resolution radar imaging are 
meta-material based antennas \cite{Brookner2016,Sleasman2017} for efficient
synthetic aperture radars. 
Some manufacturers as Metawave 
%\footnote{https://www.metawave.co/} 
%\cite{Metawave2018} 
are starting to offer products oriented to automotive sector based on the 
technology.

%In a different line, ground-penetrating radar is a technology used long
%time ago in diverse areas ranging from archeology to industrial applications.
%MIT Lincoln Laboratory created a device \cite{Cornick2016} that can be placed 
%below a vehicle to get a reading describing the geological properties of the
%first few meters under the ground. This reading is not affected by snow, 
%water, dust or any other element over the surface of the road. The idea is to
%create maps that can be used later to localize vehicles, with an accuracy of
%a few centimeters. %\cite{XXXX}
%Recently, Wavesense \cite{WaveSense} announced tests in snowy places and is
%close to commercialization stage.


\subsection{LiDAR}
\label{sec:02-c-lidar}
LiDAR (Light Detection And Ranging) is an active ranging technology that 
calculates distance to objects by measuring round-trip time of a laser light 
pulse.
Sensors for robotic and automotive applications use a low power 
NIR laser (900-1050 nm) that is invisible and eye-safe. 
Laser beams have a low divergence for reducing power decay with distance,
allowing to measure distances up to 200 m under direct sunlight.
Typically, a rotating mirror is used to change the direction of the laser 
pulse, reaching 360 degree horizontal coverage. 
Commercial solutions use an array of 
emitters to produce several vertical layers (between 4 and 128). This generates
a 3D point cloud representing the environment.
LiDAR sensors are a good choice for creating accurate digital maps, because
of their high accuracy measuring distances which averages
a few millimeters error in most cases and degrading to 0.1-0.5 meters in the 
worse. % This makes LiDAR a good choice for creating accurate digital maps.
However, they have several drawbacks to take into account.

%\begin{itemize}    
    \textbf{Low vertical resolution.} In low cost models, which usually feature 
    less than 16 layers, vertical resolution (separation between consecutive
    layers) falls down to 2 degrees. At 100 m distance, this is translated into 
    a vertical distance of 1.7 m. High end models reduce this to 0.2-0.4 
    degrees, but at a much higher cost.
    
    \textbf{Sparse measures (not dense).} 
    %according to \cite{Gatziolis2008} typical LiDAR beam divergence is between 
    %0.1 and 1 mrad (0.005 to 0.05 degrees).
    Commercial device Velodyne HDL64 has a 2 mrad divergence \cite{Glennie2010} 
    (0.11 degrees) and a vertical resolution of 0.42 degrees. At 50 meters 
    distance, the 0.3 degree gap between layers is equivalent to a blind strip
    0.26 meters tall. In low end devices (Velodyne VLP16) this gap grows to 1.5
    meters. Small targets can remain undetected, and structures based on
    wires and bars are virtually invisible.
    
    \textbf{Poor detection of dark and specular objects.} Black cars
    can appear as invisible to the LiDAR, since they combine a color that
    absorbs most radiation with a non-Lambertian material that does not scatter
    radiation back to receiver.
    
%    \item High cost. High-end models have a cost between 25 and 75 k\$.
%    Just a year ago, Velodyne models started at 9000 US\$ for the basic 16
%    layer device, although they have cut prices down to a 50\% to face the
%    recently appeared competitors. Now, some companies are selling
%    equivalent models for less than 4000 US\$. Solid state lidars promise
%    prices an order of magnitude smaller.
    
    \textbf{Affected by weather conditions.} NIR laser
    beams are affected by rain and fog because water droplets scatter the light 
    \cite{Wang2008}, reducing its operative range and producing false measures 
    in the front of the cloud. The effect of dust has been
    explored in \cite{Phillips2017}. LiDAR performance in these scenarios is 
    worse than radar, but still better than cameras and human eye.
%\end{itemize}

\subsubsection{Emerging LiDAR technologies}
\label{sec:03-lidar-emerging}

%Direct speed measurement is a really useful feature for any sensor. Up to date,
%only radars where able to capture speed, for instance using FMCW signals.

FMCW LiDAR \cite{Nordin2004} emits light continuously to measure objects speed
based on Doppler effect. In the last years some research prototypes suitable for
the automotive market start appearing \cite{Poulton2016}.
%, until recently
%a company named Blackmore announced a commercial version. 
Apart from improving target tracking capabilities, observation of speed can
be useful to enhance activity recognition and behavior prediction, for example 
by detecting the different speeds of limbs and body in cyclists and pedestrians.

%Regarding LiDARs, however, the most popular emerging technology in the last 
%years has been Solid State LiDAR. It offers advantages when it comes
%to operating under strong vibrations and dynamics, apart from being potentially
%smaller, cheaper and faster. 
%However, the market does not offer products combining high resolution and a 
%wide field of view, so mechanical devices are the only option for full 
%360-degree coverage and environment mapping.

Solid state LiDAR is an umbrella term that includes several technologies, two 
of which are oscillating micro-mirrors and Optical Phased Array (OPA).
The first technology directs laser beams using micro-mirrors that can 
rotate around two axes. Manufacturer LeddarTech commercializes devices
based on this technology \cite{LeddarTech2016}.
Optical phased arrays \cite{McManamon1996} is a technology similar to that used 
for EBF radars 
%An array of optical emitters generate coherent signals with a well
%controlled phase difference. This generates a far-field radiation pattern 
%pointing in a direction that depends on the phase. 
that allows to control the direction of the beam with high accuracy and speed.
Quanergy \cite{Eldada2017} is one of the few manufacturers commercializing
devices based on this technology.
% with a focus on the automotive sector, its S3
%model features a 120 degrees FoV with a range of 150 m. 
%They report a spot size of 5 cm at 100 m, that is  comparable to a beam
%divergence of 0.5 mrad --4 times smaller than Velodyne HDL64 model.

OPA technology can apply random-access scan patterns over the entire FoV 
(Field of View). This allows observing only specific regions of interest, and
change beam density (resolution) dynamically. 
These features can be combined to do fast inspection of the full FoV with low
resolution, and then tracking objects of interest with a higher resolution for 
enhanced shape recognition even at far distances.
%This is similar to Waymo's claims about the LiDARs they have developed for 
%their self-driving vehicles, as described in section 
%\ref{sec:04-relevantdemos}.

\subsection{Relevant information domains}
\label{sec:03-d-information-domains}

The task of a perception system is to bridge the gap between sensors providing 
data and decision algorithms requiring information.
A classical differentiation between both terms is the following: data is 
composed by raw, unorganized facts that need to be processed, while  
information is the name given to data that has been processed, organized, 
structured and presented in a proper context.

\renewcommand{\arraystretch}{1.1}
\begin{table}[b] %[H]
    \caption{Information taxonomy in Automated Driving domain}
    \label{tab:info-taxonomy}
    \begin{tabular*}{\linewidth}{lrp{5.8cm}} %{\textwidth}{lrL}
        \hline %\toprule
        \textbf{Category} & \textbf{\#}	& \textbf{Information type}	\\
        \hline %\midrule
        \multirow{2}{*}{Ego-vehicle}
        & 1 & Kinematic/dynamic (includes position) \\
        & 2 & Proprioceptive (components health/status) \\
        \hline %\midrule
        \multirow{3}{*}{Occupants}
        & 3 & Driver awareness/capacities \\
        & 4 & * Driver intentions (mind model)  \\
        & 5 & Passenger status (needs, risk factors) \\
        \hline %\midrule
        \multirow{4}{*}{Environment}
        & 6 & Spatial features: location, size, shape, fine features 
        \\
        & 7 & Identification: class, type, identity \\
        & 8 & Semantic features: signs, road marks, regulation \\
        & 9 & Contextual factors: weather, driving situation (e.g. jam, 
        off-road, emergency) \\
        \hline %\midrule
        \multirow{4}{*}{External actors}
        & 10 & Spatial features: location, size, shape, fine features  \\
        & 11 & Kinematic/dynamic: position, motion \\
        & 12 & Identification: class, type, identity \\ 
        & 13 & Semantic features: vehicle lights, pedestrian clothes, gestures 
        \\
        & 14 & * Situational engagement: collaborative/aware 
        (adults, other vehicles) vs non-collaborative/unaware 
        (animals, children) \\ 
        \hline %\bottomrule
    \end{tabular*}
\end{table}

Table \ref{tab:info-taxonomy} presents a taxonomy tightly related with 
the goals of perception stage (section \ref{sec:03-problemsapplications}). 
It allows to present conclusions about the suitability of sensor technologies 
for different perception tasks in a clear and organized way.
Elements marked with an asterisk are derived information that can 
be inferred from sensed data but not directly observed. It is mostly related 
with internal state of external entities, as the intentions of human beings and 
animals.

\subsection{Using sensors for perception}
\label{sec:03-e-sensors-for-perception}

Sensor selection and arrangement is one of the most important aspects in the 
design of a perception system for Automated Vehicles. It has a great impact
in its cost, with some setups having several times the price of the rest of 
the vehicle. 
This epigraph summarizes two aspects of the uttermost importance: type of 
information acquired and impact of environmental factors. For an analysis of
spatial coverage and range see \cite{Schoettle2017}.

\begin{figure}%[b]
    \centering
    \includegraphics[width=0.95\linewidth]{"img/information_types_sensors_"} 
    %img/information_types_sensors"}
    \caption{Sensor adequacy for relevant types of information}
    \label{fig:information_vs_sensors}
\end{figure}
\begin{figure}[b]
    \centering
    \includegraphics[width=0.68\linewidth]{"img/sensors_atmospheric_conditions_"} 
    %img/sensors_atmospheric_conditions"}
    \caption{Sensor robustness under atmospheric and environmental factors}
    \label{fig:sensors-environ}
\end{figure}

The characteristics of a sensing technology determines its 
suitability for acquiring certain types of information, and restricts its range 
of operative conditions.
Figure \ref{fig:information_vs_sensors} relates the principal sensing 
technologies currently used in the automotive market and Automated Driving
initiatives with relevant types of information identified in Table 
\ref{tab:info-taxonomy}. The adequacy of a sensor for acquiring a certain type
of information (or equivalently, the expected quality of that type of
information when captured by that sensing technology) is classified in three
levels: Good (green shading, tick), Medium (yellow shading, letter M) and Bad
(red shading, letter B).

Sensors and perception are expected to work uninterruptedly during vehicle 
operation. Weather and other environmental factor can degrade sensor
performance, but each technology is affected in a different way. 
Figure \ref{fig:sensors-environ} summarizes the effect of common external
factors in the performance of the analyzed sensing technologies, using the
same notation as Figure \ref{fig:information_vs_sensors}.

%A perception system needs to cover adequately the relevant surroundings of the
%vehicle. Ideally, this includes 360 degrees around the vehicle up to several
%hundreds meters. Figure \ref{fig:range-fov} shows usual operative range and
%field of view of relevant sensing technologies. 
%
%\begin{figure*}[h]
%    \centering
%    \includegraphics[width=0.7\textwidth]{"img/plot_range-fov"}
%    \caption{Range-FoV for depth sensors (3D sensing technology)}
%    \label{fig:range-fov}
%\end{figure*}

