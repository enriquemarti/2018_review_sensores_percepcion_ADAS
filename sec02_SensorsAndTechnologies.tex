
This section presents the three principal sensing categories for exteroceptive
perception in Automated Driving: artificial vision, radar and LiDAR.
Exteroceptive sensors and external perception acquire information about the
environment of the vehicle (e.g. road, other vehicles), as opposed to 
proprioceptive sensors that provide information about the state of the vehicle
(speed, accelerations, integrity of components) and are not covered in this 
article. Communications are also out of the scope of the review. 

Next subsections present the advantages, drawbacks and challenges for the three 
types of sensors mentioned above. Each one is followed with a review of
relevant emergent technologies in the field.

In \ref{sec:03-d-information-domains} a taxonomy of information domains is
presented. It is useful for several purposes. First it allows to 
link sensors technologies with perception algorithms described in section
\ref{sec:03-problemsapplications}, since the first provide the raw data needed
by the second. Second, the categorization is used to structure the subsequent
analysis (section \ref{sec:03-e-sensors-for-perception}) about the suitability
and adequacy of the presented sensing technologies for perception in Automated
Driving. This last part includes also the expected performance under different
environmental and weather conditions.

\subsection{Artificial Vision}
Artificial vision is a popular technology that has been used for decades in 
disciplines as mobile robotics, surveillance or industrial inspection. 
It acquires information about objects in the real world by analyzing their
images as captured by photo and video cameras. 
%Cameras are devices that gather light using
%a sensor composed by a grid of thousands or millions of individual detection 
%elements. The amount of light captured by each element is translated into the
%intensity of a pixel in the resulting image. 

This technology offers interesting features, as the low cost of sensors --only 
some types-- and providing range of information types including spatial
(shape, size, distances), dynamic (motion of objects by analyzing their 
displacement between consecutive frames) and semantic (shape analysis).

Artificial vision technology face several challenges, especially in 
applications like automated driving:

\begin{itemize}
    \item Varying light conditions: driving happens at day, at night, indoors, 
    or at dusk or dawn with the sun close to the horizon. 
    Dark spots, shadows, glares, reflections and other effects difficult the
    implementation of reliable artificial visible algorithms.
    
    \item Scenes with a High Dynamic Range (HDR) contain dark and strongly
    illuminated areas in the same frame.
    Most sensor technologies have a limited capacity of capturing both extremes
    simultaneously, so that information is lost in one or the two sides (under- 
    or overexposure). 
        
    \item Low light and high speed: cameras need higher exposures time as
    illumination is weaker. Fast moving elements appear blurred, which can 
    affect later processes as border or feature detection. Also, if the sensor
    does not capture light in its full surface simultaneously (rolling shutter)
    distortion effects can appear in those objects.
\end{itemize}

In \cite{Pueo2016} some of these problems are analyzed from the perspective of
recording scenes in sports.
In order to deal with these difficulties, different technologies and solutions 
have been proposed. 

\begin{itemize}    
    \item High Dynamic Range imaging (HDR): common sensors in photographic and 
    industrial cameras offer a dynamic range of 60-75 dB (10 to 12.5 EVs),
    that is not sufficient in mixed illumination environments as entering or 
    exiting tunnels. Sony launched its IMX390 automotive sensor with an
    extended 120 dB range (equivalent to 20 EVs) and 2k resolution. 
    An automotive grade sensor combining HDR capabilities and 
    Near Infra-Red light detection is analyzed in \cite{Maddalena2005}. 
    In \cite{Strobel2013} a sensor with 130 dB range (global shutter) and up
    to 170 dB (rolling shutter) is presented for industrial safety application.
    
    \item Global shutter sensors and Rolling shutter compensation. In rolling
    shutter cameras the elements of the sensor capture light at different time 
    intervals, creating artifacts as distorted objects under motion or frames
    half-dark-half-light in rooms illuminated by flickering lights (LEDs, 
    fluorescents). 
    These effects have been corrected using software able to compensate scene
    motion vector \cite{Chia-KaiLiang2008}\cite{Chun2008}. Global shutter
    cameras, on the other hand, have the ability to capture light in all the
    elements of the sensor simultaneously. 
        
    \item Captured spectrum: Far infrared cameras (wavelength 900-1400 nm, also
    called thermal cameras) detect the emissions of hot objects including
    living beings. 
    Thermal cameras are effective for pedestrian and animal detection
    \cite{OMalley2008}\cite{Besbes2015} in the dark and through dust and smoke.
    Near Infrared (750-900 nm) complements visible spectrum with a better
    contrast in high dynamic range scenes, improves night visibility. 
    In \cite{Pinchon2018} authors compare visible light, near infrared and far
    infrared cameras in different light and atmospheric conditions.
    
\end{itemize} 

\subsubsection{3D technology}
Traditional camera technology is essentially 2D, but there are some
types of vision sensors that can perceive depth information. This section
describes the three principal types that are already available as commercial
devices, although not always targeting the automotive market.


%\begin{itemize}
%    \item \emph{Stereo vision:} 

\textbf{Stereo vision:} depth is calculated \cite{Hamzah2016} from the 
apparent displacement of visual features in the images capture by two 
monocular cameras pointing in the same direction and separated by some
distance (known as baseline). 
            
One of the greatest advantages of stereo vision systems is their capability 
to provide dense depth maps, as opposed to sparse sensors as LiDARs. 
%Their
%resolution, and maximum and minimum perceived depth are limited by
%camera field of view and imaging sensor resolution.     
Stereo vision drawbacks include issues with low-textured patterns 
(e.g. solid colors) that difficult establishing correspondences between
frames. Also, it has a high computational complexity, and the pair of 
cameras requires a careful calibration to ensure proper translation of 
disparities to depths.

Several good performing monocular SLAM (Simultaneous Location And Mapping)
algorithms \cite{Engel2014}\cite{Engel2018} have been developed in the last
years. 
These systems cannot be classified as stereo but share some of its principles: 
the motion of a single monocular camera setup creates an artificial baseline
between consecutive frames. This allows to estimate depth and camera motion at
the same time.
    
\textbf{Structured light:} a monocular camera coupled with a device that
illuminates the scene with a known pattern of infrared light. 
The distortion of the light pattern when projected over an irregular 
surface is captured by the camera and translated to a depth map.

Structured light devices overcome some limitations of stereoscopic systems
as depending on texture patterns and having a high computational cost. 
However, they require the same high-accuracy calibration \cite{Garbat2013}
and have some additional limitations as short operative ranges (usually 
below 20 meters), limited by the power of the emitter and the intensity of 
ambient light. Reflections can affect its performance.

%    This technology can be combined with stereo vision to improve robustness
%    and operative conditions, as in the commercial device Intel RealSense 
%D4500.

%    \item \emph{Time-of-flight:} 
\textbf{Time-of-flight:} an active sensing technology 
\cite{Hansard2013}, based in the same round-trip-time principle 
of LiDAR sensors (see \ref{sec:02-c-lidar}): an emitter composed of infrared 
LEDs floods the scene
with modulated light that is captured by the sensor after being reflected by 
elements in the environment. 
The round-trip-time can be calculated for each pixel based on the phase shift 
of incoming light, which is then translated to a distance.

Using a non-directed source of light (as opposed to the low divergence laser
emitter in LiDAR) has advantages and disadvantages. 
The advantages include the ability to create dense depth maps, its high 
accuracy and high refresh rate exceeding 50 Hz. The drawbacks include 
problems with intense ambient light and a short operative range (10 to 20 
meters), making this technology unfeasible for many Automated 
Driving applications.

This can change in a short future. Alternative research line as indirect
time-of-flight \cite{Villa2017}, pulsed light time-of-flight or avalanche
photodiodes \cite{Panasonic2018} appear to be close to commercialization. 
These technologies promise ranges between 50 and 250 meters.
    
%\end{itemize}


\subsubsection{Emerging vision technologies}

Event-based vision is a bioinspired technology developed by Zurich University
and the ETH Zurich. The elements of the sensor (pixels) are triggered 
asynchronously and independently when a change on the intensity is detected 
generating a stream of activations also called \emph{events}. 
%An event can be seen as something similar to the output of a feature detection 
%algorithm for artificial vision applications.
Events can be grouped in adaptable time windows for getting a frame-like image,
reaching the microsecond scale for high speed tracking. 
The work \cite{Mueggler2014} shows tracking at 1000 FPS under regular indoor 
lightning conditions. , 
Independence of sensor elements raises the dynamic range of the sensor to 
120 dB. 
Events can be the input to visual odometry \cite{Censi2014} and SLAM
\cite{Vidal2017} applications, relieving the CPU of time consuming operations
on raw images. 
%A steering wheel control for automated driving systems 
%based on deep learning has been shown in \cite{Maqueda2018}.

%Light polarization represents an additional source of information which
%is know to be used by animals able to perceive it (some ants, 
%mantis-shrimp). 
There is an active line of research \cite{Garcia2018} around sensors able to
capture light polarization. It performs better than traditional sensors in
some adverse conditions, and provides information than no other sensor can 
capture.
 

\subsection{Radar}

Radar technology use high frequency electromagnetic waves to measure the
distance to objects based on the \emph{round-trip time} principle, which is the
time it takes the wave to reach the object, bounce on it and travel back to the
sensor. 
Many modern automotive radars use a technique known as digital beamforming
\cite{Hasch2015} to direct the emitted wave in the desired direction.
 
There are two principal technologies: pulsed radar and Frequency-Modulated
Continuous Wave (FMCW). Pulsed radar emits a short burst and measures distance
using the raw round-trip time. FMCW technology emits a signal with a well known
and stable frequency, that is modulated with another continuous signal that
varies its frequency up and down (typically using a triangular shape).
Distance is determined using the frequency shift between the emitted and 
reflected signals. 
Both technologies can exploit Doppler effect to measure the relative speed of 
the target with respect to the sensor. 

%Starting in 2004, EU allocated a permanent 5 GHz wide band around 79 GHz
%\cite{EULawandPublications2004}. Short distance applications as blind spot
%detection, parking assistance, jam assistance and pre-crash measures use the 
%upper part (77-81 GHz), since it offers a better resolution. Long distance 
%applications as ACC use a radar signal around 76-78 GHz. 
%Most modern automotive radars use FMCW technology in multi-frequency chips 
%that 
%can switch between different bands and functions dynamically.

One of the strongest arguments for including radar sensing in automated 
vehicles is its independence of light and weather conditions. 
It works in the dark, and detections are almost equally good with snow, 
rain, fog or dust \cite{Reina2015}. Long range radars can see up to 250 m
in very adverse conditions, where no other sensor works.

Radar sensors present some difficulties and drawbacks:

\begin{itemize}
    \item Sensible to target reflectivity: processing radar data is a tricky
    task, due to the heterogeneous reflectivity of the different materials. 
    Metals amplify radar signal, easing detection of vehicles but increasing
    the apparent size of small objects as discarded cans in the road, while 
    other materials (e.g. wood) are virtually transparent.
    This can cause false positives (detect a non existing obstacle) and false
    negatives (not detecting an actual obstacle).
    
    \item Resolution and accuracy: radars are very accurate measuring distance
    and speed along the line that connects the sensor with a target. However, 
    horizontal resolution depends on the characteristics of the emitted beam.
    Raw angular resolution in digital beamforming systems falls between 2 to 5
    degrees \cite{Schneider2005}, although the application of advanced
    processing techniques can improve angle of arrival accuracy to 0.1 degrees 
    in long range radars (with a narrow FoV around 30 degrees) and 1
    degree for short range sensors \cite{Kissinger2012}. 
    This angular resolution is translated into a spatial resolution that grows
    with distance. At 30 meters it can be difficult to separate (detect as
    independent targets) a pedestrian from a nearby car. 
    At 100 m distance it can be impossible to separate vehicles in neighbor
    lanes, determine if a vehicle is in our same lane, and even if a detection
    is a vehicle or a bridge over the road.
\end{itemize}

\subsubsection{Emerging radar technologies}

One of the most active research area is related with high resolution radar
imaging for automobiles. High resolution radar can extend it use from target 
tracking and speed measuring to providing richer semantic information. 
Recent research explores the use of higher frequency bands to increase
resolution, opening the door to radar imaging and creation of detailed 3D maps. 
An example can be found in \cite{Reina2015}, where a 90GHz rotating radar in
the roof of a car is used to map the environment, including vehicles, static
objects and ground.
The paper \cite{Kohler2013} demonstrates the feasibility of radars operating
between 100 and 300 GHz, analyzing atmospheric absorption and reflectivity of
materials usually found in driving scenarios.
%For example, Arbe Robotics \cite{ArbeRobotics2018} is working in a 
%model with 300 m range, a field of view of 100 degrees horizontal, 30 degrees
%vertical, and a resolution of 1 degree azimuth and 2 degrees elevation. 
%This model is expected to generate a full 4D (3D position plus speed) image of
%the scene at 25-50 Hz, thanks to the embedded machine learning and SLAM
%algorithms.

One of the key technologies that can lead to high resolution radar imaging are 
meta-material based antennas \cite{Brookner2016,Sleasman2017} for efficient
synthetic aperture radars. 
Some manufacturers as Metawave \cite{Metawave2018} are starting 
to offer products oriented to automotive sector based on the technology.

%In a different line, ground-penetrating radar is a technology used long
%time ago in diverse areas ranging from archeology to industrial applications.
%MIT Lincoln Laboratory created a device \cite{Cornick2016} that can be placed 
%below a vehicle to get a reading describing the geological properties of the
%first few meters under the ground. This reading is not affected by snow, 
%water, dust or any other element over the surface of the road. The idea is to
%create maps that can be used later to localize vehicles, with an accuracy of
%a few centimeters. %\cite{XXXX}
%Recently, Wavesense \cite{WaveSense} announced tests in snowy places and is
%close to commercialization stage.


\subsection{LiDAR}
\label{sec:02-c-lidar}
LiDAR (Light Detection And Ranging) is an active ranging technology that 
calculates distance to objects by measuring round-trip time of a laser light 
pulse.
Sensors for robotic and automotive applications use a low power 
near-infrared laser (900-1050 nm) that is invisible, eye-safe. 
Laser beams have a low divergence, so that the reflected power does not decay 
too much with distance. Thanks to this, LiDARs can measure distances up to 
200 m under direct sunlight.
Typically, a rotating mirror is used to change the direction of the laser 
pulse, reaching 360º horizontal coverage. Commercial solutions use an array of 
emitters to produce several vertical layers (between 4 and 128) 
%that, combined with rotation, 
that generate a point cloud representing the environment.
LiDAR sensors feature an extraordinary accuracy measuring distances, averaging
a few millimeters in most cases and degrading to 0.1-0.5 meters in the worst 
cases. This makes LiDAR a good choice for creating accurate digital maps.

However, they have several drawbacks to take into account:

\begin{itemize}    
    \item Low vertical resolution: in low cost models, which usually feature 
    less than 16 layers, vertical resolution (separation between consecutive
    layers) falls down to 2 degrees. At 100 m distance, this is translated into 
    a vertical distance of 1.7 m. High end models reduce this to 0.2-0.4 
    degrees, but at a much higher cost.
    
    \item Sparse measures (not dense): according to \cite{Gatziolis2008} typical
    LiDAR beam divergence is between 0.1 and 1 mrad (0.005 to 0.05 degrees).
    Commercial device Velodyne HDL64 has a 2 mrad divergence \cite{Glennie2010} 
    (0.11 degrees) and a vertical resolution of 0.42 degrees. At 50 meters 
    distance, the 0.3 degree gap is equivalent to a blind strip 0.26 meters
    tall. In low end devices (Velodyne VLP16) this grows to 1.5 meters. 
    Small targets can remain undetected, and structures based on threads and 
    bars are virtually invisible.
    
    \item Poor detection of dark and specular objects. Black cars
    can appear as invisible to the LiDAR, since they combine a color that
    absorbs most radiation with a non-Lambertian material that does not scatter
    radiation back to receiver.
    
%    \item High cost. High-end models have a cost between 25 and 75 k\$.
%    Just a year ago, Velodyne models started at 9000 US\$ for the basic 16
%    layer device, although they have cut prices down to a 50\% to face the
%    recently appeared competitors. Now, some companies are selling
%    equivalent models for less than 4000 US\$. Solid state lidars promise
%    prices an order of magnitude smaller.
    
    \item Affected by dense rain, fog and dust. Infrared laser
    beams are affected by rain and fog because water droplets scatter the light 
    \cite{Wang2008}, reducing the operative range of the device and producing 
    false measures in the front of the cloud. The effect of dust has been
    explored in \cite{Phillips2017}. LiDAR performance in these scenarios is 
    worse than radar, but still better than cameras and human eye.
\end{itemize}

\subsubsection{Emerging LiDAR technologies}
\label{sec:03-lidar-emerging}

%Direct speed measurement is a really useful feature for any sensor. Up to date,
%only radars where able to capture speed, for instance using FMCW signals.

FMCW LiDAR \cite{Nordin2004} emits light continuously to measure objects speed
based on Doppler effect. In the last years some research prototypes suitable for
the automotive market start appearing \cite{Poulton2016}.
%, until recently
%a company named Blackmore announced a commercial version. 
Apart from improving target tracking capabilities, observation of speed can
be useful to enhance activity recognition and behavior prediction, for example 
by detecting the different speeds of limbs and body in cyclists and pedestrians.

%Regarding LiDARs, however, the most popular emerging technology in the last 
%years has been Solid State LiDAR. It offers advantages when it comes
%to operating under strong vibrations and dynamics, apart from being potentially
%smaller, cheaper and faster. 
%However, the market does not offer products combining high resolution and a 
%wide field of view, so mechanical devices are the only option for full 
%360-degree coverage and environment mapping.

Solid state LiDAR is an umbrella term that includes several technologies, two 
of which are oscillating micro-mirrors and Optical Phased Array (OPA).
The first technology combines one or many laser emitters that are directed
with micro-mirrors that can rotate around two axes, so that the beam 
can be directed within a cone. Manufacturer LeddarTech commercializes devices
based on this technology \cite{LeddarTech2016}.
Optical phased arrays \cite{McManamon1996} is a technology similar to that used 
for EBF radars. 
%An array of optical emitters generate coherent signals with a well
%controlled phase difference. This generates a far-field radiation pattern 
%pointing in a direction that depends on the phase. 
that allows to control the direction of the beam with high accuracy and speed.
Quanergy \cite{Eldada2017} is one of the few manufacturers commercializing
devices based on this technology.
% with a focus on the automotive sector, its S3
%model features a 120 degrees FoV with a range of 150 m. 
%They report a spot size of 5 cm at 100 m, that is  comparable to a beam
%divergence of 0.5 mrad --4 times smaller than Velodyne HDL64 model.

OPA technology has additional advantages over mechanical LiDAR: the scan 
pattern can be random in the entire FoV, which is great for characterizing fast 
moving objects. It is possible to observe only a region of interest within the 
FoV. Also, it is possible to augment the point density within each frame for 
better resolution. The three features can be combined to do fast low resolution 
inspection of the full FoV, and then tracking with high resolution the objects
of interest for enhanced shape recognition even at far distances.
%This is similar to Waymo's claims about the LiDARs they have developed for 
%their self-driving vehicles, as described in section 
%\ref{sec:04-relevantdemos}.

\subsection{Relevant information domains}
\label{sec:03-d-information-domains}

The task of a perception system is to bridge the gap between sensors providing 
data and decision algorithms requiring information.
A classical differentiation between both terms is the following: data is 
composed by raw, unorganized facts that need to be processed. 
Information is the name given to data that has been processed, organized, 
structured and presented in a proper context.

The following taxonomy (table \ref{tab:info-taxonomy}) is tightly related with 
the goals of perception stage (covered in section
\ref{sec:03-problemsapplications}). It allows to present conclusions about the
suitability of each sensor technology for the different perception tasks in a
clear and organized way.

\begin{table}%[H]
    \caption{Information taxonomy in Automated Driving domain}
    \label{tab:info-taxonomy}
    \begin{tabular*}{\linewidth}{lr p{5.5cm}} %{\textwidth}{lrL}
        \hline %\toprule
        \textbf{Category} & \textbf{\#}	& \textbf{Information type}	\\
        \hline %\midrule
        \multirow{2}{*}{Ego-vehicle}
        & 1 & Kinematic/dynamic (includes position) \\
        & 2 & Proprioceptive (components health/status) \\
        \hline %\midrule
        \multirow{3}{*}{Passengers/driver}
        & 3 & Driver awareness/capacities \\
        & 4 & * Driver intentions (mind model)  \\
        & 5 & Passenger status (needs, risk factors) \\
        \hline %\midrule
        \multirow{4}{*}{Environment}
        & 6 & Spatial configuration: location, size, shape, fine features 
        \\
        & 7 & Identification: class, type, identity \\
        & 8 & Regulation and semantics: traffic signs, road marks, other 
        elements \\
        & 9 & Contextual factors: weather, driving situation(e.g. jam, 
        highway, off-road) \\
        \hline %\midrule
        \multirow{4}{*}{External actors}
        & 10 & Spatial features: location, size, shape, fine features  \\
        & 11 & Kinematic/dynamic: position, motion \\
        & 12 & Identification: class, type, identity \\ 
        & 13 & Semantic features: vehicle lights, pedestrian clothes, gestures 
        \\
        & 14 & * Situational engagement: collaborative/aware 
        (adult pedestrians, other vehicles) vs non-collaborative/unaware 
        (animals, children) \\ 
        \hline %\bottomrule
    \end{tabular*}
\end{table}

Elements with an asterisk are derived information. This is, they that can 
be inferred from sensed data but not directly observed. It is mostly related 
with internal state of external entities, as the intentions of human beings and 
animals.

\subsection{Using sensors for perception}
\label{sec:03-e-sensors-for-perception}

Sensor selection and arrangement is one of the most important aspects in the 
design of a perception system for automated vehicles. It also has a great impact
in its cost, with some setups having several times the price of the rest of 
the vehicle. 
Many analysis focus in spatial coverage and range, but this epigraph summarizes
two other aspects of the uttermost importance: type of information acquired and 
impact of environmental factors.

In first place, the characteristics of a sensing technology determines its 
suitability for acquiring certain types of information, and restricts its range 
of operative conditions.
Figure \ref{fig:information_vs_sensors} relates the principal sensing 
technologies currently used in the automotive market and Automated Driving
initiatives with relevant types of information identified in Table 
\ref{tab:info-taxonomy}. The adequacy of a sensor for acquiring a certain type
of information (or equivalently, the expected quality of that type of
information when captured by that sensing technology) is classified in three
levels: Good (green shading, tick), Medium (ambar shading, letter M) and Bad
(red shading, letter B).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{"img/information_types_sensors"}
    \caption{Sensor adequacy for relevant types of information}
    \label{fig:information_vs_sensors}
\end{figure}

Sensors and perception are expected to work uninterruptedly during vehicle 
operation. Weather and other environmental factor can degrade sensor
performance, but each technology is affected in a different way. 
Figure \ref{fig:sensors-environ} summarizes the effect of common external
factors in the performance of the analyzed sensing technologies, using the
same notation as Figure \ref{fig:information_vs_sensors}.

\begin{figure}[h]
\centering
\includegraphics[width=0.68\linewidth]{"img/sensors_atmospheric_conditions"}
\caption{Sensor robustness under atmospheric and environmental factors}
\label{fig:sensors-environ}
\end{figure}

%A perception system needs to cover adequately the relevant surroundings of the
%vehicle. Ideally, this includes 360 degrees around the vehicle up to several
%hundreds meters. Figure \ref{fig:range-fov} shows usual operative range and
%field of view of relevant sensing technologies. 
%
%\begin{figure*}[h]
%    \centering
%    \includegraphics[width=0.7\textwidth]{"img/plot_range-fov"}
%    \caption{Range-FoV for depth sensors (3D sensing technology)}
%    \label{fig:range-fov}
%\end{figure*}

