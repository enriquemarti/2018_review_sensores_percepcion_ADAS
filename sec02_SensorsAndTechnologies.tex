
The principal sensing technologies for exteroceptive perception in Automated 
Driving are presented here. Advantages, drawbacks and 
challenges are described for each sensor, followed by some of the emergent 
technologies that can be relevant in the future of the discipline.

Sensors provide the raw data needed by perception algorithms, described in
next section. In order to link both aspects, this section includes a
categorization of information domains that will be referenced later in the
article.
Two tables are presented to summarize the adequacy of sensor technologies for 
acquiring the identified types of information, as well as the expected
performance under different environmental and weather conditions.

\subsection{Relevant information domains}

The task of a perception system is to bridge the gap between sensors providing 
data and decision algorithms requiring information.
A classical differentiation between both terms is the following: data is 
composed by raw, unorganized facts that need to be processed. 
Information is the name given to data that has been processed, organized, 
structured and presented in a proper context.

The following taxonomy (table \ref{tab:info-taxonomy}) is tightly related with the goals of perception stage
(covered in section \ref{sec:03-problemsapplications}). It allows to
present conclusions about the suitability of each sensor technology for
the different perception tasks in a clear and organized way.

\begin{table}[H]
    \caption{Information taxonomy in Automated Driving domain}
    \label{tab:info-taxonomy}
    %\centering
    %% \tablesize{} %% You can specify the fontsize here, e.g.  \tablesize{\footnotesize}. If commented out \small will be used.
    \begin{tabularx}{\linewidth}{lrL}
        \toprule
        \textbf{Category} & \textbf{\#}	& \textbf{Information type}	\\
        \midrule
        \multirow{2}{*}{Ego-vehicle}
         & 1 & Kinematic/dynamic (includes position) \\
         & 2 & Proprioceptive (components health/status) \\
         \midrule
         \multirow{3}{*}{Passengers/driver}
         & 3 & Driver awareness/capacities \\
         & 4 & * Driver intentions (mind model)  \\
         & 5 & Passenger status (needs, risk factors) \\
         \midrule
         \multirow{4}{*}{Environment}
         & 6 & Spatial configuration: location, size, shape, fine features 
         \\
         & 7 & Identification: class, type, identity \\
         & 8 & Regulation and semantics: traffic signs, road marks, other 
         elements \\
         & 9 & Contextual factors: weather, driving situation(e.g. jam, 
         highway, off-road) \\
         \midrule
         \multirow{4}{*}{External actors}
         & 10 & Spatial features: location, size, shape, fine features  \\
         & 11 & Kinematic/dynamic: position, motion \\
         & 12 & Identification: class, type, identity \\ 
         & 13 & Semantic features: vehicle lights, pedestrian clothes, gestures 
         \\
         & 14 & * Situational engagement: collaborative/aware 
         (adult pedestrians, other vehicles) vs non-collaborative/unaware 
         (animals, children) \\ 
        \bottomrule
    \end{tabularx}
\end{table}

Elements with an asterisk are derived information. This is, they that can 
be inferred from sensed data but not directly observed. It is mostly related 
with internal state of external entities, as the intentions of human beings and 
animals.

This survey is focused in the perception of external elements, that corresponds 
to the two last categories (Environmenat and External actors).

\subsection{Principal sensor technologies for perception}

The common sensors that are present in most of the modern ADAS and autonomous vehicles projects are: artificial vision, radar and Lidar.

Most of them are ranging sensors: the measured magnitude is distance to 
external objects, by means of the round-trip delay time of a electromagnetic 
wave (radar) or a pulse of structured light (Lidar).
Artificial vision, in its most basic form, registers a grid of intensities and 
colors. This allows to extract additional information about the world, including 
traffic elements that are encoded visually to allow being interpreted by humans 
(signals, lanes, traffic lights).

This section is focused in the three principal technologies for Automated 
Driving: vision, radar and LiDAR.


\subsection{Artificial Vision}
Artificial vision is a popular technology that has been used for decades in 
disciplines as mobile robotics, surveillance or industrial inspection. It acquires
information about objects in the real world by analyzing their images as 
captured by photo and video cameras. Cameras are devices that gather light using
a sensor composed by a grid of thousands or millions of individual detection 
elements. The amount of light captured by each element is translated more or 
less directly into the intensity of a pixel in the resulting image. 

This technology offers interesting features, as the low cost of sensors --only 
some types-- and providing range of information types including spatial
(shape, size, distances), dynamic (motion of objects by analyzing their 
displacement between consecutive frames) and semantic (shape analysis).

Artificial vision technology face several challenges, especially in 
applications like automated driving:

\begin{itemize}
    \item Varying light conditions: driving is not limited to daylight 
    conditions. It can also happen at night, indoors, or even worse,
    during dusk or dawn with the sun close to the horizon. Dark spots,
    shadows and other effects difficult the implementation of reliable 
    artificial visible algorithms.
    
    \item High dynamic range in the scene (some areas are very dark 
    and some others are strongly illuminated): while varying light conditions
    can be mitigated through dynamimc exposure mechanisms, HDR conditions
    are more difficult to deal with. Most sensor technologies have a 
    limited capacity of capturing both extremes simultaneously, so that 
    information is lost in one or the two sides. In automotive applications, 
    this can mean detecting lane lines at the cost of not seeing preceding
    vehicles or vicecersa.
    
    
    \item Objects moving at great speed: cameras need higher exposures time as
    illumination is weaker. Fast moving elements appear blurred, which can 
    affect later processes as border or feature detection. Also, if the sensor
    does not capture light in its full surface simultaneously (rolling shutter)
    distorsion effects can appear in those objets
\end{itemize}

In \cite{Pueo2016} some of these problems are analyzed from the perspective of
recording scenes in sports.
In order to deal with these difficulties, different technologies and solutions 
have been proposed. 

\begin{itemize}
    \item Rolling vs. Global shutter sensors. In rolling
    shutter cameras, either sensor technology (rows activated sequentially) or
    mechanical elements (a physical shutter moving to expose and oclude the 
    sensor), the elements of the sensor capture light at differents time 
    intervals. This have negative effects as: fast moving objects appearing
    distorted, or flickering lights (fluorescents, LEDs) creating images with 
    some parts illuminated and others dark. Rolling shutter effects
    can be corrected by compensating scene motion vector, as proposed in 
    \cite{Chia-KaiLiang2008}\cite{Chun2008}.
    Global shutter cameras, on the other hand, have the ability to capture 
    light in all the elements of the sensor simultaneously.
    
    \item High Dynamic Range imaging (HDR): common sensors in photographic and 
    industrial cameras offer a dynamic range of 60-75 dB (10 to 12.5 EVs),
    that is not sufficient in mixed illumination environments as entering or 
    exiting tunnels. Sony launched its IMX390 automotive sensor with an
    extended 120 dB range (equivalent to 20 EVs) and 2k resolution. 
    An automotive grade sensor combining HDR capabilities and 
    Near Infra-Red light detection is analyzed in \cite{Maddalena2005}. 
    In \cite{Strobel2013} a sensor with 130 dB range (global shutter) and up
    to 170 dB (rolling shutter) is presented for industrial safety application. 
        
    \item Captured spectrum: apart from visible light cameras, either in color
    or grayscale, automotive applications have used infrared sensors.
    Far infrared cameras (wavelength 900-1400 nm), 
    also known as thermal cameras, detect the emissions of hot objects, 
    including living beings. Thermal cameras are effective for pedestrian and
    animal detection \cite{OMalley2008}\cite{Besbes2015} in the dark and 
    through dust and smoke. FLIR manufactured a system that was integrated
    in BMW's NightVision system since 2009 cameras. Near Infrared (750-900 nm)
    complements visible spectrum with a better contrast in high dynamic range
    scenes, improves night visibility. In \cite{Pinchon2018} authors compare
    visible light, near infrared and far infrared cameras in different 
    luminic and atmospheric conditions.
    
    Light polarization represents an additional source of information which
    is know to be used by animals able to perceive it (some ants, 
    mantis-shrimp). There is an active line of research \cite{Garcia2018}
    around sensors able  to capture this feature, since it can be useful in
    conditions adverse for traditional sensors. 
    
    
\end{itemize} 

\subsubsection{3D technology}
Although traditional camera technology is esentially 2D, there are some
types of vision sensors that can perceive depth information. This section
describes the three principal types that are already available as commercial
devices, although not always targeting the automotive market.


%\begin{itemize}
%    \item \emph{Stereo vision:} 

\textbf{Stereo vision:} depth is calculated \cite{Hamzah2016} from the 
apparent displacement of visual features in the images capture by two 
monocular cameras pointing in the same direction and separated by some
distance (known as baseline). In the last years, several good performant 
monocular SLAM algorithms \cite{Engel2014}\cite{Engel2018} have been 
developed, which are not stereo systems but share some principles: they 
exploit the motion of a single monocular camera setup, creating an 
artificial baseline between consecutive frames that allow to estimate 
depths and camera motion at the same time.
    
%    a biology-inspired setup composed by two 
%    monocular cameras pointing in the same direction and separated by some
%    distance (known as baseline). An object close to the sensor is perceived as
%    located in different positions within each camera frame, while far objects
%    have the same location on the two images. Depth is not a direct product of 
%    sensing but computed from raw measures. 
%    Extracting correspondences between pairs of frames is not trivial and a 
%    computationally demanding task.
        
One of the greatest advantages of stereo vision systems is their capability 
to provide dense depth maps, as opposed to sparse sensors as LiDARs. Their
resolution, and maximum and minimum perceived depth are limited by
camera field of view and imaging sensor resolution. 
%    Disparities smaller 
%    than one pixel of far objects could be not detected, and detecting large 
%    disparities in very close objects is computationally expensive.    
Stereo vision drawbacks include issues with low-textured patterns 
(e.g. solid colors) that difficult establishing correspondences between
frames. Also, it has a high computational complexity, and the pair of 
cameras requires a careful calibration to ensure proper translation of 
disparities to depths.

%    At present time it is possible to find several of-the-shelf sensors for 
%    stereo vision, either in the automotive field \cite{ambarella2018} or as 
%    generic application products.
    
   %\item \emph{Structured light:} 
    
\textbf{Structured light:} a monocular camera coupled with a device that
illuminates the scene with a known pattern of infrared light. 
The distortion of the luminic pattern when projected over an irregular 
surface is captured by the camera and translated to a depth map.

Structured light devices overcome some limitations of stereoscopic systems
as depending on texture patterns and having a high computational cost. 
However, they require the same high-accuracy calibration \cite{Garbat2013}
and have some additional limitations as short operative ranges (usually 
below 20 meters), limited by the power of the emitter and the intensity of 
ambient light. Reflections can affect its performance.

%    This technology can be combined with stereo vision to improve robustness
%    and operative conditions, as in the commercial device Intel RealSense 
%D4500.

%    \item \emph{Time-of-flight:} 
\textbf{Time-of-flight:} is also an active sensing technology 
\cite{Hansard2013}. It is based in the same round-trip-time principle 
of LiDAR sensors: an emitter composed of infrared LEDs illuminates the scene
with modulated light that is captured by the sensor after being reflected by 
elements in the environment. 
The round-trip-time can be calculated for each pixel based on the phase shift 
of incoming light, which is then translated to a distance.

Using a non-directed source of light (as opposed to the laser emitter in 
LiDAR) has advantages and disadvantages. 
The advantages include the ability to create dense depth maps, its high 
accuracy and high refresh rate exceeding 50 Hz. The drawbacks include 
problems with intense ambient light and a short operative range (10 to 20 
meters), making this technology unfeasible for many Automated 
Driving applications.

This can change in a short future. Alternative research line as indirect
time-of-flight \cite{Villa2017}, pulsed light time-of-flight or avalanche
photodiodes \cite{Panasonic2018} appear to be close to commercialization. 
These technologies promise ranges between 50 and 250 meters.
    
%\end{itemize}


\subsubsection{Emerging vision technologies}

The most promising technology is known as event-based vision. It is a 
bioinspired technology developed by Zurich University and the ETH Zurich. 
%It approaches the problem of processing and tracking fast 
%moving images in a completely different way: 
Instead of capturing fixed frames at discrete times, the elements of the sensor
(pixels) are triggered asynchronously and indepently when a change on the
intensity is detected generating a stream of activations also called 
\emph{events}. 
An event can be seen as something similar to the output of a feature detection 
algorithm for artificial vision applications.
Events can be grouped in adaptable time windows for getting a frame-like image,
reaching the microsecond scale for high speed tracking. 
The work \cite{Mueggler2014} shows integration windows of 1 ms (~1000 fps) in
regular indoor lightning conditions to be sufficient for high speed tracking, 
thanks also to the 120 dB dynamic range of the sensor.
An additional advantage is that the output can be used in raw form for 
applications as visual odometry \cite{Censi2014} and SLAM \cite{Vidal2017}
relieving the CPU of time consuming operations on raw images.
More recently, a steering wheel control for automated driving systems 
based on deep learning has been shown in \cite{Maqueda2018}.
 
 

\subsection{Radar}

Radar technology uses high frequency electromagnetic waves to measure the
distance to objects in the environment.
There are two principal technologies: pulsed radar emits a short burst and 
measures distance using the round trip time. Most modern automotive 
radars use Frequency-Modulated Continuous Wave (FMCW) where a signal with
a well known and stable frequency is modulated with another continuous signal
that varies its frequency up and down (tipically using a triangular shape).
Distance is determined using the frequency shift between the emitted and 
reflected signals. Both technologies allow to measure target speed based
on Doppler effect.

Starting in 2004, EU allocated a permanent 5 GHz wide band around 79 GHz 
\cite{EULawandPublications2004}. 
Short distance applications as blind spot detection, parking assistance, jam 
assistance and pre-crash measures use the upper part (77-81 GHz), since it 
offers a better resolution. Long distance applications as ACC use a radar 
signal around 76-78 GHz. However, manufacturers are implementing multifrequency 
chips that can switch between different functions dinamycally.
%This information is resumed in figure \ref{fig:radar-freqband}

One of the strongest arguments for including radar sensing in automated 
vehicles is its independence of light and weather conditions. 
It works in the dark, and detections are almost equally good with snow, 
rain, fog or dust \cite{Reina2015}. Long range radars can see up to 250 m
in very adverse conditions, where no other sensor works.

However, radar processing can be tricky due to the reflectivity of the
different materials: metals amplify radar signal, easening detection of 
vehicles but increasing the apparent size of small objects as discarded cans
in the road, while other materials (e.g. wood) are virtually transparent.
Horizontal and vertical positioning error 
grows with distance, so that at 100 m is difficult to say if an obstacle is
actually on the vehicle lane, or if it is a bridge over the road.

As Automated Driving technology advances, radars are expected to go from its
current use of detecting targets and measuring its speed to providing richer
semantic information. 
Recent research turns around using higher frequency bands to increase
resolution. This results in better shape recognition and separation of close
targets that current technology reports as a single object.
These features open the door to radar imaging and creation of detailed 3D maps. 
An example can be found in \cite{Reina2015}, where a 90GHz rotating radar in
the roof of a car is used to map the environment, including not only other
vehicles and static objects but also the ground.
In \cite{Kohler2013} explores the feasibility of radars operating between 
100 and 300 GHz. They present a prototype working at 150 GHz and conclude 
that atmosferic absorption and reflectivity of materials usually found in 
driving scenarios make this technology feasible, with the benefit of a high
resolution that may enable radar imaging.


\subsubsection{Emerging radar technologies}

One of the most active research area is related with high resolution radar
imaging for automobiles. Apart from research in early stages previously 
referred, a number of technological companies and startups are working in 
commercial products. 
For example, Arbe Robotics (http://www.arberobotics.com/) is working in a 
model with 300 m range, a field of view of 100 degrees horizontal, 30 degrees
vertical, and a resolution of 1 degree azimuth and 2 degrees elevation. 
This model is expected to generate a full 4D (3D position plus speed) image of
the scene at 25-50 Hz, thanks to the embedded machine learning and SLAM
algorithms.

One of the key technologies that could lead to this achievement are metamaterial
based antennas \cite{Brookner2016,Sleasman2017} for efficient synthetic aperture
radars. Some manufacturers as Metawave (https://www.metawave.co/) are starting 
to offer products oriented to automotive sector based on the technology.

In a different line, ground-penetrating radar is a technology used long
time ago in diverse areas ranging from archeology to industrial applications.
MIT Lincoln Laboratory created a device \cite{Cornick2016} that can be placed 
below a vehicle to get a reading describing the geological properties of the
first few meters under the ground. This reading is not affected by snow, 
water, dust or any other element over the surface of the road. The idea is to
create maps that can be used later to localize vehicles, with an accuracy of
a few centimeters. \cite{XXXX}
Recently, Wavesense company (https://wavesense.io/) announced tests in snowy
places and is close to commercialize this technology.



\subsection{LiDAR}
LiDAR (Light Detection And Ranging) is an active ranging technology that calculates distance to objects by measuring round trip time of a laser light pulse.
Devices apt for robotic and automotive applications use a low power 
near-infrared laser (900-1050 nm) that is invisible, eye-safe and able to 
measure up to 200 m under direct sunlight. The long range is achieved thanks to
laser beams having a low divergence, so that the reflected power does not decay
too much with distance.
LiDARs can be used for mapping environments. Tipically, a rotating mirror is used to change the direction of the laser pulse, reaching 360ยบ horizontal coverage. Commercial solutions use an array of emitters to produce several vertical layers (between 4 and 128) that, combined with rotation, generate a point cloud that represents the the environment.

Lidar sensors feature an extraordinary accuracy measuring distances, averaging
a few millimeters in most cases and degrading to 0.1-0.5 meters in the worst 
cases. This makes Lidar the favorite choice for creating accurate digital maps.

However, they have several drawbacks to take into account:

\begin{itemize}
    \item Sparse mesaures (not dense): according to \cite{Gatziolis2008} typical
    lidar beam divergence is between 0.1 and 1 mrad. 
    The commercial device Velodyne HDL64 covers a vertical FoV of 26.8 degrees 
    using 64 layers (consecutive layers are separated 0.42 degrees), and has a
    2.0 mrad divergence \cite{Glennie2010} (0.11 degrees). Horizontal resolution
    varies between 0.1 to 0.4 degrees. In most cases, then, there is a high 
    fraction of the total volume not iluminated by laser beams. Small targets or
    structures based on threads and bars can remain undetected.
    
    \item Low vertical resolution: in low cost models, which usually feature 
    less than 16 layers, vertical resolution (separation between consecutive
    layers) falls down to 2 degrees. At 100 m distance, this is translated into 
    a vertical distance of 1.7 m. High end models reduce this gap to 0.2-0.4 
    degrees, but at a much higher cost.
    
    \item Poor detection of dark and specular objects. Black cars
    can appear as invisible to the lidar, since they combine a color that
    absorbs most radiation with a non-Lambertian material that does not scatter
    radiation back to receiver.
    
    \item High cost. High-end models have a cost between 25 and 75 k\$.
    Just a year ago, Velodyne models started at 9000 US\$ for the basic 16
    layer device, although they have cut prices down to a 50\% to face the
    recently appeared competitors. Now, some companies are selling
    equivalent models for less than 4000 US\$. Solid state lidars promise
    prices an order of magnitude smaller.
    
    \item Affected by dense rain, fog and dust. Infrared laser
    beams are affected by rain and fog because water droplets scatter the light 
    \cite{Wang2008}, reducing the operative range of the device and producing 
    false measures in the front of the cloud. The effect of dust has been
    explored in \cite{Phillips2017}. Lidar performance in these scenarios is 
    worse than radar, but still better than cameras and human eye.
\end{itemize}

\subsubsection{Emerging LiDAR technologies}

Direct speed measurement is a really usefull feature for any sensor. Up to date,
only radars where able to capure speed, for instance using FMCW signals.
The same concept has been researched along last decades \cite{Nordin2004}
applied to LiDARs that emmit light continuously to measure objects speed based 
on Doppler effect. In the last years some research prototypes suitable for
the automotive market start appearing \cite{Poulton2016}, until recently
a company named Blackmore announced a commercial version. 
Observation of speed can improve later perception stages. A direct use is to
improve tracking of moving objects, as it makes possible to detect cyclists 
and discern when pedestrians are walking thanks to the different speeds of
body and limbs.

Regarding LiDARs, however, the most popular emerging technology in the last 
years has been Solid State LiDAR. It offers advantages when it comes
to operating under strong vibrations and dynamics, apart from being potentially
smaller, cheaper and faster. 
However, the market does not offer products combining high resolution and a 
wide field of view, so mechanical devices are the only option for full 
360-degree coverage and environment mapping.

Solid state LiDAR is an umbrella term that includes several technologies, two 
of which are oscillating micro-mirrors and Optical Phased Array (OPA).
The first technology combines one or many laser emitters that are directed
with micro-mirrors that can rotate around two axes, so that the beam 
can be directed within a cone. Manufacturer LeddarTech commercializes devices
based on this technology \cite{LeddarTech2016}.
Optical phased arrays \cite{McManamon1996} is a technology similar to that used 
for radars. An array of optical emitters generate coherent signals with a well
controlled phase difference. This generates a far-field radiation pattern 
pointing in a direction that depends on the phase. This allows to control the 
direction of the beam with high accuracy and speed. Quanergy \cite{Eldada2017} 
is one of the few manufacturers commercializing devices based on this technology
with a focus on the automotive sector, its S3 model features a 120 degrees FoV
with a range of 150 m. They report a spot size of 5 cm at 100 m, that is 
comparable to a beam divergence of 0.5 mrad --4 times smaller than Velodyne 
HDL64 model.

OPA technology has additional advantages over mechanical lidars: the scan 
pattern can be random in the entire FoV, which is great for characterizing fast 
moving objects. It is possible to observe only a region of interest within the 
FoV. Also, it is possible to augment the point density within each frame for 
better resolution. The three features can be combined to do fast low resolution 
inspection of the full FoV, and then tracking with high resolution the objects
of interest for enhanced shape recognition even at far distances.
This is similar to Waymo's claims about the LiDARs they have developed for their
self-driving vehicles, as described in section \ref{sec:04-relevantdemos}.

\subsection{Using sensors for perception}

Sensor selection and arrangement is one of the most important aspects in the 
design of a perception system for automated vehicles. It also has a great impact
in its cost, with some setups having several times the price of the rest of 
the vehicle. 
This epigraph summarizes some of the most important factors to consider: type 
of information acquired, physical coverage and impact of external factors in 
performance.

In first place, the characteristics of a sensing technology determines its 
suitability for acquiring certain types of information, and restricts its range 
of operative conditions.
Figure \ref{fig:information_vs_sensors} relates the principal sensing 
technologies currently used in the automotive market and Automated Driving
initiatives with relevant types of information identified in Table 
\ref{tab:info-taxonomy}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{"img/information_types_sensors"}
    \caption{Sensor adequacy for relevant types of information}
    \label{fig:information_vs_sensors}
\end{figure}

A perception system needs to cover adequately the relevant surroundings of the
vehicle. Ideally, this includes 360 degrees around the vehicle up to several
hundreds meters. Figure \ref{fig:range-fov} shows usual operative range and
field of view of relevant sensing technologies. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{"img/plot_range-fov"}
    \caption{Range-FoV for depth sensors (3D sensing tecnology)}
    \label{fig:range-fov}
\end{figure}

Sensors and perception are expected to work uninterruptedly during vehicle 
operation. Weather and other environmental factor can degrade sensor
performance, but each technology is affected in a different way. 
Figure \ref{fig:sensors-environ} summarizes the effect of common external
factors in the performance of the analyzed sensing technologies.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{"img/sensors_atmospheric_conditions"}
    \caption{Sensor robustness under atmospheric and environmental factors}
    \label{fig:sensors-environ}
\end{figure}

