
En esta sección se discutirán las principales tecnologías de sensores (opcional: revisión de tecnologías emergentes).

Debemos cubrir el arco "sensores -> datos -> información", que lleva desde los sensores crudos hasta información relevante en AD. Esto nos permite enganchar con la sección Problems and applications.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{"img/data_vs_information"}
    \caption{Perception as a bridge between raw data and useful information}
    \label{fig:data-vs-information}
\end{figure}

The JDL Information Fusion Model \cite{Llinas2000} is a well-known and 
widely used framework for discussions on sensor and information fusion systems.
It has been explored in the context of automotive domain
\cite{Polychronopoulos2006} and has been used as base for defining
automotive-specific sensor fusion frameworks \cite{Tatschke2006}. 
Perception layer is usually define to cover JDL levels 0 (raw data
pre-processing) and 1 (object refinement), while higher abstraction processes
belonging to levels 2 (situation refinement) and 3 (threat refinement) are
usually placed in the decision modules.
Level 4 (process refinement) is not contemplated, while Level 5 (user reinement)
is mostly related with HMIs and shared control or arbitration schemes, but
left out of the scope of this work.


\subsection{Relevant information domains}

The task of a perception system is to bridge the gap between sensors providing 
data, and decision algorithms requiring information.
A classical differentiation between both terms is the following: data is 
composed by raw, unorganized facts that need to be processed. 
Information is the name given to data that has been processed, organized, 
structured and presented in a proper context.

The fields of study of ADAS and Automated Driving systems are large and 
heterogeneous.
This makes necessary to use many types of information, that can be organized 
according to the following taxonomy / The following taxonomy gives an idea of 
the heterogeneity and complexity of ADAS/AD problems domain:

\begin{table}[H]
    \caption{Information taxonomy in Automated Driving domain}
    %\centering
    %% \tablesize{} %% You can specify the fontsize here, e.g.  \tablesize{\footnotesize}. If commented out \small will be used.
    \begin{tabularx}{\linewidth}{lrL}
        \toprule
        \textbf{Category} & \textbf{\#}	& \textbf{Information type}	\\
        \midrule
        \multirow{2}{*}{Ego-vehicle}
         & 1 & Kinematic/dynamic (includes position) \\
         & 2 & Proprioceptive (components health/status) \\
         \midrule
         \multirow{3}{*}{Passengers/driver}
         & 3 & Driver awareness/capacities \\
         & 4 & * Driver intentions (mind model)  \\
         & 5 & Passenger status (needs, risk factors) \\
         \midrule
         \multirow{4}{*}{Environment}
         & 6 & Spatial configuration: location, size, shape, fine features 
         \\
         & 7 & Identification: class, type, identity \\
         & 8 & Regulation and semantics: traffic signs, road marks, other 
         elements \\
         & 9 & Contextual factors: weather, driving situation(e.g. jam, 
         highway, off-road) \\
         \midrule
         \multirow{4}{*}{External actors}
         & 10 & Spatial features: location, size, shape, fine features  \\
         & 11 & Kinematic/dynamic: position, motion \\
         & 12 & Identification: class, type, identity \\ 
         & 13 & Semantic features: vehicle lights, pedestrian clothes, gestures 
         \\
         & 14 & * Situational engagement: collaborative/aware 
         (adult pedestrians, other vehicles) vs non-collaborative/unaware 
         (animals, children) \\ 
        \bottomrule
    \end{tabularx}
\end{table}

Elements with an asterisk are derived information. This is, something that can 
be inferred from sensed data but not directly observed. It is mostly related 
with internal state of external entities, as the intentions of human beings and 
animals. These items also happen to belong to higher levels in the JDL model.

In this survey we are focused in perception of external elements, that is, the
two last categories.

\subsection{Principal sensor technologies for perception}

There are a few categories of sensors that are included in almost every ADAS/AD
system and are sufficient to cover any reasonable perception need. 
These categories are: artificial vision, radar, Lidar and 
ultrasonic.

Most of them are ranging sensors: the measured magnitude is distance to 
external objects, by means of the round-trip delay time of a electromagnetic 
wave (radar), a pulse of structured light (Lidar) or sound (ultrasonic).
Artificial vision, in its most basic form, registers a grid of intensities and 
colors. This allows to extract aditional information about the world, including 
traffic elements that are encoded visually to allow being interpreted by humans 
(signals, lanes, traffic lights).

Sensors provide raw data that sometimes can be used directly as useful
information (e.g. GPS giving location/speed), or that must be processed/fused.

Tabla: qué sensores son útiles para obtener cada tipo de información.

\begin{table}[H]
    \caption{Sensor technologies and proposed uses}
    %\centering
    %% \tablesize{} %% You can specify the fontsize here, e.g.  \tablesize{\footnotesize}. If commented out \small will be used.
    \begin{tabularx}{\linewidth}{cLL}
        \toprule
        \textbf{Type}	& \textbf{Sensor type}	&  \textbf{Applications}\\
        \midrule
        \multirow{4}{*}{Artificial vision}	
        & Monocular single camera   & \gls{fcw} \cite{Dagan2004}, \gls{ldw}, \gls{acc} \cite{Stein2003}, road marks/signs detection, visual odometry for ego-motion \cite{Scaramuzza2008,Nguyen2013,Bak2012}, Road boundary detection for run-off road prevention based on the fusion of video and radar \cite{Janda2013} \\
        & Monocular multicamera     & \cite{Ieng2003} lateral cameras for lane detection fused with GPS for enhanced location \\
        & Stereo                    & - \\
        & Non-visible spectrum      & Pedestrian detection \cite{Besbes2015} \\
        & Time Gated CMOS           & see https://www.brightwayvision.com/technology/ \\
        \midrule
        \multirow{5}{*}{Ranging}
        & Radar                 & Ground+obstacle detection with mm-wave radar \cite{Reina2015}, Road boundary detection for run-off road prevention based on the fusion of video and radar \cite{Janda2013} \\
        & Lidar (single layer)  & - \\
        & Lidar (multilayer)    & Rapid Inspection of Pavement Markings Using Mobile Laser Scanning Point Clouds \cite{Zhang2016} \\
        & Ultrasounds           & Neighbor vehicle misbehavior \cite{Liu2015}, also for (taken from that work) ''vehicle tracking and classification [7,22–25], obstacle detection and mapmaking [26,27], vacant parking slot detection [28], smart traffic signaling [29], ultrasonic ranging and localization [30]'' \\
        & Structured light    & -  \\
        & Time-of-flight       & - \\
        & Time Gated CMOS     & (also in Artificial Vision) \\
%        \midrule
%        \multirow{3}{*}{Egomotion}
%        & GNSS      & Positioning, speed \\
%        & IMU       & Orientation (inclination, heading), change of speed \\	
%        & Odometer  & Speed \\
        
        \bottomrule
    \end{tabularx}
\end{table}

In \cite{Lundquist2011} we can find a nice review of sensor/algorithms for road 
related applications: lane detection, road geometry estimation, etc.
The work \cite{Yebes2015} talks about infering 2.5D or 3D features from monocular 
vision.
Pot-hole detection using monocular vision
Road condition with on-board sensors in \cite{CastilloAguilar2015} (tire slippage).

Monocular vision is the most popular technique due to the low cost of sensors and 
having a large research community with a high maturity degree.

%\begin{figure}[h]
%    \centering
%    
%\includegraphics[width=0.95\textwidth]{"img/imaging-technologies-for-automotive-2016-report-by-yole"}
%    \caption{Ranging technologies}
%    \label{fig:ranging-techs}
%\end{figure}

%\begin{table}[H]
%    \caption{Features of spatial sensor technologies 
%([D]etection/[I]dentification, [G]ood/[M]edium/[B]ad)}
%    \begin{tabularx}{\linewidth}{Lcccccccc}
%        \toprule
%        \textbf{Technology} & \textbf{veh} & \textbf{ped} & \textbf{signals} & 
%\textbf{road marks} & \textbf{other} & \textbf{dense} & \textbf{resol} & 
%\textbf{fog/rain/snow} \\
%        \midrule
%        Mono (visible) 		& D/I & D/I & D/I & D/I & D/I & G & G & M/B \\
%        Mono (near IR) 		& - & - & - & - & - & - & - & - \\	
%        Mono (far IR) 		& - & - & - & - & - & - & - & - \\	
%        Stereo (visible) 	& - & - & - & - & - & - & - & - \\	
%        Time-of-flight 		& - & - & - & - & - & - & - & - \\	
%        Time Gated 			& - & - & - & - & - & - & - & - \\	
%        Structured Light 	& - & - & - & - & - & - & - & - \\	
%        Radar 				& - & - & - & - & - & - & - & - \\	
%        Lidar 				& - & - & - & - & - & - & - & - \\	
%        Ultrasounds 		& - & - & - & - & - & - & - & - \\		
%        \bottomrule
%    \end{tabularx}
%\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{"img/plot_range-fov"}
    \caption{Range-FoV for depth sensors (3D sensing tecnology)}
    \label{fig:range-fov}
\end{figure}

\subsection{Artificial Vision}
Artificial vision is a popular technology that has been used for decades in 
disciplines as mobile robotics, surveillance, industrial inspection. It acquires
information about objects in the real world by analyzing their images as 
captured by photo and video cameras. Cameras are devices that gather light using
a sensor composed by a grid of thousands or millions of individual detection 
elements. The amount of light captured by each element is translated more or 
less directly into the intensity of a pixel in the resulting image. 

This technology offers interesting features, as the low cost of sensors --only 
some types-- and providing range of information types including spatial
(shape, size, distances), dynamic (motion of objects by analyzing their 
displacement between consecutive frames) and semantic (shape analysis).

Artificial vision technology face several challenges, especially in 
applications like automated driving:

\begin{itemize}
    \item Varying light conditions: driving is not limited to daylight 
    conditions. It can also happen at night, indoors, or even worse,
    during dusk or dawn with the sun close to the horizon. Dark spots,
    shadows and other effects difficult the implementation of reliable 
    artificial visible algorithms.
    
    \item High dynamic range in the scene (some areas are very dark 
    and some others are strongly illuminated): while varying light conditions
    can be mitigated through dynamimc exposure mechanisms, HDR conditions
    are more difficult to deal with. Most sensor technologies have a 
    limited capacity of capturing both extremes simultaneously, so that 
    information is lost in one or the two sides. In automotive applications, 
    this can mean detecting lane lines at the cost of not seeing preceding
    vehicles or vicecersa.
    
    
    \item Objects moving at great speed: cameras need higher exposures time as
    illumination is weaker. Fast moving elements appear blurred, which can 
    affect later processes as border or feature detection. Also, if the sensor
    does not capture light in its full surface simultaneously (rolling shutter)
\end{itemize}

In \cite{Pueo2016} some of these problems are analyzed from the perspective of
recording scenes in sports.
In order to deal with these difficulties, different technologies and solutions 
have been proposed. 

\begin{itemize}
    \item Rolling vs. Global shutter sensors. In rolling
    shutter cameras, either sensor technology (rows activated sequentially) or
    mechanical elements (a physical shutter moving to expose and oclude the 
    sensor), the elements of the sensor capture light at differents time 
    intervals. This have negative effects as: fast moving objects appearing
    distorted, or flickering lights (fluorescents, LEDs) creating images with 
    some parts illuminated and others dark. Rolling shutter effects
    can be corrected by compensating scene motion vector, as proposed in 
    \cite{Chia-KaiLiang2008}\cite{Chun2008}.
    Global shutter cameras, on the other hand, have the ability to capture 
    light in all the elements of the sensor simultaneously.
    
    \item High Dynamic Range imaging (HDR): common sensors in photographic and 
    industrial cameras offer a dynamic range of 60-75 dB (10 to 12.5 EVs),
    that is not sufficient in mixed illumination environments as entering or 
    exiting tunnels. Sony launched its IMX390 automotive sensor with an
    extended 120 dB range (equivalent to 20 EVs) and 2k resolution. 
    An automotive grade sensor combining HDR capabilities and 
    Near Infra-Red light detection is analyzed in \cite{Maddalena2005}. 
    In \cite{Strobel2013} a sensor with 130 dB range (global shutter) and up
    to 170 dB (rolling shutter) is presented for industrial safety application. 
    
    \item Refresh rate:
    
    \item Captured spectrum: apart from visible light cameras, either in color
    or grayscale, automotive applications have used infrared sensors. We can 
    distinguish to families: far infrared cameras (wavelength 900-1400 nm), 
    also known as thermal cameras, detect the emissions of hot objects, 
    including living beings. Thermal cameras are effective for pedestrian and
    animal detection \cite{OMalley2008}\cite{Besbes2015} in the dark and 
    through dust and smoke. FLIR manufactured a system that was integrated
    in BMW's NightVision system since 2009 cameras.
\end{itemize} 

Event-based vision is a bioinspired technology developed by Zurich University
and the ETH Zurich. It approaches the problem of processing and tracking fast 
moving images in a completely different way: instead of capturing fixed frames
at discrete times, the elements of the sensor are triggered asynchronously and
indepently when a change on the intensity is detected. Each activation is
called an event, and can be seen as something similar to the output of a 
feature detection algorithm for artificial vision applications.
Events can be grouped in time windows for getting a frame-like image, but this
window can be dynamically adapted for attending different time scales.
Sensor time resolution goes down to microseconds, and \cite{Mueggler2014} shows 
integration windows of 1 ms (~1000 fps) in regular indoor lightning conditions
to be sufficient for high speed tracking, thanks also to the 120 dB dynamic 
range of the sensor.
An additional advantage is that the output can be used in raw form for 
applications as visual odometry \cite{Censi2014} and SLAM \cite{Vidal2017}
relieving the CPU of time consuming operations on raw images.
More recently, a steering wheel control for automated driving systems 
based on deep learning has been shown in \cite{Maqueda2018}.
 
 
 
 Talk about 3D technologies??? estéreo, ToF, structured light, 
 

\subsection{Radar}

Radar technology use high frequency electromagnetic waves to measure the
distance to objects in the environment.
There are two principal technologies: pulsed radar emits a short burst and 
measures distance using the round trip time. Most modern automotive 
radars use Frequency-Modulated Continuous Wave (FMCW) where a signal with
a well known and stable frequency is modulated with another continuous signal
that varies its frequency up and down (tipically using a triangular shape).
Distance is determined using the frequency shift between the emitted and 
reflected signals. Both technologies allow to measure target speed based
on Doppler effect.

One of the strongest arguments for including radar sensing in automated 
vehicles is its independence of light and weather conditions. 
It works in the dark, and detections are almost equally good with snow, 
rain, fog or dust \cite{Reina2015}.
However, radar processing can be tricky due to the reflectivity of the
different materials: metals amplify radar signal, easening detection of 
vehicles but increasing the apparent size of small objects as discarded cans
in the road, while other materials (e.g. wood) are virtually transparent.

Starting in 2004, EU allocated a permanent 5 GHz wide band around 79 GHz 
\cite{EULawandPublications2004}. 
Short distance applications as blind spot detection, parking assistance, jam 
assistance and pre-crash measures use the upper part (77-81 GHz), since it 
offers a better resolution. Long distance applications as ACC use a radar 
signal around 76-78 GHz. However, manufacturers are implementing multifrequency 
chips that can switch between different functions dinamycally.
%This information is resumed in figure \ref{fig:radar-freqband}

As Automated Driving technology advances, radars are expected to go from its
current use of detecting and measuring speed to targets to provide richer
semantic information. 
Recent research turns around using higher frequency bands to increase
resolution. This results in better shape recognition and separation of close
targets that current technology reports as a single object.
These features open the door to radar imaging and creation of detailed 3D maps. 
An example can be found in \cite{Reina2015}, where a 90GHz rotating radar in
the roof of a car is used to map the environment, including not only other
vehicles and static objects but also the ground.
In \cite{Kohler2013} explores the feasibility of radars operating between 
100 and 300 GHz. They present a prototype working at 150 GHz and conclude 
that atmosferic absorption and reflectivity of materials usually found in 
driving scenarios make this technology feasible, with the benefit of a high
resolution that may enable radar imaging.


%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.8\textwidth]{"img/infineon_radar_bands_chart"}
%    \caption{Operating frequency bands of automotive radars. Taken from 
%Lachner, R. (2009). Development Status of Next Generation Automotive Radar in 
%EU. Tokyo: ITS Forum. Available at 
%http://www.itsforum.gr.jp/Public/J3Schedule/P22/lachner090226.pdf}
%    \label{fig:radar-freqband}
%\end{figure}

\subsection{LiDAR}
LiDAR (Light Detection And Ranging) is an active ranging technology that calculates distance to objects by measuring round trip time of a laser light pulse.
Devices apt for robotic and automotive applications use a low power 
near-infrared laser (900-1050 nm) that is invisible, eye-safe and able to 
measure up to 200 m under direct sunlight. The long range is achieved thanks to
laser beams having a low divergence, so that the reflected power does not decay
too much with distance.
LiDARs can be used for mapping environments. Tipically, a rotating mirror is used to change the direction of the laser pulse, reaching 360º horizontal coverage. Commercial solutions use an array of emitters to produce several vertical layers (between 4 and 128) that, combined with rotation, generate a point cloud that represents the the environment.

Lidar sensors feature an extraordinary accuracy measuring distances, averaging
a few millimeters in most cases and degrading to 0.1-0.5 meters in the worst 
cases. This makes Lidar the favorite choice for creating accurate digital maps.

However, they have several drawbacks to take into account:

\begin{itemize}
    \item Sparse mesaures (not dense): according to \cite{Gatziolis2008} typical
    lidar beam divergence is between 0.1 and 1 mrad. 
    The commercial device Velodyne HDL64 covers a vertical FoV of 26.8 degrees 
    using 64 layers (consecutive layers are separated 0.42 degrees), and has a
    2.0 mrad divergence \cite{Glennie2010} (0.11 degrees). Horizontal resolution
    varies between 0.1 to 0.4 degrees. In most cases, then, there is a high 
    fraction of the total volume not iluminated by laser beams. Small targets or
    structures based on threads and bars can remain undetected.
    
    \item Low vertical resolution: in low cost models, which usually feature 
    less than 16 layers, vertical resolution (separation between consecutive
    layers) falls down to 2 degrees. At 100 m distance, this is translated into 
    a vertical distance of 1.7 m. High end models reduce this gap to 0.2-0.4 
    degrees, but at a much higher cost.
    
    \item Poor detection of dark and specular objects. Black cars
    can appear as invisible to the lidar, since they combine a color that
    absorbs most radiation with a non-Lambertian material that does not scatter
    radiation back to receiver.
    
    \item High cost. High-end models have a cost between 25 and 75 k\$.
    Just a year ago, Velodyne models started at 9000 US\$ for the basic 16
    layer device, although they have cut prices down to a 50\% to face the
    recently appeared competitors. Now, some companies are selling
    equivalent models for less than 4000 US\$. Solid state lidars promise
    prices an order of magnitude smaller.
    
    \item Affected by dense rain, fog and dust. Infrared laser
    beams are affected by rain and fog because water droplets scatter the light 
    \cite{Wang2008}, reducing the operative range of the device and producing 
    false measures in the front of the cloud. The effect of dust has been
    explored in \cite{Phillips2017}. Lidar performance in these scenarios is 
    worse than radar, but still better than cameras and human eye.
\end{itemize}

In the last years, a FLASH based technology also named as solid state LiDAR
started to arrived to commercial products. It offers advantages when it comes
to operating under strong vibrations and dynamics, apart from being potentially
smaller, cheaper and faster. 
However, the market does not offer products combining high resolution and a 
wide field of view, so mechanical devices are the only option for full 
360-degree coverage and environment mapping.

There are two potential technologies for solid state LiDARs: oscillating
micro-mirrors and optical phased array (as Quanergy line of products).
The first technology combines one or many laser emitters that are directed
with micro-mirrors that can rotate around two axes, so that the beam 
can be directed within a cone. Manufacturer LeddarTech commercializes devices
based on this technology \cite{LeddarTech2016}.
Optical phased arrays \cite{McManamon1996} is a technology similar to that used 
for radars. An array of optical emitters generate coherent signals with a well
controlled phase difference. This generates a far-field radiation pattern 
pointing in a direction that depends on the phase. This allows to control the 
direction of the beam with high accuracy and speed. Quanergy \cite{Eldada2017} 
is one of the few manufacturers commercializing devices based on this technology
with a focus on the automotive sector, its S3 model features a 120 degrees FoV
with a range of 150 m. They report a spot size of 5 cm at 100 m, that is 
comparable to a beam divergence of 0.5 mrad --4 times smaller than Velodyne 
HDL64 model.

OPA technology has additional advantages over mechanical lidars: the scan 
pattern can be random in the entire FoV, which is great for characterizing fast 
moving objects. It is possible to observe only a region of interest within the 
FoV. Also, it is possible to augment the point density within each frame for 
better resolution. The three features can be combined to do fast low resolution 
inspection of the full FoV, and then tracking with high resolution the objects
of interest for enhanced shape recognition even at far distances.


%\begin{figure}[h]
%    \centering
%    
%\includegraphics[width=0.85\textwidth]{"img/Freescale_factTable_sensorApplication"}
%    \caption{Applications of 77 GHz radar to ADAS}
%    \label{fig:freescale}
%\end{figure}




\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{"img/information_types_sensors"}
    \caption{Sensor adequacy for relevant types of information}
    \label{fig:information_vs_sensors}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{"img/sensors_atmospheric_conditions"}
    \caption{Sensor robustness under atmospheric and environmental factors}
    \label{fig:radar-freqband}
\end{figure}


