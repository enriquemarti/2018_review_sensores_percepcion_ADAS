% Encoding: UTF-8


@InProceedings{Bak2012,
  Title                    = {Multi-sensor localization - Visual Odometry as a low cost proprioceptive sensor},
  Author                   = {Bak, Adrien and Gruyer, Dominique and Bouchafa, Samia and Aubert, Didier},
  Booktitle                = {2012 15th International IEEE Conference on Intelligent Transportation Systems},
  Year                     = {2012},
  Month                    = {sep},
  Pages                    = {1365--1370},
  Publisher                = {IEEE},

  Doi                      = {10.1109/ITSC.2012.6338771},
  ISBN                     = {978-1-4673-3063-3},
  Url                      = {http://ieeexplore.ieee.org/document/6338771/}
}

@Article{Besbes2015,
  Title                    = {Pedestrian detection in far-infrared daytime images using a hierarchical codebook of SURF},
  Author                   = {Besbes, Bassem and Rogozan, Alexandrina and Rus, Adela Maria and Bensrhair, Abdelaziz and Broggi, Alberto},
  Journal                  = {Sensors (Switzerland)},
  Year                     = {2015},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {8570--8594},
  Volume                   = {15},

  Abstract                 = {One of the main challenges in intelligent vehicles concerns pedestrian detection for driving assistance. Recent experiments have showed that state-of-the-art descriptors provide better performances on the far-infrared (FIR) spectrum than on the visible one, even in daytime conditions, for pedestrian classification. In this paper, we propose a pedestrian detector with on-board FIR camera. Our main contribution is the exploitation of the specific characteristics of FIR images to design a fast, scale-invariant and robust pedestrian detector. Our system consists of three modules, each based on speeded-up robust feature (SURF) matching. The first module allows generating regions-of-interest (ROI), since in FIR images of the pedestrian shapes may vary in large scales, but heads appear usually as light regions. ROI are detected with a high recall rate with the hierarchical codebook of SURF features located in head regions. The second module consists of pedestrian full-body classification by using SVM. This module allows one to enhance the precision with low computational cost. In the third module, we combine the mean shift algorithm with inter-frame scale-invariant SURF feature tracking to enhance the robustness of our system. The experimental evaluation shows that our system outperforms, in the FIR domain, the state-of-the-art Haar-like Adaboost-cascade, histogram of oriented gradients (HOG)/linear SVM (linSVM) and MultiFtrpedestrian detectors, trained on the FIR images.},
  Doi                      = {10.3390/s150408570},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Besbes et al. - 2015 - Pedestrian Detection in Far-Infrared Daytime Images Using a Hierarchical Codebook of SURF.pdf:pdf},
  ISSN                     = {14248220},
  Keywords                 = {Far-infrared images,Hierarchical codebook,Pedestrian classification and trackings,Pedestrian detection,SURF,SVM,Scale-invariant feature matching},
  Pmid                     = {25871724},
  Publisher                = {Multidisciplinary Digital Publishing Institute},
  Url                      = {http://www.mdpi.com/1424-8220/15/4/8570/}
}

@Article{CastilloAguilar2015,
  Title                    = {Robust road condition detection system using in-vehicle standard sensors},
  Author                   = {{Castillo Aguilar}, Juan Jes{\'{u}}s and {Cabrera Carrillo}, Juan Antonio and {Guerra Fern{\'{a}}ndez}, Antonio Jes{\'{u}}s and {Carabias Acosta}, Enrique},
  Journal                  = {Sensors (Switzerland)},
  Year                     = {2015},

  Month                    = {dec},
  Number                   = {12},
  Pages                    = {32056--32078},
  Volume                   = {15},

  Abstract                 = {The appearance of active safety systems, such as Anti-lock Braking System, Traction Control System, Stability Control System, etc., represents a major evolution in road safety. In the automotive sector, the term vehicle active safety systems refers to those whose goal is to help avoid a crash or to reduce the risk of having an accident. These systems safeguard us, being in continuous evolution and incorporating new capabilities continuously. In order for these systems and vehicles to work adequately, they need to know some fundamental information: the road condition on which the vehicle is circulating. This early road detection is intended to allow vehicle control systems to act faster and more suitably, thus obtaining a substantial advantage. In this work, we try to detect the road condition the vehicle is being driven on, using the standard sensors installed in commercial vehicles. Vehicle models were programmed in on-board systems to perform real-time estimations of the forces of contact between the wheel and road and the speed of the vehicle. Subsequently, a fuzzy logic block is used to obtain an index representing the road condition. Finally, an artificial neural network was used to provide the optimal slip for each surface. Simulations and experiments verified the proposed method.},
  Doi                      = {10.3390/s151229908},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Castillo Aguilar et al. - 2015 - Robust Road Condition Detection System Using In-Vehicle Standard Sensors.pdf:pdf},
  ISSN                     = {14248220},
  Keywords                 = {Friction estimation,Normal driving,Optimal slip estimation,Standard vehicle sensor},
  Pmid                     = {26703605},
  Publisher                = {Multidisciplinary Digital Publishing Institute},
  Url                      = {http://www.mdpi.com/1424-8220/15/12/29908}
}

@Article{chen2017turn,
  Title                    = {Turn signal detection during nighttime by CNN detector and perceptual hashing tracking},
  Author                   = {Chen, Long and Hu, Xuemin and Xu, Tong and Kuang, Hulin and Li, Qingquan},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2017},
  Number                   = {12},
  Pages                    = {3303--3314},
  Volume                   = {18},

  Publisher                = {IEEE}
}

@Article{coronado2012detection,
  Title                    = {Detection and classification of road signs for automatic inventory systems using computer vision},
  Author                   = {Coronado, Gustavo A Pel{\'a}ez and Mu{\~n}oz, Mar{\'\i}a Romero and Armingol, Jos{\'e} Mar{\'\i}a and de la Escalera, Arturo and Mu{\~n}oz, Juan Jes{\'u}s and van Bijsterveld, Wouter and Bola{\~n}o, Juan Antonio},
  Journal                  = {Integrated Computer-Aided Engineering},
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {285--298},
  Volume                   = {19},

  Publisher                = {IOS Press}
}

@InProceedings{Dagan2004,
  Title                    = {Forward collision warning with a single camera},
  Author                   = {Dagan, E. and Mano, O. and Stein, G.P. and Shashua, A.},
  Booktitle                = {IEEE Intelligent Vehicles Symposium, 2004},
  Year                     = {2004},
  Pages                    = {37--42},
  Publisher                = {IEEE},

  Abstract                 = {The large number of rear end collisions due to driver inattention has been identified as a major automotive safety issue. Even a short advance warning can significantly reduce the number and severity of the collisions. This paper describes a vision based forward collision warning (FCW) system for highway safety. The algorithm described in this paper computes time to contact (TTC) and possible collision course directly from the size and position of the vehicles in the image - which are the natural measurements for a vision based system - without having to compute a 3D representation of the scene. The use of a single low cost image sensor results in an affordable system which is simple to install. The system has been implemented on real-time hardware and has been test driven on highways. Collision avoidance tests have also been performed on test tracks.},
  Doi                      = {10.1109/IVS.2004.1336352},
  ISBN                     = {0-7803-8310-9},
  Url                      = {http://ieeexplore.ieee.org/document/1336352/}
}

@InProceedings{frejlichowski2015application,
  Title                    = {Application of the Polar--Fourier Greyscale Descriptor to the Automatic Traffic Sign Recognition},
  Author                   = {Frejlichowski, Dariusz},
  Booktitle                = {International Conference Image Analysis and Recognition},
  Year                     = {2015},
  Organization             = {Springer},
  Pages                    = {506--513}
}

@InProceedings{gao2015learning,
  Title                    = {Learning local histogram representation for efficient traffic sign recognition},
  Author                   = {Gao, Jinlu and Fang, Yuqiang and Li, Xingwei},
  Booktitle                = {Image and Signal Processing (CISP), 2015 8th International Congress on},
  Year                     = {2015},
  Organization             = {IEEE},
  Pages                    = {631--635}
}

@Article{gargoum2017automated,
  Title                    = {Automated highway sign extraction using lidar data},
  Author                   = {Gargoum, Suliman and El-Basyouny, Karim and Sabbagh, Joseph and Froese, Kenneth},
  Journal                  = {Transportation Research Record: Journal of the Transportation Research Board},
  Year                     = {2017},
  Number                   = {2643},
  Pages                    = {1--8},

  Publisher                = {Transportation Research Board of the National Academies}
}

@InProceedings{gu2011traffic,
  Title                    = {Traffic sign detection in dual-focal active camera system},
  Author                   = {Gu, Yanlei and Yendo, Tomohiro and Tehrani, Mehrdad Panahpour and Fujii, Toshiaki and Tanimoto, Masayuki},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2011 IEEE},
  Year                     = {2011},
  Organization             = {IEEE},
  Pages                    = {1054--1059}
}

@Article{guan2018robust,
  Title                    = {Robust Traffic-Sign Detection and Classification Using Mobile LiDAR Data With Digital Images},
  Author                   = {Guan, Haiyan and Yan, Wanqian and Yu, Yongtao and Zhong, Liang and Li, Dilong},
  Journal                  = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  Year                     = {2018},

  Publisher                = {IEEE}
}

@Article{hillel2014recent,
  Title                    = {Recent progress in road and lane detection: a survey},
  Author                   = {Hillel, Aharon Bar and Lerner, Ronen and Levi, Dan and Raz, Guy},
  Journal                  = {Machine vision and applications},
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {727--745},
  Volume                   = {25},

  Publisher                = {Springer}
}

@Article{hosseinyalamdary2017bayesian,
  Title                    = {A Bayesian approach to traffic light detection and mapping},
  Author                   = {Hosseinyalamdary, Siavash and Yilmaz, Alper},
  Journal                  = {ISPRS journal of photogrammetry and remote sensing},
  Year                     = {2017},
  Pages                    = {184--192},
  Volume                   = {125},

  Publisher                = {Elsevier}
}

@InProceedings{Ieng2003,
  Title                    = {Merging lateral cameras information with proprioceptive sensors in vehicle location gives centimetric precision},
  Author                   = {Ieng, S-S and Gruyer, D},
  Booktitle                = {Proceedings of the 18th International Technical Conference on the Enhanced Safety of Vehicles},
  Year                     = {2003},

  Address                  = {Nagoya, Japan},
  Month                    = {may},
  Publisher                = {National Highway Traffic Safety Administration},

  Url                      = {https://trid.trb.org/view.aspx?id=750799}
}

@InProceedings{Janda2013,
  Title                    = {Road boundary detection for run-off road prevention based on the fusion of video and radar},
  Author                   = {Janda, Florian and Pangerl, Sebastian and Lang, Eva and Fuchs, Erich},
  Booktitle                = {IEEE Intelligent Vehicles Symposium, Proceedings},
  Year                     = {2013},
  Month                    = {jun},
  Pages                    = {1173--1178},
  Publisher                = {IEEE},

  Abstract                 = {An approach for detecting the road boundary on different types of roads without any preliminary knowledge is presented. We fuse information obtained from an algorithm which detects road markings and road edges in images acquired by a video camera as well as data from a radar sensor. Each road marking, each road edge and each road barrier is tracked individually. Hence we can even capture exits or laybys. We use an edge image for road marking detection and texture information for road edge detection. Additional data provided by a radar sensor is used to measure targets referring to static barriers along the road side such as guardrails. The output of each processing unit is fused into a Kalman filter framework, where the confidence of each subsystem influences the innovation of the overall system. The underlying geometric road model comprises parameters for multiple lanes, the flanking road edge as well as the vehicle's relative pose. The work is part of the project Interactive.},
  Doi                      = {10.1109/IVS.2013.6629625},
  ISBN                     = {9781467327558},
  ISSN                     = {1931-0587},
  Url                      = {http://ieeexplore.ieee.org/document/6629625/}
}

@Article{kaliyaperumal2001algorithm,
  Title                    = {An algorithm for detecting roads and obstacles in radar images},
  Author                   = {Kaliyaperumal, Kesav and Lakshmanan, Sridhar and Kluge, Karl},
  Journal                  = {IEEE Transactions on Vehicular Technology},
  Year                     = {2001},
  Number                   = {1},
  Pages                    = {170--182},
  Volume                   = {50},

  Publisher                = {IEEE}
}

@InProceedings{kim2015lane,
  Title                    = {Lane map building and localization for automated driving using 2D laser rangefinder},
  Author                   = {Kim, Dongwook and Chung, Taeyoung and Yi, Kyongsu},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2015 IEEE},
  Year                     = {2015},
  Organization             = {IEEE},
  Pages                    = {680--685}
}

@Article{Koch2011,
  Title                    = {Pothole detection in asphalt pavement images},
  Author                   = {Koch, Christian and Brilakis, Ioannis},
  Journal                  = {Advanced Engineering Informatics},
  Year                     = {2011},

  Month                    = {aug},
  Number                   = {3},
  Pages                    = {507--515},
  Volume                   = {25},

  Abstract                 = {Pavement condition assessment is essential when developing road network maintenance programs. In practice, the data collection process is to a large extent automated. However, pavement distress detection (cracks, potholes, etc.) is mostly performed manually, which is labor-intensive and time-consuming. Existing methods either rely on complete 3D surface reconstruction, which comes along with high equipment and computation costs, or make use of acceleration data, which can only provide preliminary and rough condition surveys. In this paper we present a method for automated pothole detection in asphalt pavement images. In the proposed method an image is first segmented into defect and non-defect regions using histogram shape-based thresholding. Based on the geometric properties of a defect region the potential pothole shape is approximated utilizing morphological thinning and elliptic regression. Subsequently, the texture inside a potential defect shape is extracted and compared with the texture of the surrounding non-defect pavement in order to determine if the region of interest represents an actual pothole. This methodology has been implemented in a MATLAB prototype, trained and tested on 120 pavement images. The results show that this method can detect potholes in asphalt pavement images with reasonable accuracy. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
  Doi                      = {10.1016/j.aei.2011.01.002},
  ISBN                     = {1474-0346},
  ISSN                     = {14740346},
  Keywords                 = {Image processing,Pavement assessment,Pothole detection,Visual sensing},
  Publisher                = {Elsevier},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1474034611000036}
}

@InProceedings{kum2013lane,
  Title                    = {Lane detection system with around view monitoring for intelligent vehicle},
  Author                   = {Kum, Chang-Hoon and Cho, Dong-Chan and Ra, Moon-Soo and Kim, Whoi-Yul},
  Booktitle                = {SoC Design Conference (ISOCC), 2013 International},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {215--218}
}

@InProceedings{lee2017avm,
  Title                    = {AVM/LiDAR sensor based lane marking detection method for automated driving on complex urban roads},
  Author                   = {Lee, Hyunsung and Kim, Seonwook and Park, Sungyoul and Jeong, Yonghwan and Lee, Hojoon and Yi, Kyongsu},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2017 IEEE},
  Year                     = {2017},
  Organization             = {IEEE},
  Pages                    = {1434--1439}
}

@InProceedings{li2013new,
  Title                    = {A new 3D LIDAR-based lane markings recognition approach},
  Author                   = {Li, Tan and Zhidong, Deng},
  Booktitle                = {Robotics and Biomimetics (ROBIO), 2013 IEEE International Conference on},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {2197--2202}
}

@Article{liu2014traffic,
  Title                    = {Traffic sign recognition using group sparse coding},
  Author                   = {Liu, Huaping and Liu, Yulong and Sun, Fuchun},
  Journal                  = {Information Sciences},
  Year                     = {2014},
  Pages                    = {75--89},
  Volume                   = {266},

  Publisher                = {Elsevier}
}

@Article{Liu2015,
  Title                    = {An ultrasonic sensor system based on a two-dimensional state method for highway vehicle violation detection applications},
  Author                   = {Liu, Jun and Han, Jiuqiang and Lv, Hongqiang and Li, Bing},
  Journal                  = {Sensors (Switzerland)},
  Year                     = {2015},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {9000--9021},
  Volume                   = {15},

  Abstract                 = {With the continuing growth of highway construction and vehicle use expansion all over the world, highway vehicle traffic rule violation (TRV) detection has become more and more important so as to avoid traffic accidents and injuries in intelligent transportation systems (ITS) and vehicular ad hoc networks (VANETs). Since very few works have contributed to solve the TRV detection problem by moving vehicle measurements and surveillance devices, this paper develops a novel parallel ultrasonic sensor system that can be used to identify the TRV behavior of a host vehicle in real-time. Then a two-dimensional state method is proposed, utilizing the spacial state and time sequential states from the data of two parallel ultrasonic sensors to detect and count the highway vehicle violations. Finally, the theoretical TRV identification probability is analyzed, and actual experiments are conducted on different highway segments with various driving speeds, which indicates that the identification accuracy of the proposed method can reach about 90.97{\%}.},
  Doi                      = {10.3390/s150409000},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - An Ultrasonic Sensor System Based on a Two-Dimensional State Method for Highway Vehicle Violation Detection Applicat.pdf:pdf},
  ISSN                     = {14248220},
  Keywords                 = {Highway vehicle traffic rule violation detection,Intelligent transportation systems,Two-dimensional state method,Ultrasonic sensor system},
  Pmid                     = {1603305},
  Publisher                = {Multidisciplinary Digital Publishing Institute},
  Url                      = {http://www.mdpi.com/1424-8220/15/4/9000/}
}

@Article{Lundquist2011,
  Title                    = {Joint ego-motion and road geometry estimation},
  Author                   = {Lundquist, Christian and Sch{\"{o}}n, Thomas B.},
  Journal                  = {Information Fusion},
  Year                     = {2011},

  Month                    = {oct},
  Number                   = {4},
  Pages                    = {253--263},
  Volume                   = {12},

  Abstract                 = {We provide a sensor fusion framework for solving the problem of joint ego-motion and road geometry estimation. More specifically we employ a sensor fusion framework to make systematic use of the measurements from a forward looking radar and camera, steering wheel angle sensor, wheel speed sensors and inertial sensors to compute good estimates of the road geometry and the motion of the ego vehicle on this road. In order to solve this problem we derive dynamical models for the ego vehicle, the road and the leading vehicles. The main difference to existing approaches is that we make use of a new dynamic model for the road. An extended Kalman filter is used to fuse data and to filter measurements from the camera in order to improve the road geometry estimate. The proposed solution has been tested and compared to existing algorithms for this problem, using measurements from authentic traffic environments on public roads in Sweden. The results clearly indicate that the proposed method provides better estimates. ?? 2011 Elsevier B.V. All rights reserved.},
  Doi                      = {10.1016/j.inffus.2010.06.007},
  ISBN                     = {1566-2535},
  ISSN                     = {15662535},
  Keywords                 = {Bicycle model,Extended Kalman filter,Road geometry estimation,Sensor fusion,Single track model},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S1566253510000709}
}

@Article{ma2000simultaneous,
  Title                    = {Simultaneous detection of lane and pavement boundaries using model-based multisensor fusion},
  Author                   = {Ma, Bing and Lakshmanan, Sridhar and Hero, Alfred O},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2000},
  Number                   = {3},
  Pages                    = {135--147},
  Volume                   = {1},

  Publisher                = {IEEE}
}

@Article{miyata2017automatic,
  Title                    = {Automatic Recognition of Speed Limits on Speed-Limit Signs by Using Machine Learning},
  Author                   = {Miyata, Shigeharu},
  Journal                  = {Journal of Imaging},
  Year                     = {2017},
  Number                   = {3},
  Pages                    = {25},
  Volume                   = {3},

  Publisher                = {Multidisciplinary Digital Publishing Institute}
}

@Article{Nguyen2013,
  Title                    = {1-Point Ransac Based Robust Visual Odometry},
  Author                   = {Nguyen, Van Cuong and Heo, Moon Beom and Jee, Gyu-In},
  Journal                  = {Journal of Positioning, Navigation, and Timing},
  Year                     = {2013},

  Month                    = {apr},
  Number                   = {1},
  Pages                    = {81--89},
  Volume                   = {2},

  Doi                      = {10.11003/JKGS.2013.2.1.081},
  ISSN                     = {2288-8187},
  Keywords                 = {1-point method,Ackermann's principle,Bundle Adjustment,kpubs,kpubs.org,rotation estimation},
  Publisher                = {The Korean GNSS Society},
  Url                      = {http://koreascience.or.kr/journal/view.jsp?kj=HOHSB0{\&}py=2013{\&}vnc=v2n1{\&}sp=81}
}

@InProceedings{nie2012camera,
  Title                    = {Camera and lidar fusion for road intersection detection},
  Author                   = {Nie, Yiming and Chen, Qingyang and Chen, Tongtong and Sun, Zhenping and Dai, Bin},
  Booktitle                = {Electrical \& Electronics Engineering (EEESYM), 2012 IEEE Symposium on},
  Year                     = {2012},
  Organization             = {IEEE},
  Pages                    = {273--276}
}

@Article{ozgunalp2017multiple,
  Title                    = {Multiple lane detection algorithm based on novel dense vanishing point estimation},
  Author                   = {Ozgunalp, Umar and Fan, Rui and Ai, Xiao and Dahnoun, Naim},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2017},
  Number                   = {3},
  Pages                    = {621--632},
  Volume                   = {18},

  Publisher                = {IEEE}
}

@Article{Reina2015,
  Title                    = {Radar sensing for intelligent vehicles in urban environments},
  Author                   = {Reina, Giulio and Johnson, David and Underwood, James},
  Journal                  = {Sensors (Switzerland)},
  Year                     = {2015},

  Month                    = {jun},
  Number                   = {6},
  Pages                    = {14661--14678},
  Volume                   = {15},

  Abstract                 = {Radar overcomes the shortcomings of laser, stereovision, and sonar because it can operate successfully in dusty, foggy, blizzard-blinding, and poorly lit scenarios. This paper presents a novel method for ground and obstacle segmentation based on radar sensing. The algorithm operates directly in the sensor frame, without the need for a separate synchronised navigation source, calibration parameters describing the location of the radar in the vehicle frame, or the geometric restrictions made in the previous main method in the field. Experimental results are presented in various urban scenarios to validate this approach, showing its potential applicability for advanced driving assistance systems and autonomous vehicle operations.},
  Doi                      = {10.3390/s150614661},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reina, Johnson, Underwood - 2015 - Radar Sensing for Intelligent Vehicles in Urban Environments.pdf:pdf},
  ISSN                     = {14248220},
  Keywords                 = {Navigation systems,Perception in urban environment,Radar sensing,Robotic intelligent vehicles,Unmanned ground vehicles},
  Publisher                = {Multidisciplinary Digital Publishing Institute},
  Url                      = {http://www.mdpi.com/1424-8220/15/6/14661/}
}

@Article{Santana2016,
  Title                    = {Learning a Driving Simulator},
  Author                   = {Santana, Eder and Hotz, George},
  Year                     = {2016},

  Month                    = {aug},

  Abstract                 = {Comma.ai's approach to Artificial Intelligence for self-driving cars is based on an agent that learns to clone driver behaviors and plans maneuvers by simulating future events in the road. This paper illustrates one of our research approaches for driving simulation. One where we learn to simulate. Here we investigate variational autoencoders with classical and learned cost functions using generative adversarial networks for embedding road frames. Afterwards, we learn a transition model in the embedded space using action conditioned Recurrent Neural Networks. We show that our approach can keep predicting realistic looking video for several frames despite the transition model being optimized without a cost function in the pixel space.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1608.01230},
  Eprint                   = {1608.01230},
  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santana, Hotz - 2016 - Learning a Driving Simulator.pdf:pdf},
  Url                      = {http://arxiv.org/abs/1608.01230}
}

@Article{Scaramuzza2008,
  Title                    = {Appearance-Guided Monocular Omnidirectional Visual Odometry for Outdoor Ground Vehicles},
  Author                   = {Scaramuzza, D. and Siegwart, R.},
  Journal                  = {IEEE Transactions on Robotics},
  Year                     = {2008},

  Month                    = {oct},
  Number                   = {5},
  Pages                    = {1015--1026},
  Volume                   = {24},

  Doi                      = {10.1109/TRO.2008.2004490},
  ISSN                     = {1552-3098},
  Url                      = {http://ieeexplore.ieee.org/document/4625958/}
}

@InProceedings{schreiber2013laneloc,
  Title                    = {Laneloc: Lane marking based localization using highly accurate maps},
  Author                   = {Schreiber, Markus and Kn{\"o}ppel, Carsten and Franke, Uwe},
  Booktitle                = {Intelligent Vehicles Symposium (IV), 2013 IEEE},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {449--454}
}

@Article{soilan2016traffic,
  Title                    = {Traffic sign detection in MLS acquired point clouds for geometric and image-based semantic inventory},
  Author                   = {Soil{\'a}n, Mario and Riveiro, Bel{\'e}n and Mart{\'\i}nez-S{\'a}nchez, Joaqu{\'\i}n and Arias, Pedro},
  Journal                  = {ISPRS Journal of Photogrammetry and Remote Sensing},
  Year                     = {2016},
  Pages                    = {92--101},
  Volume                   = {114},

  Publisher                = {Elsevier}
}

@InProceedings{Stein2003,
  Title                    = {Vision-based ACC with a single camera: Bounds on range and range rate accuracy},
  Author                   = {Stein, Gideon P. and Mano, Ofer and Shashua, Amnon},
  Booktitle                = {IEEE Intelligent Vehicles Symposium, Proceedings},
  Year                     = {2003},
  Pages                    = {120--125},
  Publisher                = {IEEE},

  Abstract                 = {This paper describes a vision-based adaptive cruise control (ACC) system which uses a single camera as input. In particular, we discuss how to compute the range and range-rate from a single camera and discuss how the imaging geometry affects the range and range rate accuracy. We determine the bound on the accuracy given a particular configuration. These bounds in turn determine what steps must be made to achieve good performance. The system has been implemented on a test vehicle and driven on various highways over thousands of miles.},
  Doi                      = {10.1109/IVS.2003.1212895},
  ISBN                     = {0780378482},
  Url                      = {http://ieeexplore.ieee.org/document/1212895/}
}

@Article{sun2014application,
  Title                    = {Application of BW-ELM model on traffic sign recognition},
  Author                   = {Sun, Zhan-Li and Wang, Han and Lau, Wai-Shing and Seet, Gerald and Wang, Danwei},
  Journal                  = {Neurocomputing},
  Year                     = {2014},
  Pages                    = {153--159},
  Volume                   = {128},

  Publisher                = {Elsevier}
}

@Article{tan2016weakly,
  Title                    = {Weakly supervised metric learning for traffic sign recognition in a LIDAR-equipped vehicle},
  Author                   = {Tan, Min and Wang, Baoyuan and Wu, Zhaohui and Wang, Jingdong and Pan, Gang},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2016},
  Number                   = {5},
  Pages                    = {1415--1427},
  Volume                   = {17},

  Publisher                = {IEEE}
}

@Article{timofte2014multi,
  Title                    = {Multi-view traffic sign detection, recognition, and 3d localisation},
  Author                   = {Timofte, Radu and Zimmermann, Karel and Van Gool, Luc},
  Journal                  = {Machine vision and applications},
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {633--647},
  Volume                   = {25},

  Publisher                = {Springer}
}

@Article{villalon2017traffic,
  Title                    = {Traffic sign detection system for locating road intersections and roundabouts: the Chilean case},
  Author                   = {Villal{\'o}n-Sep{\'u}lveda, Gabriel and Torres-Torriti, Miguel and Flores-Calero, Marco},
  Journal                  = {Sensors},
  Year                     = {2017},
  Number                   = {6},
  Pages                    = {1207},
  Volume                   = {17},

  Publisher                = {Multidisciplinary Digital Publishing Institute}
}

@Article{wali2015automatic,
  Title                    = {An automatic traffic sign detection and recognition system based on colour segmentation, shape matching, and svm},
  Author                   = {Wali, Safat B and Hannan, Mahammad A and Hussain, Aini and Samad, Salina A},
  Journal                  = {Mathematical Problems in Engineering},
  Year                     = {2015},
  Volume                   = {2015},

  Publisher                = {Hindawi}
}

@InProceedings{weng2016road,
  Title                    = {Road traffic sign detection and classification from mobile LiDAR point clouds},
  Author                   = {Weng, Shengxia and Li, Jonathan and Chen, Yiping and Wang, Cheng},
  Booktitle                = {2nd ISPRS International Conference on Computer Vision in Remote Sensing (CVRS 2015)},
  Year                     = {2016},
  Organization             = {International Society for Optics and Photonics},
  Pages                    = {99010A},
  Volume                   = {9901}
}

@Article{yang2012automated,
  Title                    = {Automated extraction of road markings from mobile LiDAR point clouds},
  Author                   = {Yang, Bisheng and Fang, Lina and Li, Qingquan and Li, Jonathan},
  Journal                  = {Photogrammetric Engineering \& Remote Sensing},
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {331--338},
  Volume                   = {78},

  Publisher                = {American Society for Photogrammetry and Remote Sensing}
}

@Article{yang2016towards,
  Title                    = {Towards real-time traffic sign detection and classification},
  Author                   = {Yang, Yi and Luo, Hengliang and Xu, Huarong and Wu, Fuchao},
  Journal                  = {IEEE Transactions on Intelligent Transportation Systems},
  Year                     = {2016},
  Number                   = {7},
  Pages                    = {2022--2031},
  Volume                   = {17},

  Publisher                = {IEEE}
}

@Article{Yebes2015,
  Title                    = {Visual Object Recognition with 3D-Aware Features in KITTI Urban Scenes},
  Author                   = {Yebes, J. Javier and Bergasa, Luis M. and Garc{\'{i}}a-Garrido, Miguel {\'{A}}ngel},
  Journal                  = {Sensors (Basel, Switzerland)},
  Year                     = {2015},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {9228--9250},
  Volume                   = {15},

  Abstract                 = {Driver assistance systems and autonomous robotics rely on the deployment of several sensors for environment perception. Compared to LiDAR systems, the inexpensive vision sensors can capture the 3D scene as perceived by a driver in terms of appearance and depth cues. Indeed, providing 3D image understanding capabilities to vehicles is an essential target in order to infer scene semantics in urban environments. One of the challenges that arises from the navigation task in naturalistic urban scenarios is the detection of road participants (e.g., cyclists, pedestrians and vehicles). In this regard, this paper tackles the detection and orientation estimation of cars, pedestrians and cyclists, employing the challenging and naturalistic KITTI images. This work proposes 3D-aware features computed from stereo color images in order to capture the appearance and depth peculiarities of the objects in road scenes. The successful part-based object detector, known as DPM, is extended to learn richer models from the 2.5D data (color and disparity), while also carrying out a detailed analysis of the training pipeline. A large set of experiments evaluate the proposals, and the best performing approach is ranked on the KITTI website. Indeed, this is the first work that reports results with stereo data for the KITTI object challenge, achieving increased detection ratios for the classes car and cyclist compared to a baseline DPM.},
  Doi                      = {10.3390/s150409228},
  ISBN                     = {14248220},
  ISSN                     = {14248220},
  Keywords                 = {2.5D},
  Mendeley-tags            = {2.5D},
  Pmid                     = {102279770},
  Url                      = {http://www.ncbi.nlm.nih.gov/pubmed/25903553 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4431302 http://www.mdpi.com/1424-8220/15/4/9228/}
}

@Article{zaklouta2014real,
  Title                    = {Real-time traffic sign recognition in three stages},
  Author                   = {Zaklouta, Fatin and Stanciulescu, Bogdan},
  Journal                  = {Robotics and autonomous systems},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {16--24},
  Volume                   = {62},

  Publisher                = {Elsevier}
}

@PhdThesis{Zhang2016,
  Title                    = {Rapid Inspection of Pavement Markings Using Mobile Laser Scanning Point Clouds},
  Author                   = {Zhang, Haocheng},
  School                   = {University of Waterloo},
  Year                     = {2016},
  Month                    = {mar},

  File                     = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - 2016 - Rapid Inspection of Pavement Markings Using Mobile Laser Scanning Point Clouds.pdf:pdf},
  Url                      = {https://uwspace.uwaterloo.ca/handle/10012/10343}
}

@Article{zhang2017real,
  Title                    = {A Real-Time Chinese Traffic Sign Detection Algorithm Based on Modified YOLOv2},
  Author                   = {Zhang, Jianming and Huang, Manting and Jin, Xiaokang and Li, Xudong},
  Journal                  = {Algorithms},
  Year                     = {2017},
  Number                   = {4},
  Pages                    = {127},
  Volume                   = {10},

  Publisher                = {Multidisciplinary Digital Publishing Institute}
}

@InProceedings{zhao2012curb,
  Title                    = {Curb detection and tracking using 3D-LIDAR scanner},
  Author                   = {Zhao, Gangqiang and Yuan, Junsong},
  Booktitle                = {Image Processing (ICIP), 2012 19th IEEE International Conference on},
  Year                     = {2012},
  Organization             = {IEEE},
  Pages                    = {437--440}
}

@InProceedings{zhou2014lidar,
  Title                    = {LIDAR and vision-based real-time traffic sign detection and recognition algorithm for intelligent vehicle},
  Author                   = {Zhou, Lipu and Deng, Zhidong},
  Booktitle                = {Intelligent Transportation Systems (ITSC), 2014 IEEE 17th International Conference on},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {578--583}
}

@inproceedings{Censi2014,
abstract = {— The agility of a robotic system is ultimately limited by the speed of its processing pipeline. The use of a Dynamic Vision Sensors (DVS), a sensor producing asynchronous events as luminance changes are perceived by its pixels, makes it pos-sible to have a sensing pipeline of a theoretical latency of a few microseconds. However, several challenges must be overcome: a DVS does not provide the grayscale value but only changes in the luminance; and because the output is composed by a sequence of events, traditional frame-based visual odometry methods are not applicable. This paper presents the first visual odometry system based on a DVS plus a normal CMOS camera to provide the absolute brightness values. The two sources of data are automatically spatiotemporally calibrated from logs taken during normal operation. We design a visual odometry method that uses the DVS events to estimate the relative displacement since the previous CMOS frame by processing each event individually. Experiments show that the rotation can be estimated with surprising accuracy, while the translation can be estimated only very noisily, because it produces few events due to very small apparent motion.},
address = {Hong Kong},
author = {Censi, Andrea and Scaramuzza, Davide},
booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2014.6906931},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Censi, Scaramuzza - 2014 - Low-Latency Event-Based Visual Odometry.pdf:pdf},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
pages = {703--710},
title = {{Low-Latency Event-Based Visual Odometry}},
url = {http://rpg.ifi.uzh.ch/docs/ICRA14{\_}Censi.pdf},
year = {2014}
}
@inproceedings{Mueggler2014,
abstract = {In the last few years, we have witnessed impres- sive demonstrations of aggressive flights and acrobatics using quadrotors. However, those robots are actually blind. They do not see by themselves, but through the “eyes” of an external motion capture system. Flight maneuvers using onboard sensors are still slow compared to those attainable with motion capture systems. At the current state, the agility of a robot is limited by the latency of its perception pipeline. To obtain more agile robots, we need to use faster sensors. In this paper, we present the first onboard perception system for 6-DOF localization during high-speed maneuvers using a Dynamic Vision Sensor (DVS). Unlike a standard CMOS camera, a DVS does not wastefully send full image frames at a fixed frame rate. Conversely, similar to the human eye, it only transmits pixel-level brightness changes at the time they occur with microsecond resolution, thus, offering the possibility to create a perception pipeline whose latency is negligible compared to the dynamics of the robot. We exploit these characteristics to estimate the pose of a quadrotor with respect to a known pattern during high-speed maneuvers, such as flips, with rotational speeds up to 1,200 ◦/s. Additionally, we provide a versatile method to capture ground-truth data using a DVS},
author = {Mueggler, Elias and Huber, Basil and Scaramuzza, Davide},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2014.6942940},
isbn = {9781479969340},
issn = {21530866},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {sep},
pages = {2761--2768},
publisher = {IEEE},
title = {{Event-based, 6-DOF pose tracking for high-speed maneuvers}},
url = {http://ieeexplore.ieee.org/document/6942940/},
year = {2014}
}
@article{Vidal2017,
abstract = {Event cameras are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. These cameras do not suffer from motion blur and have a very high dynamic range, which enables them to provide reliable visual information during high speed motions or in scenes characterized by high dynamic range. However, event cameras output only little information when the amount of motion is limited, such as in the case of almost still motion. Conversely, standard cameras provide instant and rich information about the environment most of the time (in low-speed and good lighting scenarios), but they fail severely in case of fast motions, or difficult lighting such as high dynamic range or low light scenes. In this paper, we present the first state estimation pipeline that leverages the complementary advantages of these two sensors by fusing in a tightly-coupled manner events, standard frames, and inertial measurements. We show on the publicly available Event Camera Dataset that our hybrid pipeline leads to an accuracy improvement of 130{\%} over event-only pipelines, and 85{\%} over standard-frames-only visual-inertial systems, while still being computationally tractable. Furthermore, we use our pipeline to demonstrate - to the best of our knowledge - the first autonomous quadrotor flight using an event camera for state estimation, unlocking flight scenarios that were not reachable with traditional visual-inertial odometry, such as low-light environments and high-dynamic range scenes.},
archivePrefix = {arXiv},
arxivId = {1709.06310},
author = {Vidal, Antoni Rosinol and Rebecq, Henri and Horstschaefer, Timo and Scaramuzza, Davide},
doi = {10.1109/LRA.2018.2793357},
eprint = {1709.06310},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vidal et al. - 2017 - Ultimate SLAM Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenarios.pdf:pdf},
issn = {2377-3766},
journal = {IEEE ROBOTICS AND AUTOMATION LETTERS},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
number = {2},
title = {{Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM in HDR and High Speed Scenarios}},
url = {http://rpg.ifi.uzh.ch/docs/RAL18{\_}VidalRebecq.pdf http://arxiv.org/abs/1709.06310{\%}0Ahttp://dx.doi.org/10.1109/LRA.2018.2793357},
volume = {3},
year = {2017}
}
@inproceedings{Maqueda2018,
abstract = {Event cameras are bio-inspired vision sensors that naturally capture the dynamics of a scene, filtering out redundant information. This paper presents a deep neural network approach that unlocks the potential of event cameras on a challenging motion-estimation task: prediction of a vehicle's steering angle. To make the best out of this sensor-algorithm combination, we adapt state-of-the-art convolutional architectures to the output of event sensors and extensively evaluate the performance of our approach on a publicly available large scale event-camera dataset ({\~{}}1000 km). We present qualitative and quantitative explanations of why event cameras allow robust steering prediction even in cases where traditional cameras fail, e.g. challenging illumination conditions and fast motion. Finally, we demonstrate the advantages of leveraging transfer learning from traditional to event-based vision, and show that our approach outperforms state-of-the-art algorithms based on standard cameras.},
address = {Salt Lake City},
archivePrefix = {arXiv},
arxivId = {1804.01310},
author = {Maqueda, Ana I and Loquercio, Antonio and Gallego, Guillermo and Garcia, Narciso and Scaramuzza, Davide},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2018.00568},
eprint = {1804.01310},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maqueda et al. - 2018 - Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars.pdf:pdf},
keywords = {Computer Vision,Machine Learning},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
title = {{Event-based Vision meets Deep Learning on Steering Prediction for Self-driving Cars}},
url = {http://rpg.ifi.uzh.ch/docs/CVPR18{\_}Maqueda.pdf http://arxiv.org/abs/1804.01310},
year = {2018}
}
@article{OMalley2008,
abstract = {Abstract In automotive design, the issue of safety remains a growing priority. Recently the focus has extended beyond the occupants of the vehicle and has turned towards other Vulnerable Road Users (VRU). Simple night vision systems have already become an ...},
author = {O'Malley, R. and Glavin, Martin and Jones, E.},
doi = {10.1049/cp:20080657},
isbn = {978-0-86341-931-7},
journal = {Signals and Systems Conference, 208.(ISSC 2008). IET Irish},
keywords = {- pedestrian detection,active safety,agery,driver assist,infrared,obstacle detection,thermal im-},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
pages = {168--173},
pmid = {4780948},
publisher = {IEE},
title = {{A review of automotive infrared pedestrian detection techniques}},
url = {http://digital-library.theiet.org/content/conferences/10.1049/cp{\_}20080657 papers2://publication/uuid/FFD556E9-8AF3-48F2-9C9B-3B57247056B7{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4780948{\%}5Cnpapers2://publication/uuid/FC2526CD-D332-4959-B833},
year = {2008}
}
@article{Maddalena2005,
abstract = {After penetrating over a decade the consumer and industrial world, digital imaging is slowly but inevitably gaining marketshare in the automotive world. Cameras will become a key sensor in increasing car safety, driving assistance and driving comfort. The image sensors for automotive will be dominated by CMOS sensors as the requirements are different from the consumer market or the industrial or medical markets. Dynamic range, temperature range, cost, speed and many others are key parameters that need to be optimized. For this reason, automotive sensors differ from the other market's sensors and need to use different design and processing techniques in order to achieve the automotive specifications. This paper will show how Melexis has developed two CMOS imagers to target the automotive safety market and automotive CMOS imagers in general.},
address = {Berlin/Heidelberg},
author = {Maddalena, S. and Darmon, A. and Diels, R.},
doi = {10.1007/3-540-27463-4_29},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maddalena, Darmon, Diels - 2005 - Automotive CMOS Image Sensors.pdf:pdf},
journal = {Advanced Microsystems for Automotive {\ldots}},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
pages = {401--412},
publisher = {Springer-Verlag},
title = {{Automotive CMOS image sensors}},
url = {http://link.springer.com/10.1007/3-540-27463-4{\_}29 http://www.springerlink.com/index/M354109378G10242.pdf},
year = {2005}
}
@article{Strobel2013,
author = {Strobel, Markus and D{\"{o}}ttling, Dietmar},
doi = {10.1515/aot-2012-0081},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Strobel, D{\"{o}}ttling - 2013 - High dynamic range CMOS (HDRC) imagers for safety systems.pdf:pdf},
issn = {2192-8584},
journal = {Advanced Optical Technologies},
keywords = {CMOS image sensor,OCIS codes: 110.4850,SafetyEYE,global shutter,high dynamic range CMOS (HDRC),safe camera system},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {jan},
number = {2},
pages = {147--157},
publisher = {THOSS Media {\&} De Gruyter},
title = {{High dynamic range CMOS (HDRC) imagers for safety systems}},
url = {https://www.degruyter.com/view/j/aot.2013.2.issue-2/aot-2012-0081/aot-2012-0081.xml},
volume = {2},
year = {2013}
}
@article{Chun2008,
abstract = {This paper focuses on the rolling shutter distortion of CMOS image sensor coming from its unique readout mechanism as the main cause for image degradation when there are fast-moving objects. This paper proposes a post image processing scheme based on motion vector detection to suppress the rolling shutter distortion. Motion vector detection is performed based on an optical flow method at a reasonable computational complexity. A practical implementation scheme is also described.},
author = {Chun, Jung Bum and Jung, Hunjoon and Kyung, Chong Min},
doi = {10.1109/TCE.2008.4711190},
issn = {00983063},
journal = {IEEE Transactions on Consumer Electronics},
keywords = {CMOS image sensor,Post-processing technique,Rolling-shutter distortion},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {nov},
number = {4},
pages = {1479--1487},
title = {{Suppressing rolling-shutter distortion of CMOS image sensors by motion vector detection}},
url = {http://ieeexplore.ieee.org/document/4711190/},
volume = {54},
year = {2008}
}
@article{Chia-KaiLiang2008,
abstract = {Due to the sequential-readout structure of complementary metal-oxide semiconductor image sensor array, each scanline of the acquired image is exposed at a different time, resulting in the so-called electronic rolling shutter that induces geometric image distortion when the object or the video camera moves during image capture. In this paper, we propose an image processing technique using a planar motion model to address the problem. Unlike previous methods that involve complex 3-D feature correspondences, a simple approach to the analysis of inter- and intraframe distortions is presented. The high-resolution velocity estimates used for restoring the image are obtained by global motion estimation, BEzier curve fitting, and local motion estimation without resort to correspondence identification. Experimental results demonstrate the effectiveness of the algorithm.},
author = {Liang, Chia Kai and Chang, Li Wen and Chen, Homer H.},
doi = {10.1109/TIP.2008.925384},
isbn = {1057-7149 (Print)},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Complementary metal-oxide semiconductor (CMOS) sen,Motion analysis,Rolling shutter},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {aug},
number = {8},
pages = {1323--1330},
pmid = {18632342},
title = {{Analysis and compensation of rolling shutter effect}},
url = {http://ieeexplore.ieee.org/document/4549748/},
volume = {17},
year = {2008}
}
@article{Pueo2016,
abstract = {Video analysis can be a qualitative or quantitative process to analyze motion occurring in a single plane using one camera (two-dimensional or 2D) or in more than one plane using two or more cameras simultaneously (three-dimensional or 3D). Quantitative 2D video analysis is performed through a digitizing process that converts body segments or sport implements into digital horizontal and vertical coordinates in the computer. In order for these measurements to be accurate, image capture by means of video cameras must be sharp and motion blur-free, especially in high speed motions. In this paper, a detailed introduction to factors affecting image quality will be presented. Furthermore, selection of the most appropriate camera setting to undertake high speed motion analysis with the best quality possible, both spatially (focus and resolution) and temporally (frame rate, motion blur, shutter options and lighting), will be discussed. Rather than considering commercial criteria, the article will focus on key features to choose the most convenient model both from technical and economical perspectives. Then, a revision of available cameras on the market as of 2015 will be carried out, with selected models grouped into three categories: high-, mid- and low-range, according to their maximum performance in relation to high speed features. Finally, a suggested recording procedure to minimize perspective errors and produce high quality video recordings will be presented. This guideline starts with indications for camera selection prior to purchase or for testing if a given camera would fulfil the minimum features. A good video recording dramatically improves the analysis quality and enables digitizing software to produce accurate measurements},
author = {Pueo, Basilio},
doi = {10.14198/jhse.2016.111.05},
issn = {19885202},
journal = {Journal of Human Sport and Exercise},
keywords = {Biomechanics,Frame rate,Motion blur,Performance,Shutter speed},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {dec},
number = {1},
pages = {53--73},
title = {{High speed cameras for motion analysis in sports science}},
url = {http://hdl.handle.net/10045/61909},
volume = {11},
year = {2016}
}
@article{Aqel2016,
abstract = {Accurate localization of a vehicle is a fundamental challenge and one of the most important tasks of mobile robots. For autonomous navigation, motion tracking, and obstacle detection and avoidance, a robot must maintain knowledge of its position over time. Vision-based odometry is a robust technique utilized for this purpose. It allows a vehicle to localize itself robustly by using only a stream of images captured by a camera attached to the vehicle. This paper presents a review of state-of-the-art visual odometry (VO) and its types, approaches, applications, and challenges. VO is compared with the most common localization sensors and techniques, such as inertial navigation systems, global positioning systems, and laser sensors. Several areas for future research are also highlighted.},
author = {Aqel, Mohammad O A and Marhaban, Mohammad H and Saripan, M Iqbal and Ismail, Napsiah Bt},
doi = {10.1186/s40064-016-3573-7},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aqel et al. - 2016 - Review of visual odometry types, approaches, challenges, and applications.pdf:pdf},
issn = {2193-1801},
journal = {SpringerPlus},
keywords = {Global positioning system,Image stream,Inertial navigation system,Localization sensors,Visual odometry},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
number = {1},
pages = {1897},
pmid = {27843754},
publisher = {Springer},
title = {{Review of visual odometry: types, approaches, challenges, and applications.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27843754 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5084145},
volume = {5},
year = {2016}
}
@article{DePonteMuller2017,
abstract = {Future driver assistance systems will rely on accurate, reliable and continuous knowledge on the position of other road participants, including pedestrians, bicycles and other vehicles. The usual approach to tackle this requirement is to use on-board ranging sensors inside the vehicle. Radar, laser scanners or vision-based systems are able to detect objects in their line-of-sight. In contrast to these non-cooperative ranging sensors, cooperative approaches follow a strategy in which other road participants actively support the estimation of the relative position. The limitations of on-board ranging sensors regarding their detection range and angle of view and the facility of blockage can be approached by using a cooperative approach based on vehicle-to-vehicle communication. The fusion of both, cooperative and non-cooperative strategies, seems to offer the largest benefits regarding accuracy, availability and robustness. This survey offers the reader a comprehensive review on different techniques for vehicle relative positioning. The reader will learn the important performance indicators when it comes to relative positioning of vehicles, the different technologies that are both commercially available and currently under research, their expected performance and their intrinsic limitations. Moreover, the latest research in the area of vision-based systems for vehicle detection, as well as the latest work on GNSS-based vehicle localization and vehicular communication for relative positioning of vehicles, are reviewed. The survey also includes the research work on the fusion of cooperative and non-cooperative approaches to increase the reliability and the availability.},
author = {{de Ponte M{\"{u}}ller}, Fabian and Fabian},
doi = {10.3390/s17020271},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de Ponte M{\"{u}}ller, Fabian - 2017 - Survey on Ranging Sensors and Cooperative Techniques for Relative Positioning of Vehicles.pdf:pdf},
issn = {1424-8220},
journal = {Sensors},
keywords = {GNSS,cooperative,laser scanner,localization,relative positioning,to,vehicle,vehicle sensors},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {jan},
number = {2},
pages = {271},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Survey on Ranging Sensors and Cooperative Techniques for Relative Positioning of Vehicles}},
url = {http://www.mdpi.com/1424-8220/17/2/271},
volume = {17},
year = {2017}
}
@article{Velez2017,
author = {Velez, Gorka and Otaegui, Oihana},
doi = {10.1049/iet-its.2016.0026},
issn = {1751-956X},
journal = {IET Intelligent Transport Systems},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {apr},
number = {3},
pages = {103--112},
title = {{Embedding vision-based advanced driver assistance systems: a survey}},
url = {http://digital-library.theiet.org/content/journals/10.1049/iet-its.2016.0026},
volume = {11},
year = {2017}
}

@article{Kohler2013,
author = {K{\"{o}}hler, Mike and Hasch, J{\"{u}}rgen and Bl{\"{o}}cher, Hans Ludwig and Schmidt, Lorenz Peter},
doi = {10.1017/S175907871200075X},
issn = {17590787},
journal = {International Journal of Microwave and Wireless Technologies},
keywords = {Automotive radar},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {feb},
number = {1},
pages = {49--54},
title = {{Feasibility of automotive radar at frequencies beyond 100 GHz}},
url = {http://www.journals.cambridge.org/abstract{\_}S175907871200075X},
volume = {5},
year = {2013}
}
@article{Kishida2015,
abstract = {High-resolution millimeter-wave radar that operates in the 79 GHz band is expected to achieve a significant increase in the distance resolution of radar systems because of the availability of a wide frequency bandwidth of 4 GHz as compared with 0.5 GHz of the existing 77 GHz-band mil-limeter-wave radar. For this reason, it has the potential to distinguish between a vehicle and a human, which was conventionally difficult, and recognize their movements. Therefore it raises expectations for use as a surrounding monitoring radar in driving safety support and auto-matic driving. As one of FUJITSU TEN's efforts regarding sensing technologies for driving safety support and automatic driving, it has been developing 79 GHz-band high-resolution millimeter-wave radar. This paper presents specifications of radar for application to systems that assist in safe driving and automatic driving and the results of testing a prototype for a wider bandwidth that is required to accomplish the technology's purpose. This radar increases the ability to de-tect a pedestrian in the surroundings of a vehicle, which was difficult to do with the existing 77 GHz-band radar. Furthermore, this paper also describes how the newly developed radar of-fers the possibility of improving the performance of a sensor for automatic driving and systems to assist in safe driving.},
author = {Kishida, Masayuki and Ohguchi, Katsuyuki and Shono, Masayoshi},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kishida, Ohguchi, Shono - 2015 - 79 GHz-Band High-Resolution Millimeter-Wave Radar.pdf:pdf},
journal = {FUJITSU Sci. Tech. J},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
number = {4},
pages = {55--59},
title = {{79 GHz-Band High-Resolution Millimeter- Wave Radar}},
url = {https://www.fujitsu.com/global/documents/about/resources/publications/fstj/archives/vol51-4/paper09.pdf},
volume = {51},
year = {2015}
}

@misc{EULawandPublications2004,
booktitle = {EU Law and Publications},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
pages = {2},
title = {{2004/545/EC: Commission Decision of 8 July 2004 on the harmonisation of radio spectrum in the 79 GHz range for the use of automotive short-range radar equipment in the Community}},
url = {https://publications.europa.eu/en/publication-detail/-/publication/9d425670-b54b-4c65-8461-824dbf71facc/language-en},
urldate = {2018-07-19},
year = {2004}
}

@article{Gatziolis2008,
abstract = {Light detection and ranging (LIDAR) is an emerging remote-sensing technology with promising potential to assist in mapping, monitoring, and assessment of forest resources. Continuous technological advancement and substantial reductions in data acquisition cost have enabled acquisition of laser data over entire states and regions. These developments have triggered an explosion of interest in LIDAR technology. Despite a growing body of peer-reviewed literature documenting the merits of LIDAR for forest assessment, management, and planning, there seems to be little information describing in detail the acquisition, quality assessment, and processing of laser data for forestry applications. This report addresses this information deficit by providing a foundational knowledge base containing answers to the most frequently asked questions. Keywords: LIDAR, Pacific Northwest, FIA, forest inventory, laser, absolute and relative accuracy, precision, registration, stand penetration, DEM, canopy surface, resolution, data storage, data quality assessment, topography, scanning.},
author = {Gatziolis, Demetrios and Andersen, Hans Erik},
doi = {Gen. Tech. Rep. PNW-GTR-768},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gatziolis, Andersen - 2008 - A guide to LIDAR data acquisition and processing for the forests of the Pacific Northwest.pdf:pdf},
journal = {General Technical Report PNW-GTR-768},
keywords = {DEM,FIA,LIDAR,Pacific Northwest,absolute and relative accuracy,canopy surface,data quality assessment,data storage,forest inventory,laser,precision,registration,resolution,scanning.,stand penetration,topography},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
number = {July},
pages = {1--40},
title = {{A Guide to LIDAR Data Acquisition and Processing for the Forests of the Pacific Northwest}},
url = {https://www.fs.usda.gov/treesearch/pubs/30652 http://www.arlis.org/docs/vol1/A/276932054.pdf},
volume = {768},
year = {2008}
}
@article{Glennie2010,
abstract = {The static calibration and analysis of the Velodyne HDL-64E S2 scanning LiDAR system is presented and analyzed. The mathematical model for measurements for the HDL-64E S2 scanner is derived and discussed. A planar feature based least squares adjustment approach is presented and utilized in a minimally constrained network in order to derive an optimal solution for the laser's internal calibration parameters. Finally, the results of the adjustment along with a detailed examination of the adjustment residuals are given. A three-fold improvement in the planar misclosure residual RMSE over the standard factory calibration model was achieved by the proposed calibration. Results also suggest that there may still be some unmodelled distortions in the range measurements from the scanner. However, despite this, the overall precision of the adjusted laser scanner data appears to make it a viable choice for high accuracy mobile scanning applications.},
author = {Glennie, Craig and Lichti, Derek D.},
doi = {10.3390/rs2061610},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glennie, Lichti - 2010 - Static Calibration and Analysis of the Velodyne HDL-64E S2 for High Accuracy Mobile Scanning.pdf:pdf},
isbn = {2072-4292},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Accuracy,Error analysis,Lidar,System calibration},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {jun},
number = {6},
pages = {1610--1624},
publisher = {Molecular Diversity Preservation International},
title = {{Static calibration and analysis of the velodyne HDL-64E S2 for high accuracy mobile scanning}},
url = {http://www.mdpi.com/2072-4292/2/6/1610},
volume = {2},
year = {2010}
}

@article{Wang2008,
abstract = {In the visible and near IR, absorption is negligible so that the atmospheric extinction can be derived by atmospheric scattering which is mainly contributed by fog droplet, rain droplet, another types of droplet and small articles. The forward-scattering visibility meter (FVM) works by illuminating with near IR light a small sample volume of about 100 mL of air and measuring the intensity scattered in the angular range of 30° to 36° degrees. The scattered intensity is proportional to the extinction coefficient regardless of the article size distribution and after wavelength calibration. The ratio of scattered signal to extinction coefficient of fog and haze can be achieved by comparative test of FVM outputs and manual observations. Nevertheless, as a result of the application of the measurement during rain with the ratio of fog and haze, an unacceptable error is raised. To obtain an accuracy extinction measurement during rain, an appropriated ratio of scattered signal to extinction coefficient of rain would be found. The calculation for different size distributions of fog and rain with Mie theory has been made in this paper. And a comparison of extinction measurements made with two FVMs and manual observations during fog and rain has been made. The result shows that during rain the FVM extinction coefficient is from 20{\%} to 60{\%} greater than that of manual observations. This result can be used to define correction factors so that the FVM using forward-scattering near IR spectroscopy not only can be used to estimate extinction during fog and haze as well as during rain.},
author = {Wang, M. and Liu, W.-Q. and Lu, Y.-H. and Zhao, X.-S. and Song, B.-C. and Zhang, Y.-J. and Wang, Y.-P. and Lian, C.-H. and Chen, Jun and Cheng, Yin and Liu, J.-G. and Wei, Q.-N.},
issn = {10000593},
journal = {Guang Pu Xue Yu Guang Pu Fen Xi/Spectroscopy and Spectral Analysis},
keywords = {Atmospheric extinction,Extinction coefficient,Forward-scattering,Near IR,Phase function,Size distribution,Visibility},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {aug},
number = {8},
pages = {1776--80},
pmid = {18975801},
title = {{Study on the measurement of the atmospheric extinction of fog and rain by forward-scattering near infrared spectroscopy}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18975801},
volume = {28},
year = {2008}
}

@article{Phillips2017,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Phillips, Tyson Govan and Guenther, Nicky and McAree, Peter Ross},
doi = {10.1002/rob.21701},
eprint = {10.1.1.91.5767},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Guenther, McAree - 2017 - When the Dust Settles The Four Behaviors of LiDAR in the Presence of Fine Airborne Particulates.pdf:pdf},
isbn = {9783902661623},
issn = {15564967},
journal = {Journal of Field Robotics},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {aug},
number = {5},
pages = {985--1009},
pmid = {22164016},
publisher = {Wiley-Blackwell},
title = {{When the Dust Settles: The Four Behaviors of LiDAR in the Presence of Fine Airborne Particulates}},
url = {http://doi.wiley.com/10.1002/rob.21701},
volume = {34},
year = {2017}
}

@article{McManamon1996,
abstract = {{Optical phased arrays represent an enabling new technology that makes possible simple affordable, lightweight, optical sensors offering very precise stabilization, random-access pointing programmable multiple simultaneous beams, a dynamic focus/defocus capability, and moderate to excellent optical power handling capability. These new arrays steer or otherwise operate on an already formed beam. A phase profile is imposed on an optical beam as it is either transmitted through or reflected from the phase shifter array. The imposed phase profile steers, focuses, fans out, or corrects phase aberrations on the beam. The array of optical phase shifters is realized through lithographic patterning of an electrical addressing network on the superstrate of a liquid crystal waveplate. Refractive index changes sufficiently large to realize full-wave differential phase shifts can be effected using low ({\textless}10 V) voltages applied to the liquid crystal phase plate electrodes. High efficiency large-angle steering with phased arrays requires phase shifter spacing on the order of a wavelength or less; consequently addressing issues make 1-D optical arrays much more practical than 2-D arrays. Orthogonal oriented 1-D phased arrays are used to deflect a beam in both dimensions. Optical phased arrays with apertures on the order of 4 cm by 4 cm have been fabricated for steering green, red, 1.06 $\mu$m, and 10.6 $\mu$m radiation. System concepts that include a passive acquisition sensor as well as a laser radar are presented{\}}, keywords={\{}aberrations;arrays;liquid crystal devices;lithography;optical radar;optical sensors;phase shifters;phased array radar;refractive index;1.06 micrometre;10 V;10.6 micrometre;1D optical arrays;dynamic focus/defocus capability;electrical addressing network;full-wave differential phase shifts;large-angle steering;laser radar;liquid crystal waveplate;lithographic patterning;optical phased array technology;optical power handling capability;optical sensors;passive acquisition sensor;phase aberrations;phase profile;phase shifter array;programmable multiple simultaneous beams;random-access pointing;refractive index changes;Laser radar;Liquid crystals;Optical arrays;Optical beams;Optical refraction;Optical sensors;Optical variables control;Phase shifters;Phased arrays;Sensor arrays}},
author = {Mcmanamon, Paul F. and Dorschner, Terry A. and Corkum, David L. and Friedman, Larry J. and Hobbs, Douglas S. and Holz, Michael and Liberman, Sergey and Nguyen, Huy Q. and Resler, Daniel P. and Sharp, Richard C. and Watson, Edward A. and Dorschner, T. A. and Friedman, L. J. and Hobbs, D. S. and Holz, M. and Resler, D. P. and Sharp, R. C.},
doi = {10.1109/5.482231},
issn = {00189219},
journal = {Proceedings of the IEEE},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
month = {feb},
number = {2},
pages = {268--298},
title = {{Optical phased array technology}},
url = {http://ieeexplore.ieee.org/document/482231/},
volume = {84},
year = {1996}
}

@techreport{LeddarTech2016,
abstract = {Remote sensing consists of acquiring information about a specific object in the vicinity of a sensor without making physical contact with the object. Countless applications such as automotive driver assistance systems and autonomous driving, drone and robot collision avoidance and navigation, traffic management and level sensing exist thanks to this technique. Multiple technology options are available for remote sensing; we can divide them into three broad applications: Presence or proximity detection, where the absence or presence of an object in a general area is the only information that is required (e.g., for security applications). This is the simplest form of remote sensing; Speed measurement, where the exact position of an object does not need to be known but where its accurate speed is required (e.g., for law enforcement applications); and Detection and ranging, where the position of an object relative to the sensor needs to be precisely and accurately determined. This paper will concentrate on technologies capable of providing a detection and ranging functionality, as it is the most complex of the three applications. From the position information, presence and speed can be retrieved so technologies capable of detection and ranging can be universally applied to all remote sensing applications.},
address = {Quebec},
author = {Olivier, Pierre},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Olivier - 2016 - Leddar Optical Time-of-Flight sensing Technology A new approach to detection and ranging.pdf:pdf},
institution = {LeddarTech},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
pages = {13},
title = {{LEDDAR optical Time-of-Flight sensing technology: A new approach to detection and ranging}},
url = {https://leddartech.com/app/uploads/dlm{\_}uploads/2016/02/Leddar-Optical-Time-of-Flight-Sensing-Technology-1.pdf https://d1wx5us9wukuh0.cloudfront.net/app/uploads/dlm{\_}uploads/2016/02/Leddar-Optical-Time-of-Flight-Sensing-Technology-1.pdf},
year = {2016}
}

@misc{Eldada2017,
address = {San Francisco},
author = {Eldada, Louay},
booktitle = {Automated Vehicles Symposium},
file = {:D$\backslash$:/Users/109123/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eldada - 2017 - LiDAR and the Autonomous Vehicle Revolution for Truck and Ride Sharing Fleets.pdf:pdf},
mendeley-groups = {Tecnalia/Publicaciones/2017-01{\_}SotA{\_}ADAS},
title = {{LiDAR and the Autonomous Vehicle Revolution for Truck and Ride Sharing Fleets}},
url = {https://higherlogicdownload.s3.amazonaws.com/AUVSI/14c12c18-fde1-4c1d-8548-035ad166c766/UploadedImages/2017/PDFs/Proceedings/ESS/Wednesday 1330-1400{\_}Louay Eldada.pdf},
year = {2017}
}

@Article{blanc2004obstacle,
  author    = {Blanc, Christophe and Aufrere, Romuald and Malaterre, Laurent and Gallice, Jean and Alizon, Joseph},
  title     = {Obstacle detection and tracking by millimeter wave radar},
  journal   = {IFAC Proceedings Volumes},
  year      = {2004},
  volume    = {37},
  number    = {8},
  pages     = {322--327},
  publisher = {Elsevier},
}

@InProceedings{garcia2012data,
  author       = {Garcia, Fernando and Cerri, Pietro and Broggi, Alberto and de la Escalera, Arturo and Armingol, Jos{\'e} Mar{\'\i}a},
  title        = {Data fusion for overtaking vehicle detection based on radar and optical flow},
  booktitle    = {Intelligent Vehicles Symposium (IV), 2012 IEEE},
  year         = {2012},
  pages        = {494--499},
  organization = {IEEE},
}

@InProceedings{gohring2011radar,
  author       = {G{\"o}hring, Daniel and Wang, Miao and Schn{\"u}rmacher, Michael and Ganjineh, Tinosch},
  title        = {Radar/lidar sensor fusion for car-following on highways},
  booktitle    = {Automation, Robotics and Applications (ICARA), 2011 5th International Conference on},
  year         = {2011},
  pages        = {407--412},
  organization = {IEEE},
}

@InProceedings{song2007lateral,
  author       = {Song, Kai-Tai and Chen, Hung-Yi},
  title        = {Lateral driving assistance using optical flow and scene analysis},
  booktitle    = {Intelligent Vehicles Symposium, 2007 IEEE},
  year         = {2007},
  pages        = {624--629},
  organization = {IEEE},
}

@InProceedings{blanc2007larasidecam,
  author       = {Blanc, Nicolas and Steux, Bruno and Hinz, Thomas},
  title        = {LaRASideCam: A fast and robust vision-based blindspot detection system},
  booktitle    = {Intelligent Vehicles Symposium, 2007 IEEE},
  year         = {2007},
  pages        = {480--485},
  organization = {IEEE},
}

@InProceedings{chang2008real,
  author       = {Chang, Wen-Chung and Cho, Chih-Wei},
  title        = {Real-time side vehicle tracking using parts-based boosting},
  booktitle    = {Systems, Man and Cybernetics, 2008. SMC 2008. IEEE International Conference on},
  year         = {2008},
  pages        = {3370--3375},
  organization = {IEEE},
}

@Article{gandhi2006vehicle,
  author    = {Gandhi, Tarak and Trivedi, Mohan M},
  title     = {Vehicle surround capture: Survey of techniques and a novel omni-video-based approach for dynamic panoramic surround maps},
  journal   = {IEEE Transactions on Intelligent Transportation Systems},
  year      = {2006},
  volume    = {7},
  number    = {3},
  pages     = {293--308},
  publisher = {IEEE},
}

@InProceedings{liu2007rear,
  author       = {Liu, Wei and Wen, XueZhi and Duan, Bobo and Yuan, Huai and Wang, Nan},
  title        = {Rear vehicle detection and tracking for lane change assist},
  booktitle    = {Intelligent Vehicles Symposium, 2007 IEEE},
  year         = {2007},
  pages        = {252--257},
  organization = {IEEE},
}

@Comment{jabref-meta: databaseType:bibtex;}
